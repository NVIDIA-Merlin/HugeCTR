<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>HugeCTR Layer Classes and Methods &mdash; Merlin HugeCTR  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Additional Resources" href="../additional_resources.html" />
    <link rel="prev" title="HugeCTR Python Interface" href="python_interface.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_feature_details_intro.html">Features in Detail</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API Documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="python_interface.html">Python Interface</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Layer Classes and Methods</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">HugeCTR API Documentation</a> &raquo;</li>
      <li>HugeCTR Layer Classes and Methods</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="tex2jax_ignore mathjax_ignore section" id="hugectr-layer-classes-and-methods">
<h1>HugeCTR Layer Classes and Methods<a class="headerlink" href="#hugectr-layer-classes-and-methods" title="Permalink to this headline"></a></h1>
<p>This document introduces different layer classes and corresponding methods in the Python API of HugeCTR. The description of each method includes its functionality, arguments, and examples of usage.</p>
<div class="section" id="input-layer">
<h2>Input Layer<a class="headerlink" href="#input-layer" title="Permalink to this headline"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hugectr</span><span class="o">.</span><span class="n">Input</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Input</span></code> layer specifies the parameters related to the data input. <code class="docutils literal notranslate"><span class="pre">Input</span></code> layer should be added to the Model instance first so that the following <code class="docutils literal notranslate"><span class="pre">SparseEmbedding</span></code> and <code class="docutils literal notranslate"><span class="pre">DenseLayer</span></code> instances can access the inputs with their specified names.</p>
<p><strong>Arguments</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">label_dim</span></code>: Integer, the label dimension. 1 implies it is a binary label. For example, if an item is clicked or not. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">label_name</span></code>: String, the name of the label tensor to be referenced by following layers. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dense_dim</span></code>: Integer, the number of dense (or continuous) features. If there is no dense feature, set it to 0. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dense_name</span></code>: Integer, the name of the dense input tensor to be referenced by following layers. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data_reader_sparse_param_array</span></code>: List[hugectr.DataReaderSparseParam], the list of the sparse parameters for categorical inputs. Each <code class="docutils literal notranslate"><span class="pre">DataReaderSparseParam</span></code> instance should be constructed with  <code class="docutils literal notranslate"><span class="pre">sparse_name</span></code>, <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code>, <code class="docutils literal notranslate"><span class="pre">is_fixed_length</span></code> and <code class="docutils literal notranslate"><span class="pre">slot_num</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">sparse_name</span></code> is the name of the sparse input tensors to be referenced by following layers. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code> is the maximum number of features for each slot for the specified spare input. The <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code> can be an <code class="docutils literal notranslate"><span class="pre">int</span></code> which means average nnz per slot so the maximum number of features per sample should be <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span> <span class="pre">*</span> <span class="pre">slot_num</span></code>. Or you can use List[int] to initialize <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code>, then the maximum number of features per sample should be <code class="docutils literal notranslate"><span class="pre">sum(nnz_per_slot)</span></code> and in this case, the length of the array <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code> should be the same with <code class="docutils literal notranslate"><span class="pre">slot_num</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_fixed_length</span></code> is used to identify whether categorical inputs has the same length for each slot among all samples. If different samples have the same number of features for each slot, then user can set <code class="docutils literal notranslate"><span class="pre">is_fixed_length</span> <span class="pre">=</span> <span class="pre">True</span></code> and HugeCTR can use this information to reduce data transferring time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">slot_num</span></code> specifies the number of slots used for this sparse input in the dataset.</p></li>
</ul>
</li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">label_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">label_name</span> <span class="o">=</span> <span class="s2">&quot;label&quot;</span><span class="p">,</span>
                        <span class="n">dense_dim</span> <span class="o">=</span> <span class="mi">13</span><span class="p">,</span> <span class="n">dense_name</span> <span class="o">=</span> <span class="s2">&quot;dense&quot;</span><span class="p">,</span>
                        <span class="n">data_reader_sparse_param_array</span> <span class="o">=</span>
                            <span class="p">[</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DataReaderSparseParam</span><span class="p">(</span><span class="s2">&quot;data1&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">26</span><span class="p">)]))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">label_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">label_name</span> <span class="o">=</span> <span class="s2">&quot;label&quot;</span><span class="p">,</span>
                        <span class="n">dense_dim</span> <span class="o">=</span> <span class="mi">13</span><span class="p">,</span> <span class="n">dense_name</span> <span class="o">=</span> <span class="s2">&quot;dense&quot;</span><span class="p">,</span>
                        <span class="n">data_reader_sparse_param_array</span> <span class="o">=</span>
                            <span class="p">[</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DataReaderSparseParam</span><span class="p">(</span><span class="s2">&quot;wide_data&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                            <span class="n">hugectr</span><span class="o">.</span><span class="n">DataReaderSparseParam</span><span class="p">(</span><span class="s2">&quot;deep_data&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">26</span><span class="p">)]))</span>
</pre></div>
</div>
</div>
<div class="section" id="sparse-embedding">
<h2>Sparse Embedding<a class="headerlink" href="#sparse-embedding" title="Permalink to this headline"></a></h2>
<p><strong>SparseEmbedding class</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hugectr</span><span class="o">.</span><span class="n">SparseEmbedding</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">SparseEmbedding</span></code> specifies the parameters related to the sparse embedding layer. One or several <code class="docutils literal notranslate"><span class="pre">SparseEmbedding</span></code> layers should be added to the Model instance after <code class="docutils literal notranslate"><span class="pre">Input</span></code> and before <code class="docutils literal notranslate"><span class="pre">DenseLayer</span></code>.</p>
<p><strong>Arguments</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">embedding_type</span></code>: The embedding type to be used. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Embedding_t.LocalizedSlotSparseEmbeddingOneHot</span></code>. There is NO default value and it should be specified by users. For detail about different embedding types, please refer to <a class="reference internal" href="#embedding-types-detail"><span class="std std-doc">Embedding Types Detail</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">workspace_size_per_gpu_in_mb</span></code>: Integer, the workspace memory size in megabyte per GPU. This workspace memory must be big enough to hold all the embedding vocabulary and its corresponding optimizer state used during the training and evaluation. There is NO default value and it should be specified by users. To understand how to set this value, please refer to <a class="reference internal" href="../QAList.html#how-to-set-workspace-size-per-gpu-in-mb-and-slot-size-array"><span class="std std-doc">How to set workspace_size_per_gpu_in_mb and slot_size_array</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embedding_vec_size</span></code>: Integer, the embedding vector size. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">combiner</span></code>: String, the intra-slot reduction operation, currently <code class="docutils literal notranslate"><span class="pre">sum</span></code> or <code class="docutils literal notranslate"><span class="pre">mean</span></code> are supported. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sparse_embedding_name</span></code>: String, the name of the sparse embedding tensor to be referenced by following layers. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bottom_name</span></code>: String, the number of the bottom tensor to be consumed by this sparse embedding layer. Please note that it should be a predefined sparse input name. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">slot_size_array</span></code>: List[int], the cardinality array of input features. It should be consistent with that of the sparse input. This parameter is used in <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingHash</span></code> and <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingOneHot</span></code>, which can help avoid wasting memory caused by imbalance vocabulary size. Please refer <a class="reference internal" href="../QAList.html#how-to-set-workspace-size-per-gpu-in-mb-and-slot-size-array"><span class="std std-doc">How to set workspace_size_per_gpu_in_mb and slot_size_array</span></a>. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optimizer</span></code>: OptParamsPy, the optimizer dedicated to this sparse embedding layer. If the user does not specify the optimizer for the sparse embedding, it will adopt the same optimizer as dense layers.</p></li>
</ul>
</div>
<div class="section" id="embedding-types-detail">
<h2>Embedding Types Detail<a class="headerlink" href="#embedding-types-detail" title="Permalink to this headline"></a></h2>
<div class="section" id="distributedslotsparseembeddinghash-layer">
<h3>DistributedSlotSparseEmbeddingHash Layer<a class="headerlink" href="#distributedslotsparseembeddinghash-layer" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">DistributedSlotSparseEmbeddingHash</span></code> stores embeddings in an embedding table and gets them by using a set of integers or indices. The embedding table can be segmented into multiple slots or feature fields, which spans multiple GPUs and nodes. With <code class="docutils literal notranslate"><span class="pre">DistributedSlotSparseEmbeddingHash</span></code>, each GPU will have a portion of a slot. This type of embedding is useful when there’s an existing load imbalance among slots and OOM issues.</p>
<p><strong>Important Notes</strong>:</p>
<ul class="simple">
<li><p>In a single embedding layer, it is assumed that input integers represent unique feature IDs, which are mapped to unique embedding vectors.
All the embedding vectors in a single embedding layer must have the same size. If you want some input categorical features to have different embedding vector sizes, use multiple embedding layers.</p></li>
<li><p>The input indices’ data type, <code class="docutils literal notranslate"><span class="pre">input_key_type</span></code>, is specified in the solver. By default,  the 32-bit integer (I32) is used, but the 64-bit integer type (I64) is also allowed even if it is constrained by the dataset type. For additional information, see <a class="reference internal" href="python_interface.html#solver"><span class="std std-doc">Solver</span></a>.</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">SparseEmbedding</span><span class="p">(</span>
            <span class="n">embedding_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Embedding_t</span><span class="o">.</span><span class="n">DistributedSlotSparseEmbeddingHash</span><span class="p">,</span>
            <span class="n">workspace_size_per_gpu_in_mb</span> <span class="o">=</span> <span class="mi">23</span><span class="p">,</span>
            <span class="n">embedding_vec_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">combiner</span> <span class="o">=</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span>
            <span class="n">sparse_embedding_name</span> <span class="o">=</span> <span class="s2">&quot;sparse_embedding1&quot;</span><span class="p">,</span>
            <span class="n">bottom_name</span> <span class="o">=</span> <span class="s2">&quot;input_data&quot;</span><span class="p">,</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="localizedslotsparseembeddinghash-layer">
<h3>LocalizedSlotSparseEmbeddingHash Layer<a class="headerlink" href="#localizedslotsparseembeddinghash-layer" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingHash</span></code> layer to store embeddings in an embedding table and get them by using a set of integers or indices. The embedding table can be segmented into multiple slots or feature fields, which spans multiple GPUs and nodes. Unlike the DistributedSlotSparseEmbeddingHash layer, with this type of embedding layer, each individual slot is located in each GPU and not shared. This type of embedding layer provides the best scalability.</p>
<p><strong>Important Notes</strong>:</p>
<ul class="simple">
<li><p>In a single embedding layer, it is assumed that input integers represent unique feature IDs, which are mapped to unique embedding vectors.
All the embedding vectors in a single embedding layer must have the same size. If you want some input categorical features to have different embedding vector sizes, use multiple embedding layers.</p></li>
<li><p>The input indices’ data type, <code class="docutils literal notranslate"><span class="pre">input_key_type</span></code>, is specified in the solver. By default, the 32-bit integer (I32) is used, but the 64-bit integer type (I64) is also allowed even if it is constrained by the dataset type. For additional information, see <a class="reference internal" href="python_interface.html#solver"><span class="std std-doc">Solver</span></a>.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">SparseEmbedding</span><span class="p">(</span>
            <span class="n">embedding_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Embedding_t</span><span class="o">.</span><span class="n">LocalizedSlotSparseEmbeddingHash</span><span class="p">,</span>
            <span class="n">workspace_size_per_gpu_in_mb</span> <span class="o">=</span> <span class="mi">23</span><span class="p">,</span>
            <span class="n">embedding_vec_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">combiner</span> <span class="o">=</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span>
            <span class="n">sparse_embedding_name</span> <span class="o">=</span> <span class="s2">&quot;sparse_embedding1&quot;</span><span class="p">,</span>
            <span class="n">bottom_name</span> <span class="o">=</span> <span class="s2">&quot;input_data&quot;</span><span class="p">,</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="localizedslotsparseembeddingonehot-layer">
<h3>LocalizedSlotSparseEmbeddingOneHot Layer<a class="headerlink" href="#localizedslotsparseembeddingonehot-layer" title="Permalink to this headline"></a></h3>
<p>The LocalizedSlotSparseEmbeddingOneHot layer stores embeddings in an embedding table and gets them by using a set of integers or indices. The embedding table can be segmented into multiple slots or feature fields, which spans multiple GPUs and nodes. This is a performance-optimized version of LocalizedSlotSparseEmbeddingHash for the case where NVSwitch is available and inputs are one-hot categorical features.</p>
<p><strong>Note</strong>: LocalizedSlotSparseEmbeddingOneHot can only be used together with the Raw dataset format. Unlike other types of embeddings, LocalizedSlotSparseEmbeddingOneHot only supports single-node training and can be used only in a NVSwitch equipped system such as DGX-2 and DGX A100.
The input indices’ data type, <code class="docutils literal notranslate"><span class="pre">input_key_type</span></code>, is specified in the solver. By default, the 32-bit integer (I32) is used, but the 64-bit integer type (I64) is also allowed even if it is constrained by the dataset type. For additional information, see <a class="reference internal" href="python_interface.html#solver"><span class="std std-doc">Solver</span></a>.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">SparseEmbedding</span><span class="p">(</span>
            <span class="n">embedding_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Embedding_t</span><span class="o">.</span><span class="n">LocalizedSlotSparseEmbeddingOneHot</span><span class="p">,</span>
            <span class="n">slot_size_array</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1221</span><span class="p">,</span> <span class="mi">754</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
            <span class="n">embedding_vec_size</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
            <span class="n">combiner</span> <span class="o">=</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span>
            <span class="n">sparse_embedding_name</span> <span class="o">=</span> <span class="s2">&quot;sparse_embedding1&quot;</span><span class="p">,</span>
            <span class="n">bottom_name</span> <span class="o">=</span> <span class="s2">&quot;input_data&quot;</span><span class="p">,</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="dense-layers">
<h2>Dense Layers<a class="headerlink" href="#dense-layers" title="Permalink to this headline"></a></h2>
<p><strong>DenseLayer class</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">DenseLayer</span></code> specifies the parameters related to the dense layer or the loss function. HugeCTR currently supports multiple dense layers and loss functions. Please <strong>NOTE</strong> that the final sigmoid function is fused with the loss function to better utilize memory bandwidth.</p>
<p><strong>Arguments</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">layer_type</span></code>: The layer type to be used. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Add</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.BatchNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Cast</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Concat</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Dropout</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.ELU</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.FmOrder2</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.FusedInnerProduct</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.InnerProduct</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Interaction</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.MultiCross</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.ReLU</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.ReduceSum</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Reshape</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Sigmoid</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Slice</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.WeightMultiply</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.ElementwiseMultiply</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.GRU</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Scale</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.FusedReshapeConcat</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.FusedReshapeConcatGeneral</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Softmax</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.PReLU_Dice</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.ReduceMean</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Sub</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Gather</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.BinaryCrossEntropyLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.CrossEntropyLoss</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.MultiCrossEntropyLoss</span></code>. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bottom_names</span></code>: List[str], the list of bottom tensor names to be consumed by this dense layer. Each name in the list should be the predefined tensor name. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_names</span></code>: List[str], the list of top tensor names, which specify the output tensors of this dense layer. There is NO default value and it should be specified by users.</p></li>
<li><p>For details about the usage of each layer type and its parameters, please refer to <a class="reference internal" href="#dense-layers-usage"><span class="std std-doc">Dense Layers Usage</span></a>.</p></li>
</ul>
</div>
<div class="section" id="dense-layers-usage">
<h2>Dense Layers Usage<a class="headerlink" href="#dense-layers-usage" title="Permalink to this headline"></a></h2>
<div class="section" id="fullyconnected-layer">
<h3>FullyConnected Layer<a class="headerlink" href="#fullyconnected-layer" title="Permalink to this headline"></a></h3>
<p>The FullyConnected layer is a densely connected layer (or MLP layer). It is usually made of a <code class="docutils literal notranslate"><span class="pre">InnerProduct</span></code> layer and a <code class="docutils literal notranslate"><span class="pre">ReLU</span></code>.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_output</span></code>: Integer, the number of output elements for the <code class="docutils literal notranslate"><span class="pre">InnerProduct</span></code> or <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> layer. The default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_init_type</span></code>: Specifies how to initialize the weight array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias_init_type</span></code>: Specifies how to initialize the bias array for the <code class="docutils literal notranslate"><span class="pre">InnerProduct</span></code>, <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> or <code class="docutils literal notranslate"><span class="pre">MultiCross</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: (batch_size, num_output)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">InnerProduct</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;relu1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc2&quot;</span><span class="p">],</span>
                            <span class="n">num_output</span><span class="o">=</span><span class="mi">1024</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;relu2&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="fusedfullyconnected-layer">
<h3>FusedFullyConnected Layer<a class="headerlink" href="#fusedfullyconnected-layer" title="Permalink to this headline"></a></h3>
<p>The FusedFullyConnected layer fuses a common case where FullyConnectedLayer and ReLU are used together to save memory bandwidth.</p>
<p><strong>Note</strong>: This layer can only be used with Mixed Precision mode enabled.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_output</span></code>: Integer, the number of output elements for the <code class="docutils literal notranslate"><span class="pre">InnerProduct</span></code> or <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> layer. The default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_init_type</span></code>: Specifies how to initialize the weight array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias_init_type</span></code>: Specifies how to initialize the bias array for the <code class="docutils literal notranslate"><span class="pre">InnerProduct</span></code>, <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> or <code class="docutils literal notranslate"><span class="pre">MultiCross</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.
Input and Output Shapes:</p></li>
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: (batch_size, num_output)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">FusedInnerProduct</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc2&quot;</span><span class="p">],</span>
                            <span class="n">num_output</span><span class="o">=</span><span class="mi">1024</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="multicross-layer">
<h3>MultiCross Layer<a class="headerlink" href="#multicross-layer" title="Permalink to this headline"></a></h3>
<p>The MultiCross layer is a cross network where explicit feature crossing is applied across cross layers.</p>
<p><strong>Note</strong>: This layer doesn’t currently support Mixed Precision mode.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_layers</span></code>: Integer, number of cross layers in the cross network. It should be set as a positive number if you want to use the cross network. The default value is 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_init_type</span></code>: Specifies how to initialize the weight array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias_init_type</span></code>: Specifies how to initialize the bias array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">MultiCross</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice11&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;multicross1&quot;</span><span class="p">],</span>
                            <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="fmorder2-layer">
<h3>FmOrder2 Layer<a class="headerlink" href="#fmorder2-layer" title="Permalink to this headline"></a></h3>
<p>TheFmOrder2 layer is the second-order factorization machine (FM), which models linear and pairwise interactions as dot products of latent vectors.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">out_dim</span></code>: Integer, the output vector size. It should be set as a positive number if you want to use factorization machine. The default value is 0.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: (batch_size, out_dim)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">FmOrder2</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice32&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fmorder2&quot;</span><span class="p">],</span>
                            <span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="weightmultiply-layer">
<h3>WeightMultiply Layer<a class="headerlink" href="#weightmultiply-layer" title="Permalink to this headline"></a></h3>
<p>The Multiply Layer maps input elements into a latent vector space by multiplying each feature with a corresponding weight vector.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weight_dims</span></code>: List[Integer], the shape of the weight matrix (slot_dim, vec_dim) where vec_dim corresponds to the latent vector length for the <code class="docutils literal notranslate"><span class="pre">WeightMultiply</span></code> layer. It should be set correctly if you want to employ the weight multiplication. The default value is [].</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_init_type</span></code>: Specifies how to initialize the weight array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, slot_dim) where slot_dim represents the number of input features</p></li>
<li><p>output: (batch_size, slot_dim * vec_dim)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice32&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fmorder2&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">10</span><span class="p">]),</span>
                            <span class="n">weight_init_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Initializer_t</span><span class="o">.</span><span class="n">XavierUniform</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="elementwisemultiply-layer">
<h3>ElementwiseMultiply Layer<a class="headerlink" href="#elementwisemultiply-layer" title="Permalink to this headline"></a></h3>
<p>The ElementwiseMultiply Layer maps two inputs into a single resulting vector by performing an element-wise multiplication of the two inputs.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: 2x(batch_size, num_elem)</p></li>
<li><p>output: (batch_size, num_elem)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ElementwiseMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice1&quot;</span><span class="p">,</span><span class="s2">&quot;slice2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;eltmultiply1&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="batchnorm-layer">
<h3>BatchNorm Layer<a class="headerlink" href="#batchnorm-layer" title="Permalink to this headline"></a></h3>
<p>The BatchNorm layer implements a cuDNN based batch normalization.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">factor</span></code>: Float, exponential average factor such as runningMean = runningMean*(1-factor) + newMean*factor for the <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> layer. The default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eps</span></code>: Float, epsilon value used in the batch normalization formula for the <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> layer. The default value is 1e-5.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gamma_init_type</span></code>: Specifies how to initialize the gamma (or scale) array for the <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">beta_init_type</span></code>: Specifies how to initialize the beta (or offset) array for the <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elem)</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice32&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fmorder2&quot;</span><span class="p">],</span>
                            <span class="n">factor</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.00001</span><span class="p">,</span>
                            <span class="n">gamma_init_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Initializer_t</span><span class="o">.</span><span class="n">XavierUniform</span><span class="p">,</span>
                            <span class="n">beta_init_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Initializer_t</span><span class="o">.</span><span class="n">XavierUniform</span><span class="p">)</span>
</pre></div>
</div>
<p>When training a model, each BatchNorm layer stores mean and variance in a JSON file using the following format:
“snapshot_prefix” + “<em>dense</em>” + str(iter) + ”.model”</p>
<p>Example: my_snapshot_dense_5000.model<br></p>
<p>In the JSON file, you can find the batch norm parameters as shown below:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;layers&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">        </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;BatchNorm&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;mean&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">-0.192325</span><span class="p">,</span><span class="w"> </span><span class="mf">0.003050</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.323447</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.034817</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.091861</span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;var&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.738942</span><span class="p">,</span><span class="w"> </span><span class="mf">0.410794</span><span class="p">,</span><span class="w"> </span><span class="mf">1.370279</span><span class="p">,</span><span class="w"> </span><span class="mf">1.156337</span><span class="p">,</span><span class="w"> </span><span class="mf">0.638146</span><span class="p">]</span><span class="w"></span>
<span class="w">        </span><span class="p">},</span><span class="w"></span>
<span class="w">        </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;BatchNorm&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;mean&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">-0.759954</span><span class="p">,</span><span class="w"> </span><span class="mf">0.251507</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.648882</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.176316</span><span class="p">,</span><span class="w"> </span><span class="mf">0.515163</span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;var&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">1.434012</span><span class="p">,</span><span class="w"> </span><span class="mf">1.422724</span><span class="p">,</span><span class="w"> </span><span class="mf">1.001451</span><span class="p">,</span><span class="w"> </span><span class="mf">1.756962</span><span class="p">,</span><span class="w"> </span><span class="mf">1.126412</span><span class="p">]</span><span class="w"></span>
<span class="w">        </span><span class="p">},</span><span class="w"></span>
<span class="w">        </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;BatchNorm&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;mean&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.851878</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.837513</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.694674</span><span class="p">,</span><span class="w"> </span><span class="mf">0.791046</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.849544</span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;var&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">1.694500</span><span class="p">,</span><span class="w"> </span><span class="mf">5.405566</span><span class="p">,</span><span class="w"> </span><span class="mf">4.211646</span><span class="p">,</span><span class="w"> </span><span class="mf">1.936811</span><span class="p">,</span><span class="w"> </span><span class="mf">5.659098</span><span class="p">]</span><span class="w"></span>
<span class="w">        </span><span class="p">}</span><span class="w"></span>
<span class="w">      </span><span class="p">]</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</div>
<div class="section" id="concat-layer">
<h3>Concat Layer<a class="headerlink" href="#concat-layer" title="Permalink to this headline"></a></h3>
<p>The Concat layer concatenates a list of inputs.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">axis</span></code>:  Integer, the dimension to concat for the <code class="docutils literal notranslate"><span class="pre">Concat</span></code> layer. If the input is N-dimensional, 0 &lt;= axis &lt; N. The default value is 1.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: 3D: {(batch_size, num_feas_0, num_elems_0), (batch_size, num_feas + 1, num_elems_1), …} or 2D: {(batch_size, num_elems_0), (batch_size, num_elems_1), …}</p></li>
<li><p>output: 3D and axis=1: (batch_size, num_feas_0+num_feas_1+…, num_elems). 3D and axis=2: (batch_size, num_feas, num_elems_0+num_elems_1+…). 2D: (batch_size, num_elems_0+num_elems_1+…)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Concat</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reshape3&quot;</span><span class="p">,</span><span class="s2">&quot;weight_multiply2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;concat2&quot;</span><span class="p">],</span>
                            <span class="n">axis</span> <span class="o">=</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="reshape-layer">
<h3>Reshape Layer<a class="headerlink" href="#reshape-layer" title="Permalink to this headline"></a></h3>
<p>The Reshape layer reshapes a 3D input tensor into 2D shape.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">leading_dim</span></code>: Integer, the innermost dimension of the output tensor. It must be the multiple of the total number of input elements. If it is unspecified, n_slots * num_elems (see below) is used as the default leading_dim.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">time_step</span></code>: Integer, the second dimension of the 3D output tensor. It must be the multiple of the total number of input elements and must be defined with leading_dim.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">selected</span></code>: Boolean, whether to use the selected mode for the <code class="docutils literal notranslate"><span class="pre">Reshape</span></code> layer. The default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">selected_slots</span></code>: List[int], the selected slots for the <code class="docutils literal notranslate"><span class="pre">Reshape</span></code> layer. It will be ignored if <code class="docutils literal notranslate"><span class="pre">selected</span></code> is False. The default value is [].</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, n_slots, num_elems)</p></li>
<li><p>output: (tailing_dim, leading_dim) where tailing_dim is batch_size * n_slots * num_elems / leading_dim</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Reshape</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sparse_embedding1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reshape1&quot;</span><span class="p">],</span>
                            <span class="n">leading_dim</span><span class="o">=</span><span class="mi">416</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="slice-layer">
<h3>Slice Layer<a class="headerlink" href="#slice-layer" title="Permalink to this headline"></a></h3>
<p>The Slice layer extracts multiple output tensors from a 2D input tensors.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ranges</span></code>: List[Tuple[int, int]], used for the Slice layer. A list of tuples in which each one represents a range in the input tensor to generate the corresponding output tensor. For example, (2, 8) indicates that 6 elements starting from the second element in the input tensor are used to create an output tensor. Note that the start index is inclusive and the end index is exclusive. The number of tuples corresponds to the number of output tensors. Ranges are allowed to overlap unless it is a reverse or negative range. The default value is [].</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elems)</p></li>
<li><p>output: {(batch_size, b-a), (batch_size, d-c), …) where ranges ={[a, b), [c, d), …} and len(ranges) &lt;= 5</p></li>
</ul>
<p>Example:</p>
<p>You can apply the Slice layer to actually slicing a tensor. In this case, it must be explicitly added with Python API.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Slice</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice21&quot;</span><span class="p">,</span> <span class="s2">&quot;slice22&quot;</span><span class="p">],</span>
                            <span class="n">ranges</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">),(</span><span class="mi">10</span><span class="p">,</span><span class="mi">13</span><span class="p">)]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice21&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply1&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice22&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply2&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<p>The Slice layer can also be employed to create copies of a tensor, which helps to express a branch topology in your model graph.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Slice</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice21&quot;</span><span class="p">,</span> <span class="s2">&quot;slice22&quot;</span><span class="p">],</span>
                            <span class="n">ranges</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="mi">13</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">13</span><span class="p">)]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice21&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply1&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">10</span><span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice22&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply2&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<p>From HugeCTR v.3.3, the aforementioned, Slice layer based branching can be abstracted away. When the same tensor is referenced multiple times in constructing a model in Python, the HugeCTR parser can internally add a Slice layer to handle such a situation. Thus, the example below behaves as the same as the one above whilst simplifying the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply1&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">10</span><span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply2&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="dropout-layer">
<h3>Dropout Layer<a class="headerlink" href="#dropout-layer" title="Permalink to this headline"></a></h3>
<p>The Dropout layer randomly zeroizes or drops some of the input elements.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dropout_rate</span></code>: Float, The dropout rate to be used for the <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> layer. It should be between 0 and 1. Setting it to 1 indicates that there is no dropped element at all. The default value is 0.5.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elems)</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Dropout</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;relu1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dropout1&quot;</span><span class="p">],</span>
                            <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="elu-layer">
<h3>ELU Layer<a class="headerlink" href="#elu-layer" title="Permalink to this headline"></a></h3>
<p>The ELU layer represents the Exponential Linear Unit.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">elu_alpha</span></code>: Float, the scalar that decides the value where this ELU function saturates for negative values. The default value is 1.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ELU</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;elu1&quot;</span><span class="p">],</span>
                            <span class="n">elu_alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="relu-layer">
<h3>ReLU Layer<a class="headerlink" href="#relu-layer" title="Permalink to this headline"></a></h3>
<p>The ReLU layer represents the Rectified Linear Unit.</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;relu1&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="sigmoid-layer">
<h3>Sigmoid Layer<a class="headerlink" href="#sigmoid-layer" title="Permalink to this headline"></a></h3>
<p>The Sigmoid layer represents the Sigmoid Unit.</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sigmoid1&quot;</span><span class="p">]))</span>
</pre></div>
</div>
<p><strong>Note</strong>: The final sigmoid function is fused with the loss function to better utilize memory bandwidth, so do NOT add a Sigmoid layer before the loss layer.</p>
</div>
<div class="section" id="interaction-layer">
<h3>Interaction Layer<a class="headerlink" href="#interaction-layer" title="Permalink to this headline"></a></h3>
<p>The interaction layer is used to explicitly capture second-order interactions between features.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: {(batch_size, num_elems), (batch_size, num_feas, num_elems)} where the first tensor typically represents a fully connected layer and the second is an embedding.</p></li>
<li><p>output: (batch_size, output_dim) where output_dim = num_elems + (num_feas + 1) * (num_feas + 2 ) / 2 - (num_feas + 1) + 1</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Interaction</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;layer1&quot;</span><span class="p">,</span> <span class="s2">&quot;layer3&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;interaction1&quot;</span><span class="p">]))</span>
</pre></div>
</div>
<p><strong>Important Notes</strong>:
There are optimizations that can be employed on the <code class="docutils literal notranslate"><span class="pre">Interaction</span></code> layer and the following <code class="docutils literal notranslate"><span class="pre">GroupFusedInnerProduct</span></code> layer during fp16 training. In this case, you should specify two output tensor names for the <code class="docutils literal notranslate"><span class="pre">Interaction</span></code> layer, and use them as the input tensors for the following <code class="docutils literal notranslate"><span class="pre">GroupFusedInnerProduct</span></code> layer. Please refer to the example of <a class="reference internal" href="python_interface.html#groupdenselayer"><span class="std std-doc">GroupDenseLayer</span></a> for the detailed usage.</p>
</div>
<div class="section" id="add-layer">
<h3>Add Layer<a class="headerlink" href="#add-layer" title="Permalink to this headline"></a></h3>
<p>The Add layer adds up an arbitrary number of tensors that have the same size in an element-wise manner.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: Nx(batch_size, num_elems) where N is the number of input tensors</p></li>
<li><p>output: (batch_size, num_elems)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Add</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc4&quot;</span><span class="p">,</span> <span class="s2">&quot;reducesum1&quot;</span><span class="p">,</span> <span class="s2">&quot;reducesum2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="reducesum-layer">
<h3>ReduceSum Layer<a class="headerlink" href="#reducesum-layer" title="Permalink to this headline"></a></h3>
<p>The ReduceSum Layer sums up all the elements across a specified dimension.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">axis</span></code>:  Integer, the dimension to reduce for the <code class="docutils literal notranslate"><span class="pre">ReduceSum</span></code> layer. If the input is N-dimensional, 0 &lt;= axis &lt; N. The default value is 1.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, …) where … represents any number of elements with an arbitrary number of dimensions</p></li>
<li><p>output: Dimension corresponding to axis is set to 1. The others remain the same as the input.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fmorder2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reducesum1&quot;</span><span class="p">],</span>
                            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="section" id="gru-layer">
<h4>GRU Layer<a class="headerlink" href="#gru-layer" title="Permalink to this headline"></a></h4>
<p>The GRU layer is Gated Recurrent Unit.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_output</span></code>: Number of output elements.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batchsize</span></code>: Number of batchsize.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SeqLength</span></code>: Length of the sequence.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vector_size</span></code>: size of the input vector.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_init_type</span></code>: Specifies how to initialize the weight array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias_init_type</span></code>: Specifies how to initialize the bias array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (1, batch_size<em>SeqLength</em>embedding_vec_size)</p></li>
<li><p>output: (1, batch_size<em>SeqLength</em>embedding_vec_size)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">GRU</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;GRU1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;conncat1&quot;</span><span class="p">],</span>
                            <span class="n">num_output</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                            <span class="n">batchsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span>
                            <span class="n">SeqLength</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                            <span class="n">vector_size</span><span class="o">=</span><span class="mi">20</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="preludice-layer">
<h4>PReLUDice Layer<a class="headerlink" href="#preludice-layer" title="Permalink to this headline"></a></h4>
<p>The PReLUDice layer represents the Parametric Rectified Linear Unit, which adaptively adjusts the rectified point according to distribution of input data.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">elu_alpha</span></code>: A scalar that decides the value where this activation function saturates for negative values.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eps</span></code>: Epsilon value used in the PReLU/Dice formula.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">PReLU_Dice</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc_din_i1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dice_1&quot;</span><span class="p">],</span>
                            <span class="n">elu_alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="scale-layer">
<h4>Scale Layer<a class="headerlink" href="#scale-layer" title="Permalink to this headline"></a></h4>
<p>The Scale layer scales the input 2D tensor to specific size on the designate axis.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">axis</span></code>: Along the designate axis to scale the tensor. The designate axis could be axis 0, 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">factor</span> </code>: scale factor.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elems)</p></li>
<li><p>output: if axis = 0; (batch_size, num_elems * factor), if axis = 1; (batch_size * factor, num_elems)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Scale</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;item1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Scale_item&quot;</span><span class="p">],</span>
                            <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">factor</span> <span class="o">=</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="fusedreshapeconcat-layer">
<h4>FusedReshapeConcat Layer<a class="headerlink" href="#fusedreshapeconcat-layer" title="Permalink to this headline"></a></h4>
<p>The FusedReshapeConcat layer cross combines the input tensors and outputs item tensor, AD tensor.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: {(batch_size, num_feas + 1, num_elems_0), (batch_size, num_feas + 1, num_elems_1), …}, the input tensors are embeddings.</p></li>
<li><p>output: {(batch_size x num_feas, (num_elems_0 + num_elems_1 + …)), (batch_size, (num_elems_0 + num_elems_1 + …))}.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">FusedReshapeConcat</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sparse_embedding_good&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_embedding_cate&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;FusedReshapeConcat_item_his_em&quot;</span><span class="p">,</span> <span class="s2">&quot;FusedReshapeConcat_item&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="fusedreshapeconcatgeneral-layer">
<h4>FusedReshapeConcatGeneral Layer<a class="headerlink" href="#fusedreshapeconcatgeneral-layer" title="Permalink to this headline"></a></h4>
<p>The FusedReshapeConcatGeneral layer cross combines the input tensors and outputs item tensor, AD tensor.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: {(batch_size, num_feas, num_elems_0), (batch_size, num_feas, num_elems_1), …}, the input tensors are embeddings.</p></li>
<li><p>output: (batch_size x num_feas, (num_elems_0 + num_elems_1 + …)).</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">FusedReshapeConcatGeneral</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sparse_embedding_good&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_embedding_cate&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;FusedReshapeConcat_item_his_em&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="softmax-layer">
<h4>Softmax Layer<a class="headerlink" href="#softmax-layer" title="Permalink to this headline"></a></h4>
<p>The Softmax layer computes softmax activations.</p>
<p>Parameter: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elems)</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Softmax</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reshape1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;softmax_i&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="sub-layer">
<h4>Sub Layer<a class="headerlink" href="#sub-layer" title="Permalink to this headline"></a></h4>
<p>Inputs: x tensor, y tensor in same size.
Produce x - y in element wise manner.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: Nx(batch_size, num_elems) where N is the number of input tensors</p></li>
<li><p>output: (batch_size, num_elems)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Sub</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Scale_item1&quot;</span><span class="p">,</span> <span class="s2">&quot;item_his1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sub_ih&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="reducemean-layer">
<h4>ReduceMean Layer<a class="headerlink" href="#reducemean-layer" title="Permalink to this headline"></a></h4>
<p>The ReduceMean Layer computes the mean of elements across a specified dimension.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">axis</span></code>: The dimension to reduce. If the input is N-dimensional, 0 &lt;= axis &lt; N.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, …) where … represents any number of elements with an arbitrary number of dimensions</p></li>
<li><p>output: Dimension corresponding to axis is set to 1. The others remain the same as the input.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fmorder2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reducemean1&quot;</span><span class="p">],</span>
                            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="matrixmutiply-layer">
<h4>MatrixMutiply Layer<a class="headerlink" href="#matrixmutiply-layer" title="Permalink to this headline"></a></h4>
<p>The MatrixMutiply Layer is a binary operation that produces a matrix output from two matrix inputs by performing matrix mutiplication.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: 2D: (m, n), (n, k) or 3D: (batch_size, m, n), (batch_size, n, k)</p></li>
<li><p>output: 2D: (m, k) or 3D: (batch_size, m, k)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">MatrixMutiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice1&quot;</span><span class="p">,</span><span class="s2">&quot;slice2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;MatrixMutiply1&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="gather-layer">
<h4>Gather Layer<a class="headerlink" href="#gather-layer" title="Permalink to this headline"></a></h4>
<p>The Gather layer gather multiple output tensor slices from an input tensors on the last dimension.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">indices</span></code>: A list of indices in which each one represents an index in the input tensor to generate the corresponding output tensor. For example, [2, 8] indicates the second and eights tensor slice in the input tensor which are used to create an output tensor.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elems)</p></li>
<li><p>output: (num_indices, num_elems)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Gather</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reshape1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gather1&quot;</span><span class="p">],</span>
                            <span class="n">indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="binarycrossentropyloss">
<h3>BinaryCrossEntropyLoss<a class="headerlink" href="#binarycrossentropyloss" title="Permalink to this headline"></a></h3>
<p>BinaryCrossEntropyLoss calculates loss from labels and predictions where each label is binary. The final sigmoid function is fused with the loss function to better utilize memory bandwidth.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code>: Boolean, whether to use regulariers. THe default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">regularizer_type</span></code>: The regularizer type for the <code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropyLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> or <code class="docutils literal notranslate"><span class="pre">MultiCrossEntropyLoss</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L2</span></code>. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code> is False. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lambda</span></code>: Float, the lambda value of the regularization term. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularier</span></code> is False. The default value is 0.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: [(batch_size, 1), (batch_size, 1)] where the first tensor represents the predictions while the second tensor represents the labels</p></li>
<li><p>output: (batch_size, 1)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">BinaryCrossEntropyLoss</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="crossentropyloss">
<h3>CrossEntropyLoss<a class="headerlink" href="#crossentropyloss" title="Permalink to this headline"></a></h3>
<p>CrossEntropyLoss calculates loss from labels and predictions between the forward propagation phases and backward propagation phases. It assumes that each label is two-dimensional.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code>: Boolean, whether to use regulariers. THe default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">regularizer_type</span></code>: The regularizer type for the <code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropyLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> or <code class="docutils literal notranslate"><span class="pre">MultiCrossEntropyLoss</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L2</span></code>. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code> is False. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lambda</span></code>: Float, the lambda value of the regularization term. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularier</span></code> is False. The default value is 0.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: [(batch_size, 2), (batch_size, 2)] where the first tensor represents the predictions while the second tensor represents the labels</p></li>
<li><p>output: (batch_size, 2)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span>
                            <span class="n">use_regularizer</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                            <span class="n">regularizer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Regularizer_t</span><span class="o">.</span><span class="n">L2</span><span class="p">,</span>
                            <span class="k">lambda</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="multicrossentropyloss">
<h3>MultiCrossEntropyLoss<a class="headerlink" href="#multicrossentropyloss" title="Permalink to this headline"></a></h3>
<p>MultiCrossEntropyLoss calculates loss from labels and predictions between the forward propagation phases and backward propagation phases. It allows labels in an arbitrary dimension, but all the labels must be in the same shape.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code>: Boolean, whether to use regulariers. THe default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">regularizer_type</span></code>: The regularizer type for the <code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropyLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> or <code class="docutils literal notranslate"><span class="pre">MultiCrossEntropyLoss</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L2</span></code>. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code> is False. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lambda</span></code>: Float, the lambda value of the regularization term. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularier</span></code> is False. The default value is 0.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: [(batch_size, *), (batch_size, *)] where the first tensor represents the predictions while the second tensor represents the labels. * represents any even number of elements.</p></li>
<li><p>output: (batch_size, *)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">MultiCrossEntropyLoss</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span>
                            <span class="n">use_regularizer</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                            <span class="n">regularizer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Regularizer_t</span><span class="o">.</span><span class="n">L1</span><span class="p">,</span>
                            <span class="k">lambda</span> <span class="o">=</span> <span class="mf">0.1</span>
                            <span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="python_interface.html" class="btn btn-neutral float-left" title="HugeCTR Python Interface" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../additional_resources.html" class="btn btn-neutral float-right" title="Additional Resources" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v3.6
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../v3.5/api/hugectr_layer_book.html">v3.5</a></dd>
      <dd><a href="hugectr_layer_book.html">v3.6</a></dd>
      <dd><a href="../../v3.7/api/hugectr_layer_book.html">v3.7</a></dd>
      <dd><a href="../../v3.8/api/hugectr_layer_book.html">v3.8</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../master/api/hugectr_layer_book.html">master</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>