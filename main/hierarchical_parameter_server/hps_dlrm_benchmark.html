<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Benchmark HPS-integrated DLRM &mdash; Merlin HugeCTR  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="canonical" href="https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_dlrm_benchmark.html" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sparse Operation Kit" href="../sparse_operation_kit.html" />
    <link rel="prev" title="HPS Plugin" href="hps_trt_api/hps_plugin.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Beginning in January 2023, versions for all NVIDIA Merlin projects
      will change from semantic versioning like <code>4.0</code>
      to calendar versioning like <code>23.01</code>.</p>
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">HUGECTR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_embedding_training_cache.html">Embedding Training Cache</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Hierarchical Parameter Server</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="hps_database_backend.html">HPS Database Backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_tf_user_guide.html">HPS Plugin for TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_trt_user_guide.html">HPS Plugin for TensorRT</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Benchmark HPS-integrated DLRM</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../sparse_operation_kit.html">Sparse Operation Kit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Hierarchical Parameter Server</a></li>
      <li class="breadcrumb-item active">Benchmark HPS-integrated DLRM</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="tex2jax_ignore mathjax_ignore section" id="benchmark-hps-integrated-dlrm">
<h1>Benchmark HPS-integrated DLRM<a class="headerlink" href="#benchmark-hps-integrated-dlrm" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#benchmark-setup" id="id1">Benchmark Setup</a></p></li>
<li><p><a class="reference internal" href="#results" id="id2">Results</a></p></li>
<li><p><a class="reference internal" href="#resources" id="id3">Resources</a></p></li>
</ul>
</div>
<div class="section" id="benchmark-setup">
<h2>Benchmark Setup<a class="headerlink" href="#benchmark-setup" title="Permalink to this headline"></a></h2>
<p>We create the DLRM model with native TensorFlow and its counterpart with HPS Plugin for TensorFlow using the <a class="reference download internal" download="" href="../_downloads/dbb8b96b5782fa5bb7367e4c0cc75722/create_tf_models.py"><span class="xref download myst">create_tf_models.py</span></a> script. The DLRM model with native TF is in the SavedModel format and the size is about 16GB, which is almost the size of embedding weights because the size of dense layer weights is small. The DLRM model with the plugin leverages HPS to store the embedding table and perform embedding lookup. The JSON configuration file and the embedding table file required by HPS are also generated by the script.</p>
<p>Furthermore, we build HPS-integrated TensorRT engines for the DLRM model using the <a class="reference download internal" download="" href="../_downloads/e858fb50f7c9701eb1852ee88cd4ab31/create_trt_engines.py"><span class="xref download myst">create_trt_engines.py</span></a> script, in both fp32 and fp16 modes. The workflow can be summarized as three steps: convert TF SavedModel to ONNX, perform ONNX graph surgery to insert HPS plugin layer and build the HPS-integrated TensorRT engines.</p>
<p>We compare three deployment methods on the Triton Inference Server:</p>
<ul class="simple">
<li><p><strong>DLRM with Native TensorFlow</strong>: The experimental option <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/VariablePolicy">VariablePolicy.SAVE_VARIABLE_DEVICES</a> is used to enable the CPU and GPU hybrid deployment of the DLRM SavedModel, i.e., the embedding table is on CPU while the MLP layers are on GPU. This deployment method is common for native TF models with large embedding tables and can be regarded as the baseline of this benchmark. The deployment is on the Triton backend for TensorFlow.</p></li>
<li><p><strong>DLRM with HPS Plugin for TensorFlow</strong>: In this DLRM SavedModel, <code class="docutils literal notranslate"><span class="pre">tf.nn.embedding_lookup</span></code> is replaced by <code class="docutils literal notranslate"><span class="pre">hps.LookupLayer</span></code> to perform embedding lookup and the MLP layers are kept unchanged. The deployment is on the Triton backend for TensorFlow.</p></li>
<li><p><strong>DLRM with HPS Plugin for TensorRT</strong>: The HPS plugin layer is integrated into the built TensorRT engines, and the MLP layers are accelerated by TensorRT. The TensorRT engines are built with mininum batch size 1, optimum 1024 and maximum 131072. Both fp32 and fp16 modes are investigated. The deployment is on the Triton backend for TensorRT.</p></li>
</ul>
<p>The benchmark is conducted on the <code class="docutils literal notranslate"><span class="pre">A100-SXM4-80GB</span></code> GPU with one Triton model instance on it. The GPU embedding cache of HPS is turned on and the cache percentage is configured as <code class="docutils literal notranslate"><span class="pre">0.2</span></code>. For details about how to deploy HPS-integrated TF models and TRT engines on Triton, please refer to <span class="xref myst">hps_tensorflow_triton_deployment_demo.ipynb</span> and <span class="xref myst">demo_for_tf_trained_model.ipynb</span>.</p>
<p>After launching the Triton Inference Server, we send the same batch of inference data repeatedly using Triton Performance Analyzer. In this case, the embedding lookup is served by the GPU embedding cache of HPS and the best-case performance of HPS can be studied. The command and the sample data to measure the latency for batch size 1 can be found below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>perf_analyzer -m <span class="si">${</span><span class="nv">MODEL_NAME</span><span class="si">}</span> -u localhost:8000 --input-data <span class="m">1</span>.json --shape categorical_features:1,26 --shape numerical_features:1,13
</pre></div>
</div>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="w"></span>
<span class="nt">&quot;data&quot;</span><span class="p">:[</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="nt">&quot;categorical_features&quot;</span><span class="p">:[</span><span class="mi">276633</span><span class="p">,</span><span class="mi">7912898</span><span class="p">,</span><span class="mi">7946796</span><span class="p">,</span><span class="mi">7963854</span><span class="p">,</span><span class="mi">7971191</span><span class="p">,</span><span class="mi">7991237</span><span class="p">,</span><span class="mi">7991368</span><span class="p">,</span><span class="mi">7998351</span><span class="p">,</span><span class="mi">7999728</span><span class="p">,</span><span class="mi">8014930</span><span class="p">,</span><span class="mi">13554004</span><span class="p">,</span><span class="mi">14136456</span><span class="p">,</span><span class="mi">14382203</span><span class="p">,</span><span class="mi">14382219</span><span class="p">,</span><span class="mi">14384425</span><span class="p">,</span><span class="mi">14395091</span><span class="p">,</span><span class="mi">14395194</span><span class="p">,</span><span class="mi">14395215</span><span class="p">,</span><span class="mi">14396165</span><span class="p">,</span><span class="mi">14671338</span><span class="p">,</span><span class="mi">22562171</span><span class="p">,</span><span class="mi">25307802</span><span class="p">,</span><span class="mi">32394527</span><span class="p">,</span><span class="mi">32697105</span><span class="p">,</span><span class="mi">32709007</span><span class="p">,</span><span class="mi">32709104</span><span class="p">],</span><span class="w"></span>
<span class="nt">&quot;numerical_features&quot;</span><span class="p">:[</span><span class="mf">3.76171875</span><span class="p">,</span><span class="mf">3.806640625</span><span class="p">,</span><span class="mf">1.609375</span><span class="p">,</span><span class="mf">4.04296875</span><span class="p">,</span><span class="mf">1.7919921875</span><span class="p">,</span><span class="mf">1.0986328125</span><span class="p">,</span><span class="mf">1.0986328125</span><span class="p">,</span><span class="mf">1.609375</span><span class="p">,</span><span class="mf">2.9453125</span><span class="p">,</span><span class="mf">1.0986328125</span><span class="p">,</span><span class="mf">1.38671875</span><span class="p">,</span><span class="mf">8.3984375</span><span class="p">,</span><span class="mf">1.9462890625</span><span class="p">]</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="p">]</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>We take the forward latency at the server side as our benchmark metric, which is reported by the performance analyzer via the <code class="docutils literal notranslate"><span class="pre">compute</span> <span class="pre">infer</span></code> field:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  Server:
    Inference count: <span class="m">28589</span>
    Execution count: <span class="m">28589</span>
    Successful request count: <span class="m">28589</span>
    Avg request latency: <span class="m">562</span> usec <span class="o">(</span>overhead <span class="m">9</span> usec + queue <span class="m">9</span> usec + compute input <span class="m">59</span> usec + compute infer <span class="m">431</span> usec + compute output <span class="m">53</span> usec<span class="o">)</span>
</pre></div>
</div>
</div>
<div class="section" id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline"></a></h2>
<p>The benchmark is conducted with the Merlin TensorFlow container <code class="docutils literal notranslate"><span class="pre">nvcr.io/nvidia/merlin/merlin-tensorflow:23.02</span></code> on a machine with <code class="docutils literal notranslate"><span class="pre">A100-SXM4-80GB</span> <span class="pre">+</span> <span class="pre">2</span> <span class="pre">x</span> <span class="pre">AMD</span> <span class="pre">EPYC</span> <span class="pre">7742</span> <span class="pre">64-Core</span> <span class="pre">Processor</span></code>. The software versions are listed below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>TensorFlow version: <span class="m">2</span>.10.0
Triton version: <span class="m">22</span>.11
TensorRT version: <span class="m">8</span>.5.1-1+cuda11.8
</pre></div>
</div>
<p>The per-batch forward latency, in milliseconds, measured at the server side is shown in the following table and Figure 1. The Y-axis is logarithmic. The FP16 TRT engine with HPS achieves the best performance on almost all batch sizes, and has about 10x speedup to the Native TF baseline on large batch sizes.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Batch size</p></th>
<th class="head"><p>Native TF</p></th>
<th class="head"><p>TF with HPS</p></th>
<th class="head"><p>FP32 TRT with HPS</p></th>
<th class="head"><p>FP16 TRT with HPS</p></th>
<th class="head"><p>Speedup - FP16 TRT with HPS to Native TF</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>32</p></td>
<td><p>551</p></td>
<td><p>612</p></td>
<td><p>380</p></td>
<td><p>389</p></td>
<td><p>1.42</p></td>
</tr>
<tr class="row-odd"><td><p>64</p></td>
<td><p>608</p></td>
<td><p>667</p></td>
<td><p>381</p></td>
<td><p>346</p></td>
<td><p>1.76</p></td>
</tr>
<tr class="row-even"><td><p>256</p></td>
<td><p>832</p></td>
<td><p>639</p></td>
<td><p>438</p></td>
<td><p>428</p></td>
<td><p>1.94</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>1911</p></td>
<td><p>849</p></td>
<td><p>604</p></td>
<td><p>534</p></td>
<td><p>3.58</p></td>
</tr>
<tr class="row-even"><td><p>2048</p></td>
<td><p>4580</p></td>
<td><p>1059</p></td>
<td><p>927</p></td>
<td><p>766</p></td>
<td><p>5.98</p></td>
</tr>
<tr class="row-odd"><td><p>4096</p></td>
<td><p>9872</p></td>
<td><p>1459</p></td>
<td><p>1446</p></td>
<td><p>1114</p></td>
<td><p>8.86</p></td>
</tr>
<tr class="row-even"><td><p>8192</p></td>
<td><p>19643</p></td>
<td><p>2490</p></td>
<td><p>2432</p></td>
<td><p>1767</p></td>
<td><p>11.12</p></td>
</tr>
<tr class="row-odd"><td><p>16384</p></td>
<td><p>35292</p></td>
<td><p>4131</p></td>
<td><p>4355</p></td>
<td><p>3053</p></td>
<td><p>11.56</p></td>
</tr>
<tr class="row-even"><td><p>32768</p></td>
<td><p>54090</p></td>
<td><p>7795</p></td>
<td><p>6816</p></td>
<td><p>5247</p></td>
<td><p>10.31</p></td>
</tr>
<tr class="row-odd"><td><p>65536</p></td>
<td><p>107742</p></td>
<td><p>15036</p></td>
<td><p>13012</p></td>
<td><p>10022</p></td>
<td><p>10.75</p></td>
</tr>
<tr class="row-even"><td><p>131072</p></td>
<td><p>213990</p></td>
<td><p>29374</p></td>
<td><p>25440</p></td>
<td><p>19340</p></td>
<td><p>11.06</p></td>
</tr>
</tbody>
</table>
<a class="reference internal image-reference" href="../_images/hps_dlrm_latency.png"><img alt="The DLRM inference latency for different deployment methods" src="../_images/hps_dlrm_latency.png" style="width: 720px;" /></a>
<div style="text-align:center;">Figure 1. The DLRM inference latency.</div>
<p><br></br></p>
</div>
<div class="section" id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/triton-inference-server/tensorflow_backend">Triton TensorFlow Backend</a></p></li>
<li><p><a class="reference external" href="https://github.com/triton-inference-server/tensorrt_backend">Triton TensorRT Backend</a></p></li>
<li><p><a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/user_guide/perf_analyzer.md">Triton Performance Analyzer</a></p></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hps_trt_api/hps_plugin.html" class="btn btn-neutral float-left" title="HPS Plugin" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../sparse_operation_kit.html" class="btn btn-neutral float-right" title="Sparse Operation Kit" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: main
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../v4.0/index.html">v4.0</a></dd>
      <dd><a href="../../v4.1/index.html">v4.1</a></dd>
      <dd><a href="../../v4.1.1/index.html">v4.1.1</a></dd>
      <dd><a href="../../v4.2/index.html">v4.2</a></dd>
      <dd><a href="../../v4.3/index.html">v4.3</a></dd>
      <dd><a href="../../v4.3.1/index.html">v4.3.1</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="hps_dlrm_benchmark.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>