<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>HugeCTR Example Notebooks &mdash; Merlin HugeCTR  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />

  
    <link rel="canonical" href="https://nvidia-merlin.github.io/HugeCTR/main/notebooks/index.html" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hierarchical Parameter Server Demo" href="hps_demo.html" />
    <link rel="prev" title="Performance" href="../performance.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      After the HugeCTR <code>v23.08</code>, the offline inference  will be deprecated.
      Check out our HPS plugins for TensorRT and TensorFlow as alternatives.</p>
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">HUGECTR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hierarchical_parameter_server/index.html">Hierarchical Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sparse_operation_kit.html">Sparse Operation Kit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Example Notebooks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hps_demo.html">Hierarchical Parameter Server Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="training_with_remote_filesystem.html">HugeCTR Training with Remote File System Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="embedding_collection.html">HugeCTR Embedding Collection</a></li>
<li class="toctree-l2"><a class="reference internal" href="hugectr_e2e_demo_with_nvtabular.html">HugeCTR End-end Example with NVTabular</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">HugeCTR Example Notebooks</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="hugectr-example-notebooks">
<h1>HugeCTR Example Notebooks<a class="headerlink" href="#hugectr-example-notebooks" title="Permalink to this heading"></a></h1>
<p>This directory contains a set of Jupyter notebook that demonstrate how to use HugeCTR.</p>
<p>The simplest way to run a one of our notebooks is with a Docker container.
A container provides a self-contained, isolated, and reproducible environment for repetitive experiments.
Docker images are available from the NVIDIA GPU Cloud (NGC).</p>
<section id="clone-the-hugectr-repository">
<h2>1. Clone the HugeCTR Repository<a class="headerlink" href="#clone-the-hugectr-repository" title="Permalink to this heading"></a></h2>
<p>Use the following command to clone the HugeCTR repository:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/NVIDIA/HugeCTR
</pre></div>
</div>
</section>
<section id="pull-the-ngc-docker-and-run-it">
<h2>2. Pull the NGC Docker and run it<a class="headerlink" href="#pull-the-ngc-docker-and-run-it" title="Permalink to this heading"></a></h2>
<p>Pull the container using the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>nvcr.io/nvidia/merlin/merlin-hugectr:24.04
</pre></div>
</div>
<p>Launch the container in interactive mode (mount the HugeCTR root directory into the container for your convenience) by running this command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--cap-add<span class="w"> </span>SYS_NICE<span class="w"> </span>--ipc<span class="o">=</span>host<span class="w"> </span>--ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1<span class="w"> </span>--ulimit<span class="w"> </span><span class="nv">stack</span><span class="o">=</span><span class="m">67108864</span><span class="w"> </span>-u<span class="w"> </span>root<span class="w"> </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/HugeCTR<span class="w"> </span>-w<span class="w"> </span>/HugeCTR<span class="w"> </span>--network<span class="o">=</span>host<span class="w"> </span>--runtime<span class="o">=</span>nvidia<span class="w"> </span>nvcr.io/nvidia/merlin/merlin-hugectr:24.04
</pre></div>
</div>
<blockquote>
<div><p>To run the  Sparse Operation Kit notebooks, specify the <code class="docutils literal notranslate"><span class="pre">nvcr.io/nvidia/merlin/merlin-tensorflow:24.04</span></code> container.</p>
</div></blockquote>
</section>
<section id="customized-building-optional">
<h2>3. Customized Building (Optional)<a class="headerlink" href="#customized-building-optional" title="Permalink to this heading"></a></h2>
<p>HugeCTR is already installed in the NGC container. But you can also setup HugeCTR from source to customize the build more. This is useful for developmental purposes.</p>
<ol class="arabic simple">
<li><p>Go to HugeCTR repo and update third party modules</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>HugeCTR
$<span class="w"> </span>git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init<span class="w"> </span>--recursive
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>There are options to customize the build using parameters, which are detailed <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hugectr_contributor_guide.html#build-hugectr-training-container-from-source">here</a>
Here are some examples of how you can build HugeCTR using these build options:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example 1</span>
$<span class="w"> </span>mkdir<span class="w"> </span>-p<span class="w"> </span>build<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
$<span class="w"> </span>cmake<span class="w"> </span>-DCMAKE_BUILD_TYPE<span class="o">=</span>Release<span class="w"> </span>-DSM<span class="o">=</span><span class="m">70</span><span class="w"> </span>..<span class="w"> </span><span class="c1"># Target is NVIDIA V100 with all others by default</span>
$<span class="w"> </span>make<span class="w"> </span>-j<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>make<span class="w"> </span>install
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example 2</span>
$<span class="w"> </span>mkdir<span class="w"> </span>-p<span class="w"> </span>build<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
$<span class="w"> </span>cmake<span class="w"> </span>-DCMAKE_BUILD_TYPE<span class="o">=</span>Release<span class="w"> </span>-DSM<span class="o">=</span><span class="s2">&quot;70;80&quot;</span><span class="w"> </span>-DENABLE_MULTINODES<span class="o">=</span>ON<span class="w"> </span>..<span class="w"> </span><span class="c1"># Target is NVIDIA V100 / A100 with the multi-node mode on.</span>
$<span class="w"> </span>make<span class="w"> </span>-j<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>make<span class="w"> </span>install
</pre></div>
</div>
<p>By default, HugeCTR is installed at <code class="docutils literal notranslate"><span class="pre">/usr/local</span></code>. However, you can use <code class="docutils literal notranslate"><span class="pre">CMAKE_INSTALL_PREFIX</span></code> to install HugeCTR to non-default location:</p>
<p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">cmake</span> <span class="pre">-DCMAKE_INSTALL_PREFIX=/opt/HugeCTR</span> <span class="pre">-DSM=70</span> <span class="pre">..</span></code></p>
<p>Refer to the</p>
<blockquote>
<div><p><a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_contributor_guide.html#how-to-start-your-development">How to Start Your Development</a>
documentation for more details on building HugeCTR From Source</p>
</div></blockquote>
</section>
<section id="start-the-jupyter-notebook">
<h2>4. Start the Jupyter Notebook<a class="headerlink" href="#start-the-jupyter-notebook" title="Permalink to this heading"></a></h2>
<ol class="arabic">
<li><p>Start Jupyter using these commands:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/HugeCTR/notebooks
jupyter-notebook<span class="w"> </span>--allow-root<span class="w"> </span>--ip<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8888</span><span class="w"> </span>--NotebookApp.token<span class="o">=</span><span class="s1">&#39;hugectr&#39;</span>
</pre></div>
</div>
</li>
<li><p>Connect to your host machine using the 8888 port by accessing its IP address or name from your web browser: <code class="docutils literal notranslate"><span class="pre">http://[host</span> <span class="pre">machine]:8888</span></code></p>
<p>Use the token available from the output by running the command above to log in. For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">http://[host</span> <span class="pre">machine]:8888/?token=aae96ae9387cd28151868fee318c3b3581a2d794f3b25c6b</span></code></p>
</li>
<li><p>Optional: Import MPI.</p>
<p>By default, HugeCTR initializes and finalizes MPI when you run the <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">hugectr</span></code> statement within the NGC Merlin container.
If you build and install HugeCTR yourself, specify the <code class="docutils literal notranslate"><span class="pre">ENABLE_MULTINODES=ON</span></code> argument when you build.
See <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_user_guide.html#building-hugectr-from-scratch">Build HugeCTR from Source</a>.</p>
<p>If your program uses MPI for a reason other than interacting with HugeCTR, initialize MPI with the <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">mpi4py</span> <span class="pre">import</span> <span class="pre">MPI</span></code> statement before you import HugeCTR.</p>
</li>
<li><p>Important Note:</p>
<p>HugeCTR is written in CUDA/C++ and wrapped to Python using Pybind11. The C++ output will not display in Notebook cells unless you run the Python script in a command line manner.</p>
</li>
</ol>
</section>
<section id="notebook-list">
<h2>Notebook List<a class="headerlink" href="#notebook-list" title="Permalink to this heading"></a></h2>
<p>The notebooks are located within the container and can be found in the <code class="docutils literal notranslate"><span class="pre">/HugeCTR/notebooks</span></code> directory.</p>
<p>Here’s a list of notebooks that you can run:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#hugectr_e2e_demo.ipynb"><span class="xref myst">hugectr_e2e_demo_with_nvtabular.ipynb</span></a>: Notebook to preprocess data using NVTabular, train the model with HugeCTR, and do the offline inference with the HugeCTR HPS.</p></li>
<li><p><a class="reference internal" href="#continuous_training.ipynb"><span class="xref myst">continuous_training.ipynb</span></a> (deprecated): Notebook to introduce how to deploy continued training with HugeCTR.</p></li>
<li><p>~multi_gpu_offline_inference.ipynb~: It was deprecated. Check out <a class="reference internal" href="#hps_trt/notebooks/demo_for_tf_trained_model.ipynb"><span class="xref myst">this HPS TRT notebook</span></a> as an alternative.</p></li>
<li><p><a class="reference internal" href="hps_demo.html"><span class="std std-doc">hps_demo.ipynb</span></a>: Demonstrate how to utilize HPS Python APIs together with ONNX Runtime APIs to create an ensemble inference model.</p></li>
<li><p><a class="reference internal" href="#training_and_inference_with_remote_filesystem.ipynb"><span class="xref myst">training_and_inference_with_remote_filesystem.ipynb</span></a>: Demonstrates how to train a model with data that is stored in a remote file system such as Hadoop HDFS and AWS S3.</p></li>
</ul>
<p>The <a class="reference internal" href="#./multi-modal-data/"><span class="xref myst">multi-modal-data</span></a> series of notebooks demonstrate how to use of multi-modal data such as text and images for the task of movie recommendation.
The notebooks use the Movielens-25M dataset.</p>
<p>More notebooks on the Hierarchical Parameter Server (HPS) are available with its <a class="reference internal" href="#hps_trt/notebooks"><span class="xref myst">TensorRT</span></a> and <a class="reference internal" href="#hps_tf/notebooks"><span class="xref myst">Tensorflow</span></a> plugins.</p>
<p>For Sparse Operation Kit notebooks, refer to the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/master/sparse_operation_kit/notebooks">sparse_operation_kit/notebooks/</a> directory of the repository or the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/index.html">documentation</a>.</p>
</section>
<section id="system-specifications">
<h2>System Specifications<a class="headerlink" href="#system-specifications" title="Permalink to this heading"></a></h2>
<p>The specifications of the system on which each notebook can run successfully are summarized in the table. The notebooks are verified on the system below but it does not mean the minimum requirements.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Notebook</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>#GPUs</p></th>
<th class="head"><p>Author</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#multi-modal-data"><span class="xref myst">multi-modal-data</span></a></p></td>
<td><p>Intel® Xeon® CPU E5-2698 v4 &#64; 2.20GHz<br />512 GB Memory</p></td>
<td><p>Tesla V100-SXM2-32GB<br />32 GB Memory</p></td>
<td><p>1</p></td>
<td><p>Vinh Nguyen</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#continuous_training.ipynb"><span class="xref myst">continuous_training.ipynb</span></a></p></td>
<td><p>Intel® Xeon® CPU E5-2698 v4 &#64; 2.20GHz<br />512 GB Memory</p></td>
<td><p>Tesla V100-SXM2-32GB<br />32 GB Memory</p></td>
<td><p>1</p></td>
<td><p>Xiaolei Shi</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="hps_demo.html"><span class="std std-doc">hps_demo.ipynb</span></a></p></td>
<td><p>Intel® Xeon® CPU E5-2698 v4 &#64; 2.20GHz<br />512 GB Memory</p></td>
<td><p>Tesla V100-SXM2-32GB<br />32 GB Memory</p></td>
<td><p>1</p></td>
<td><p>Kingsley Liu, Matthias Langer and Yingcan Wei</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="training_with_remote_filesystem.html"><span class="std std-doc">training_with_remote_filesystem.ipynb</span></a></p></td>
<td><p>Intel® Xeon® CPU E5-2698 v4 &#64; 2.20GHz<br />512 GB Memory</p></td>
<td><p>Tesla V100-SXM2-32GB<br />32 GB Memory</p></td>
<td><p>1</p></td>
<td><p>Jerry Shi</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="hugectr_e2e_demo_with_nvtabular.html"><span class="std std-doc">hugectr_e2e_demo_with_nvtabular.ipynb</span></a></p></td>
<td><p>Intel® Xeon® CPU E5-2698 v4 &#64; 2.20GHz<br />512 GB Memory</p></td>
<td><p>Tesla V100-SXM2-32GB<br />32 GB Memory</p></td>
<td><p>1</p></td>
<td><p>Jerry Shi</p></td>
</tr>
</tbody>
</table>
</section>
<div class="toctree-wrapper compound">
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../performance.html" class="btn btn-neutral float-left" title="Performance" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hps_demo.html" class="btn btn-neutral float-right" title="Hierarchical Parameter Server Demo" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: main
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../v23.06.00/index.html">v23.06.00</a></dd>
      <dd><a href="../../v23.06.01/index.html">v23.06.01</a></dd>
      <dd><a href="../../v23.08.00/index.html">v23.08.00</a></dd>
      <dd><a href="../../v23.09.00/index.html">v23.09.00</a></dd>
      <dd><a href="../../v23.12.00/index.html">v23.12.00</a></dd>
      <dd><a href="../../v24.04.00/index.html">v24.04.00</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="index.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>