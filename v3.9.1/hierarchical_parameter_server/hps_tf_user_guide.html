<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Hierarchical Parameter Server Plugin for TensorFlow &mdash; Merlin HugeCTR  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hierarchical Parameter Server Notebooks" href="notebooks/index.html" />
    <link rel="prev" title="Hierarchical Parameter Server Database Backend" href="../hugectr_parameter_server.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">HUGECTR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_embedding_training_cache.html">Embedding Training Cache</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Hierarchical Parameter Server</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../hugectr_parameter_server.html">HPS Database Backend</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">HPS Plugin for TensorFlow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notebooks/index.html">Notebooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/index.html">API Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Hierarchical Parameter Server</a> &raquo;</li>
      <li>Hierarchical Parameter Server Plugin for TensorFlow</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="tex2jax_ignore mathjax_ignore section" id="hierarchical-parameter-server-plugin-for-tensorflow">
<h1>Hierarchical Parameter Server Plugin for TensorFlow<a class="headerlink" href="#hierarchical-parameter-server-plugin-for-tensorflow" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction-to-the-hps-plugin-for-tensorflow" id="id1">Introduction to the HPS Plugin for TensorFlow</a></p></li>
<li><p><a class="reference internal" href="#benefits-of-the-plugin-for-tensorflow" id="id2">Benefits of the Plugin for TensorFlow</a></p></li>
<li><p><a class="reference internal" href="#workflow" id="id3">Workflow</a></p></li>
<li><p><a class="reference internal" href="#installation" id="id4">Installation</a></p>
<ul>
<li><p><a class="reference internal" href="#compute-capability" id="id5">Compute Capability</a></p></li>
<li><p><a class="reference internal" href="#installing-hps-using-ngc-containers" id="id6">Installing HPS Using NGC Containers</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#example-notebooks" id="id7">Example Notebooks</a></p></li>
</ul>
</div>
<div class="section" id="introduction-to-the-hps-plugin-for-tensorflow">
<h2>Introduction to the HPS Plugin for TensorFlow<a class="headerlink" href="#introduction-to-the-hps-plugin-for-tensorflow" title="Permalink to this headline"></a></h2>
<p>Hierarchical Parameter Server (HPS) is a distributed inference framework that is dedicated to deploying large embedding tables and realizing the low-latency retrieval of embeddings.
The framework combines a high-performance GPU embedding cache with a hierarchical storage architecture that encompasses different types of database backends.
The plugin is provided as a Python toolkit that you can integrate easily into the TensorFlow (TF) model graph.
Integration with the graph facilitates the TensorFlow model deployment of large embedding tables.</p>
</div>
<div class="section" id="benefits-of-the-plugin-for-tensorflow">
<h2>Benefits of the Plugin for TensorFlow<a class="headerlink" href="#benefits-of-the-plugin-for-tensorflow" title="Permalink to this headline"></a></h2>
<p>When you deploy deep learning models with large embedding tables in TensorFlow, you are faced with the following challenges:</p>
<ul class="simple">
<li><p><strong>Large Embedding Tables</strong>: Trained embedding tables of hundreds of gigabytes cannot fit into the GPU memory.</p></li>
<li><p><strong>Low Latency Requirement</strong>: Online inference requires that the latency of embedding lookup should be low to maintain the quality of experience and the user engagement.</p></li>
<li><p><strong>Scalability on multiple GPUs</strong>: Dozens of models need to be deployed on multiple GPUs and each model can have several embedding tables.</p></li>
<li><p><strong>Pre-trained embeddings</strong>: Large embedding tables need to be loaded as pre-trained embeddings for tasks like transfer learning.</p></li>
</ul>
<p>The HPS plugin for TensorFlow mitigates these challenges and helps in the following ways:</p>
<ul class="simple">
<li><p>Extend the GPU memory by utilizing other memory resources available within the cluster, such as CPU-accessible RAM and non-volatile memory such as HDDs and SDDs, as shown in Fig. 1.</p></li>
<li><p>Use the GPU embedding cache to exploit the long-tail characteristics of the keys. The cache automatically stores the embeddings for hot keys as queries are constantly received, providing the low-latency lookup service.</p></li>
<li><p>Manage the embedding tables of multiple models in a structured manner across the whole memory hierarchy of GPUs, CPUs, and SSDs.</p></li>
<li><p>Make the lookup service subscribable through custom TensorFlow layers, enabling transfer learning with large embedding tables.</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/memory_hierarchy.png"><img alt="../_images/memory_hierarchy.png" class="align-center" src="../_images/memory_hierarchy.png" style="width: 1080px;" /></a>
<div align=center>Fig. 1: HPS Memory Hierarchy </div>
<p><br></br></p>
</div>
<div class="section" id="workflow">
<h2>Workflow<a class="headerlink" href="#workflow" title="Permalink to this headline"></a></h2>
<p>The workflow of leveraging HPS for deployment of TensorFlow models is illustrated in Fig. 2.</p>
<a class="reference internal image-reference" href="../_images/workflow.png"><img alt="../_images/workflow.png" class="align-center" src="../_images/workflow.png" style="width: 1080px;" /></a>
<div align=center>Fig. 2: Workflow of deploying TF models with HPS </div>
<p><br></br></p>
<p>The steps in the workflow can be summarized as:</p>
<ul class="simple">
<li><p><strong>Train</strong>: The model graph should be created with native TensorFlow embedding layers (e.g., <code class="docutils literal notranslate"><span class="pre">tf.nn.embedding_lookup_sparse</span></code>) or model parallelism enabled <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/index.html">SOK</a> embedding layers (e,g., <code class="docutils literal notranslate"><span class="pre">sok.DistributedEmbedding</span></code>). There is no restriction on the usage of dense layers or the topology of the model graph as long as the model can be successfully trained with TensorFlow.</p></li>
<li><p><strong>Dissect the training graph</strong>: The subgraph composided of only dense layers should be extracted from the trained graph, and then saved separately. For native TensorFlow embedding layers, the trained embedding weights should be obtained and converted to the HPS-compatible formats. For <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/index.html">SOK</a> embedding layers, <code class="docutils literal notranslate"><span class="pre">sok.Saver.dump_to_file</span></code> can be utilized to derive the desired formats. Basically, each embedding table should be stored in a directory with two binary files, i.e., <code class="docutils literal notranslate"><span class="pre">key</span></code> (int64) and <code class="docutils literal notranslate"><span class="pre">emb_vector</span></code> (float32). For example, if there are totally 1000 trained keys and the embedding vector size is 16, then the size of <code class="docutils literal notranslate"><span class="pre">key</span></code> file and the <code class="docutils literal notranslate"><span class="pre">emb_vector</span></code> file should be 1000*8 bytes and 1000*16*4 bytes respectively.</p></li>
<li><p><strong>Create and save the inference graph</strong>: The inference graph should be created with HPS layers (e.g., <code class="docutils literal notranslate"><span class="pre">hps.SparseLookupLayer</span></code>) and the saved subgraph of dense layers. It can be then saved as a whole so as to be deployed in the production environment.</p></li>
<li><p><strong>Deploy the inference graph with HPS</strong>: The configurations for the models to be deployed should be specified in a JSON file and the HPS should be started via <code class="docutils literal notranslate"><span class="pre">hps.Init</span></code> before any executions. The saved inference graph can be deployed to perform online inference leveraging the benefits of the HPS embedding lookup. Please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_parameter_server.html#configuration">HPS Configuration</a> for more information.</p></li>
</ul>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline"></a></h2>
<div class="section" id="compute-capability">
<h3>Compute Capability<a class="headerlink" href="#compute-capability" title="Permalink to this headline"></a></h3>
<p>We support the following compute capabilities:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Compute Capability</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>SM</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>7.0</p></td>
<td><p>NVIDIA V100 (Volta)</p></td>
<td><p>70</p></td>
</tr>
<tr class="row-odd"><td><p>7.5</p></td>
<td><p>NVIDIA T4 (Turing)</p></td>
<td><p>75</p></td>
</tr>
<tr class="row-even"><td><p>8.0</p></td>
<td><p>NVIDIA A100 (Ampere)</p></td>
<td><p>80</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="installing-hps-using-ngc-containers">
<h3>Installing HPS Using NGC Containers<a class="headerlink" href="#installing-hps-using-ngc-containers" title="Permalink to this headline"></a></h3>
<p>All NVIDIA Merlin components are available as open source projects. However, a more convenient way to utilize these components is by using our Merlin NGC containers. These containers allow you to package your software application, libraries, dependencies, and runtime compilers in a self-contained environment. When installing HPS using NGC containers, the application environment remains portable, consistent, reproducible, and agnostic to the underlying host system’s software configuration.</p>
<p>HPS is included in the Merlin Docker containers that are available from the <a class="reference external" href="https://catalog.ngc.nvidia.com/containers">NVIDIA container repository</a>. To use these Docker containers, you’ll first need to install the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> to provide GPU support for Docker. You can use the NGC links referenced in the table above to obtain more information about how to launch and run these containers.</p>
<p>The following sample command pulls and starts the Merlin TensorFlow container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run the container in interactive mode</span>
$ docker run --gpus<span class="o">=</span>all --rm -it --cap-add SYS_NICE nvcr.io/nvidia/merlin/merlin-tensorflow:22.08
</pre></div>
</div>
<p>You can check the existence of the HPS Python toolkit after launching this container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$ python3 -c <span class="s2">&quot;import hierarchical_parameter_server as hps&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="example-notebooks">
<h2>Example Notebooks<a class="headerlink" href="#example-notebooks" title="Permalink to this headline"></a></h2>
<p>We provide a collection of examples as <a class="reference internal" href="notebooks/index.html"><span class="doc std std-doc">Jupyter Notebooks</span></a> that cover the following topics:</p>
<ul class="simple">
<li><p>Basic workflow of HPS deployment for TensorFlow models</p></li>
<li><p>Migrating from SOK training to HPS inference</p></li>
<li><p>Leveraging HPS to load pre-trained embeddings</p></li>
</ul>
</div>
<div class="toctree-wrapper compound">
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../hugectr_parameter_server.html" class="btn btn-neutral float-left" title="Hierarchical Parameter Server Database Backend" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="notebooks/index.html" class="btn btn-neutral float-right" title="Hierarchical Parameter Server Notebooks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v3.9.1
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../v3.7/index.html">v3.7</a></dd>
      <dd><a href="../../v3.8/index.html">v3.8</a></dd>
      <dd><a href="../../v3.9/hierarchical_parameter_server/hps_tf_user_guide.html">v3.9</a></dd>
      <dd><a href="hps_tf_user_guide.html">v3.9.1</a></dd>
      <dd><a href="../../v4.0/hierarchical_parameter_server/hps_tf_user_guide.html">v4.0</a></dd>
      <dd><a href="../../v4.1/hierarchical_parameter_server/hps_tf_user_guide.html">v4.1</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>