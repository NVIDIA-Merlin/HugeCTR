{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e290f79",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hps-hps-tensorflow-triton-deployment/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HPS TensorRT Plugin Demo for HugeCTR Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ecbde",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to build and deploy the HPS-integrated TensorRT engine for the model trained with HugeCTR.\n",
    "\n",
    "For more details about HPS, please refer to [HugeCTR Hierarchical Parameter Server (HPS)](https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2affa91b",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Use NGC\n",
    "\n",
    "The HPS TensorRT plugin is preinstalled in the 23.01 and later [Merlin HugeCTR Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-hugectr): `nvcr.io/nvidia/merlin/merlin-hugectr:23.01`.\n",
    "\n",
    "You can check the existence of the required libraries by running the following Python code after launching this container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb124ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "plugin_lib_name = \"/usr/local/hps_trt/lib/libhps_plugin.so\"\n",
    "plugin_handle = ctypes.CDLL(plugin_lib_name, mode=ctypes.RTLD_GLOBAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50485d41",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "HugeCTR provides a tool to generate synthetic datasets. The [Data Generator](https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#data-generator-api) is capable of generating datasets of different file formats and different distributions. We will generate one-hot Parquet datasets with power-law distribution for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13dd00e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][05:12:08.561][INFO][RK0][main]: Generate Parquet dataset\n",
      "[HCTR][05:12:08.561][INFO][RK0][main]: train data folder: ./data_parquet, eval data folder: ./data_parquet, slot_size_array: 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, nnz array: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, #files for train: 16, #files for eval: 4, #samples per file: 40960, Use power law distribution: 1, alpha of power law: 1.3\n",
      "[HCTR][05:12:08.564][INFO][RK0][main]: ./data_parquet exist\n",
      "[HCTR][05:12:08.568][INFO][RK0][main]: ./data_parquet/train/gen_0.parquet\n",
      "[HCTR][05:12:10.204][INFO][RK0][main]: ./data_parquet/train/gen_1.parquet\n",
      "[HCTR][05:12:10.455][INFO][RK0][main]: ./data_parquet/train/gen_2.parquet\n",
      "[HCTR][05:12:10.709][INFO][RK0][main]: ./data_parquet/train/gen_3.parquet\n",
      "[HCTR][05:12:10.957][INFO][RK0][main]: ./data_parquet/train/gen_4.parquet\n",
      "[HCTR][05:12:11.196][INFO][RK0][main]: ./data_parquet/train/gen_5.parquet\n",
      "[HCTR][05:12:11.437][INFO][RK0][main]: ./data_parquet/train/gen_6.parquet\n",
      "[HCTR][05:12:11.681][INFO][RK0][main]: ./data_parquet/train/gen_7.parquet\n",
      "[HCTR][05:12:11.920][INFO][RK0][main]: ./data_parquet/train/gen_8.parquet\n",
      "[HCTR][05:12:12.171][INFO][RK0][main]: ./data_parquet/train/gen_9.parquet\n",
      "[HCTR][05:12:12.411][INFO][RK0][main]: ./data_parquet/train/gen_10.parquet\n",
      "[HCTR][05:12:12.650][INFO][RK0][main]: ./data_parquet/train/gen_11.parquet\n",
      "[HCTR][05:12:12.885][INFO][RK0][main]: ./data_parquet/train/gen_12.parquet\n",
      "[HCTR][05:12:13.120][INFO][RK0][main]: ./data_parquet/train/gen_13.parquet\n",
      "[HCTR][05:12:13.341][INFO][RK0][main]: ./data_parquet/train/gen_14.parquet\n",
      "[HCTR][05:12:13.577][INFO][RK0][main]: ./data_parquet/train/gen_15.parquet\n",
      "[HCTR][05:12:13.818][INFO][RK0][main]: ./data_parquet/file_list.txt done!\n",
      "[HCTR][05:12:13.827][INFO][RK0][main]: ./data_parquet/val/gen_0.parquet\n",
      "[HCTR][05:12:14.066][INFO][RK0][main]: ./data_parquet/val/gen_1.parquet\n",
      "[HCTR][05:12:14.299][INFO][RK0][main]: ./data_parquet/val/gen_2.parquet\n",
      "[HCTR][05:12:14.537][INFO][RK0][main]: ./data_parquet/val/gen_3.parquet\n",
      "[HCTR][05:12:14.751][INFO][RK0][main]: ./data_parquet/file_list_test.txt done!\n"
     ]
    }
   ],
   "source": [
    "import hugectr\n",
    "from hugectr.tools import DataGeneratorParams, DataGenerator\n",
    "\n",
    "data_generator_params = DataGeneratorParams(\n",
    "  format = hugectr.DataReaderType_t.Parquet,\n",
    "  label_dim = 1,\n",
    "  dense_dim = 13,\n",
    "  num_slot = 26,\n",
    "  i64_input_key = True,\n",
    "  nnz_array = [1 for _ in range(26)],\n",
    "  source = \"./data_parquet/file_list.txt\",\n",
    "  eval_source = \"./data_parquet/file_list_test.txt\",\n",
    "  slot_size_array = [10000 for _ in range(26)],\n",
    "  check_type = hugectr.Check_t.Non,\n",
    "  dist_type = hugectr.Distribution_t.PowerLaw,\n",
    "  power_law_type = hugectr.PowerLaw_t.Short,\n",
    "  num_files = 16,\n",
    "  eval_num_files = 4,\n",
    "  num_samples_per_file = 40960)\n",
    "data_generator = DataGenerator(data_generator_params)\n",
    "data_generator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2777eb4",
   "metadata": {},
   "source": [
    "## Train with HugeCTR\n",
    "\n",
    "We can train a DLRM model with HugeCTR Python APIs. The trained sparse and dense model files will be saved separately. The model graph will be dumped into a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244b8ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "solver = hugectr.CreateSolver(\n",
    "    model_name=\"dlrm\",\n",
    "    max_eval_batches=160,\n",
    "    batchsize_eval=1024,\n",
    "    batchsize=1024,\n",
    "    lr=0.001,\n",
    "    vvgpu=[[0]],\n",
    "    repeat_dataset=True,\n",
    "    use_mixed_precision=True,\n",
    "    use_cuda_graph=True,\n",
    "    scaler=1024,\n",
    "    i64_input_key=True,\n",
    ")\n",
    "reader = hugectr.DataReaderParams(\n",
    "    data_reader_type=hugectr.DataReaderType_t.Parquet,\n",
    "    source=[\"./data_parquet/file_list.txt\"],\n",
    "    eval_source=\"./data_parquet/file_list_test.txt\",\n",
    "    slot_size_array=[10000 for _ in range(26)],\n",
    "    check_type=hugectr.Check_t.Non,\n",
    ")\n",
    "optimizer = hugectr.CreateOptimizer(\n",
    "    optimizer_type=hugectr.Optimizer_t.Adam,\n",
    "    update_type=hugectr.Update_t.Global,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=0.0001,\n",
    ")\n",
    "\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(\n",
    "    hugectr.Input(\n",
    "        label_dim=1,\n",
    "        label_name=\"label\",\n",
    "        dense_dim=13,\n",
    "        dense_name=\"numerical_features\",\n",
    "        data_reader_sparse_param_array=[hugectr.DataReaderSparseParam(\"keys\", 1, True, 26)],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.SparseEmbedding(\n",
    "        embedding_type=hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash,\n",
    "        workspace_size_per_gpu_in_mb=5000,\n",
    "        embedding_vec_size=128,\n",
    "        combiner=\"mean\",\n",
    "        sparse_embedding_name=\"sparse_embedding1\",\n",
    "        bottom_name=\"keys\",\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.MLP,\n",
    "        bottom_names=[\"numerical_features\"],\n",
    "        top_names=[\"mlp1\"],\n",
    "        num_outputs=[512, 256, 128],\n",
    "        act_type=hugectr.Activation_t.Relu,\n",
    "        use_bias=True,\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.Interaction,\n",
    "        bottom_names=[\"mlp1\", \"sparse_embedding1\"],\n",
    "        top_names=[\"interaction1\"],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.MLP,\n",
    "        bottom_names=[\"interaction1\"],\n",
    "        top_names=[\"mlp2\"],\n",
    "        num_outputs=[1024, 1024, 512, 256, 1],\n",
    "        use_bias=True,\n",
    "        activations=[\n",
    "            hugectr.Activation_t.Relu,\n",
    "            hugectr.Activation_t.Relu,\n",
    "            hugectr.Activation_t.Relu,\n",
    "            hugectr.Activation_t.Relu,\n",
    "            hugectr.Activation_t.Non,\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    hugectr.DenseLayer(\n",
    "        layer_type=hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "        bottom_names=[\"mlp2\", \"label\"],\n",
    "        top_names=[\"loss\"],\n",
    "    )\n",
    ")\n",
    "model.graph_to_json(\"dlrm_hugectr_graph.json\")\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter=1200, display=200, eval_interval=1000, snapshot=1000, snapshot_prefix=\"dlrm_hugectr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c85ed6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "An error occurred while trying to map in the address of a function.\n",
      "  Function Name: cuIpcOpenMemHandle_v2\n",
      "  Error string:  /usr/lib/x86_64-linux-gnu/libcuda.so.1: undefined symbol: cuIpcOpenMemHandle_v2\n",
      "CUDA-aware support is disabled.\n",
      "--------------------------------------------------------------------------\n",
      "HugeCTR Version: 4.1\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][05:12:24.539][INFO][RK0][main]: Initialize model: dlrm\n",
      "[HCTR][05:12:24.539][INFO][RK0][main]: Global seed is 2950905596\n",
      "[HCTR][05:12:24.542][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][05:12:26.698][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][05:12:26.698][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][05:12:26.698][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][05:12:26.699][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][05:12:26.700][INFO][RK0][main]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HCTR][05:12:26.705][INFO][RK0][main]: num of DataReader workers for train: 1\n",
      "[HCTR][05:12:26.705][INFO][RK0][main]: num of DataReader workers for eval: 1\n",
      "[HCTR][05:12:26.782][INFO][RK0][main]: Vocabulary size: 260000\n",
      "[HCTR][05:12:26.782][INFO][RK0][main]: max_vocabulary_size_per_gpu_=3413333\n",
      "[HCTR][05:12:26.791][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "[HCTR][05:12:26.795][INFO][RK0][main]: Save the model graph to dlrm_hugectr_graph.json successfully\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][05:12:27.772][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][05:12:27.781][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][05:12:27.783][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: Model structure on each GPU\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   numerical_features             keys                          \n",
      "(1024,1)                                (1024,13)                               \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      keys                          sparse_embedding1             (1024,26,128)                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "MLP                                     numerical_features            mlp1                          (1024,128)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Interaction                             mlp1                          interaction1                  (1024,480)                    \n",
      "                                        sparse_embedding1                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "MLP                                     interaction1                  mlp2                          (1024,1)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  mlp2                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: Use non-epoch mode with number of iterations: 1200\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: Evaluation interval: 1000, snapshot interval: 1000\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: Use mixed precision: True, scaler: 1024.000000, use cuda graph: True\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: Training source file: ./data_parquet/file_list.txt\n",
      "[HCTR][05:12:27.785][INFO][RK0][main]: Evaluation source file: ./data_parquet/file_list_test.txt\n",
      "[HCTR][05:12:31.522][INFO][RK0][main]: Iter: 200 Time(200 iters): 3.72017s Loss: 0.693168 lr:0.001\n",
      "[HCTR][05:12:35.188][INFO][RK0][main]: Iter: 400 Time(200 iters): 3.64947s Loss: 0.694016 lr:0.001\n",
      "[HCTR][05:12:38.814][INFO][RK0][main]: Iter: 600 Time(200 iters): 3.60927s Loss: 0.69323 lr:0.001\n",
      "[HCTR][05:12:42.432][INFO][RK0][main]: Iter: 800 Time(200 iters): 3.60078s Loss: 0.693079 lr:0.001\n",
      "[HCTR][05:12:46.050][INFO][RK0][main]: Iter: 1000 Time(200 iters): 3.60162s Loss: 0.693134 lr:0.001\n",
      "[HCTR][05:12:46.206][INFO][RK0][main]: Evaluation, AUC: 0.498656\n",
      "[HCTR][05:12:46.206][INFO][RK0][main]: Eval Time for 160 iters: 0.156138s\n",
      "[HCTR][05:12:46.206][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][05:12:46.272][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][05:12:47.456][INFO][RK0][main]: Dumping sparse weights to files, successful\n",
      "[HCTR][05:12:47.958][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][05:12:47.958][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][05:12:56.286][INFO][RK0][main]: Done\n",
      "[HCTR][05:12:56.840][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][05:12:56.840][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][05:13:06.514][INFO][RK0][main]: Done\n",
      "[HCTR][05:13:06.555][INFO][RK0][main]: Dumping sparse optimzer states to files, successful\n",
      "[HCTR][05:13:06.561][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][05:13:06.693][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][05:13:06.694][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][05:13:06.823][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n",
      "[HCTR][05:13:10.414][INFO][RK0][main]: Finish 1200 iterations with batchsize: 1024 in 42.63s.\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5028eac",
   "metadata": {},
   "source": [
    "## Build the HPS-integrated TensorRT engine\n",
    "The sparse saved model `dlrm_hugectr0_sparse_1000.model` is already in the format that HPS requires. In order to use HPS in the inference stage, we need to create JSON configuration file for HPS.\n",
    "\n",
    "Then we convert the dense saved model `dlrm_hugectr_dense_1000.model` to ONNX using `hugectr2onnx`, and employ the ONNX GraphSurgoen tool to replace the input embedding vectors with with the placeholder of HPS TensorRT plugin layer.\n",
    "\n",
    "After that, we can build the TensorRT engine, which is comprised of the HPS TensorRT plugin layer and the dense network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c917e25",
   "metadata": {},
   "source": [
    "### Step1: Prepare JSON configuration file for HPS\n",
    "\n",
    "Please note that the storage format in the `dlrm_hugectr0_sparse_1000.model/key` file is int64, while the HPS TensorRT plugin only supports int32 when loading the keys into memory. There is no overflow since the key value range is 0~260000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d2f5ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dlrm_hugectr.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlrm_hugectr.json\n",
    "{\n",
    "    \"supportlonglong\": false,\n",
    "    \"models\": [{\n",
    "        \"model\": \"dlrm\",\n",
    "        \"sparse_files\": [\"dlrm_hugectr0_sparse_1000.model\"],\n",
    "        \"num_of_worker_buffer_in_pool\": 3,\n",
    "        \"embedding_table_names\":[\"sparse_embedding0\"],\n",
    "        \"embedding_vecsize_per_table\": [128],\n",
    "        \"maxnum_catfeature_query_per_table_per_sample\": [26],\n",
    "        \"default_value_for_each_table\": [1.0],\n",
    "        \"deployed_device_list\": [0],\n",
    "        \"max_batch_size\": 1024,\n",
    "        \"cache_refresh_percentage_per_iteration\": 0.2,\n",
    "        \"hit_rate_threshold\": 1.0,\n",
    "        \"gpucacheper\": 1.0,\n",
    "        \"gpucache\": true\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e130fca",
   "metadata": {},
   "source": [
    "### Step2: Convert to ONNX and do ONNX graph surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1683bd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR2ONNX][INFO]: Converting Data layer to ONNX\n",
      "Skip sparse embedding layers in converted ONNX model\n",
      "[HUGECTR2ONNX][INFO]: Converting DistributedSlotSparseEmbeddingHash layer to ONNX\n",
      "[HUGECTR2ONNX][INFO]: Converting MLP layer to ONNX\n",
      "[HUGECTR2ONNX][INFO]: Converting Interaction layer to ONNX\n",
      "[HUGECTR2ONNX][INFO]: Converting MLP layer to ONNX\n",
      "[HUGECTR2ONNX][INFO]: Converting Sigmoid layer to ONNX\n",
      "[HUGECTR2ONNX][INFO]: The model is checked!\n",
      "[HUGECTR2ONNX][INFO]: The model is saved at dlrm_hugectr_dense.onnx\n"
     ]
    }
   ],
   "source": [
    "# hugectr2onnx\n",
    "import hugectr2onnx\n",
    "hugectr2onnx.converter.convert(onnx_model_path = \"dlrm_hugectr_dense.onnx\",\n",
    "                            graph_config = \"dlrm_hugectr_graph.json\",\n",
    "                            dense_model = \"dlrm_hugectr_dense_1000.model\",\n",
    "                            convert_embedding = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b561a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX graph surgery to insert HPS the TensorRT plugin placeholder\n",
    "import onnx_graphsurgeon as gs\n",
    "from onnx import  shape_inference\n",
    "import numpy as np\n",
    "import onnx\n",
    "\n",
    "graph = gs.import_onnx(onnx.load(\"dlrm_hugectr_dense.onnx\"))\n",
    "saved = []\n",
    "\n",
    "for i in graph.inputs:\n",
    "    if i.name == \"sparse_embedding1\":\n",
    "        categorical_features = gs.Variable(name=\"categorical_features\", dtype=np.int32, shape=(\"unknown_1\", 26))\n",
    "        node = gs.Node(op=\"HPS_TRT\", attrs={\"ps_config_file\": \"dlrm_hugectr.json\\0\", \"model_name\": \"dlrm\\0\", \"table_id\": 0, \"emb_vec_size\": 128}, inputs=[categorical_features], outputs=[i])\n",
    "        graph.nodes.append(node)\n",
    "        saved.append(categorical_features)\n",
    "    elif i.name == \"numerical_features\":\n",
    "        i.shape = (\"unknown_2\", 13)\n",
    "        saved.append(i)\n",
    "\n",
    "graph.inputs = saved\n",
    "\n",
    "graph.cleanup().toposort()\n",
    "onnx.save(gs.export_onnx(graph), \"dlrm_hugectr_with_hps.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf55d6",
   "metadata": {},
   "source": [
    "### Step3: Build the TensorRT engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7549dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2022-05:13:31] [TRT] [I] [MemUsageChange] Init CUDA: CPU +262, GPU +0, now: CPU 1014, GPU 886 (MiB)\n",
      "[12/14/2022-05:13:33] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +170, GPU +46, now: CPU 1239, GPU 932 (MiB)\n",
      "[12/14/2022-05:13:33] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "[12/14/2022-05:13:33] [TRT] [W] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[12/14/2022-05:13:33] [TRT] [I] No importer registered for op: HPS_TRT. Attempting to import as plugin.\n",
      "[12/14/2022-05:13:33] [TRT] [I] Searching for plugin: HPS_TRT, plugin_version: 1, plugin_namespace: \n",
      "=====================================================HPS Parse====================================================\n",
      "[HCTR][05:13:33.812][INFO][RK0][main]: dense_file is not specified using default: \n",
      "[HCTR][05:13:33.812][INFO][RK0][main]: num_of_refresher_buffer_in_pool is not specified using default: 1\n",
      "[HCTR][05:13:33.812][INFO][RK0][main]: maxnum_des_feature_per_sample is not specified using default: 26\n",
      "[HCTR][05:13:33.812][INFO][RK0][main]: refresh_delay is not specified using default: 0\n",
      "[HCTR][05:13:33.812][INFO][RK0][main]: refresh_interval is not specified using default: 0\n",
      "[HCTR][05:13:33.812][INFO][RK0][main]: use_static_table is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][05:13:33.813][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][05:13:33.813][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][05:13:33.813][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][05:13:33.813][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][05:13:33.813][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][05:13:33.813][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][05:13:36.189][INFO][RK0][main]: Table: hps_et.dlrm.sparse_embedding0; cached 239950 / 239950 embeddings in volatile database (HashMapBackend); load: 239950 / 18446744073709551615 (0.00%).\n",
      "[HCTR][05:13:36.196][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][05:13:36.196][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][05:13:36.205][INFO][RK0][main]: Model name: dlrm\n",
      "[HCTR][05:13:36.205][INFO][RK0][main]: Max batch size: 1024\n",
      "[HCTR][05:13:36.205][INFO][RK0][main]: Number of embedding tables: 1\n",
      "[HCTR][05:13:36.205][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 1.000000\n",
      "[HCTR][05:13:36.205][INFO][RK0][main]: Use static table: False\n",
      "[HCTR][05:13:36.205][INFO][RK0][main]: Use I64 input key: False\n",
      "[HCTR][05:13:36.205][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000\n",
      "[HCTR][05:13:36.205][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][05:13:36.205][INFO][RK0][main]: The size of worker memory pool: 3\n",
      "[HCTR][05:13:36.205][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][05:13:36.205][INFO][RK0][main]: The refresh percentage : 0.200000\n",
      "[HCTR][05:13:36.270][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][05:13:36.270][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][05:13:36.419][INFO][RK0][main]: EC initialization for model: \"dlrm\", num_tables: 1\n",
      "[HCTR][05:13:36.419][INFO][RK0][main]: EC initialization on device: 0\n",
      "[HCTR][05:13:36.440][INFO][RK0][main]: Creating lookup session for dlrm on device: 0\n",
      "[12/14/2022-05:13:36] [TRT] [I] Successfully created plugin: HPS_TRT\n",
      "[12/14/2022-05:13:37] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +335, GPU +146, now: CPU 5763, GPU 1314 (MiB)\n",
      "[12/14/2022-05:13:37] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +116, GPU +54, now: CPU 5879, GPU 1368 (MiB)\n",
      "[12/14/2022-05:13:37] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/14/2022-05:13:37] [TRT] [W] Using kFASTER_DYNAMIC_SHAPES_0805 preview feature.\n",
      "[12/14/2022-05:13:52] [TRT] [I] Total Activation Memory: 34118830080\n",
      "[12/14/2022-05:13:52] [TRT] [I] Detected 2 inputs and 1 output network tensors.\n",
      "[12/14/2022-05:13:52] [TRT] [I] Total Host Persistent Memory: 20304\n",
      "[12/14/2022-05:13:52] [TRT] [I] Total Device Persistent Memory: 10752\n",
      "[12/14/2022-05:13:52] [TRT] [I] Total Scratch Memory: 32505856\n",
      "[12/14/2022-05:13:52] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 16 MiB, GPU 4628 MiB\n",
      "[12/14/2022-05:13:52] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 16 steps to complete.\n",
      "[12/14/2022-05:13:52] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.09284ms to assign 4 blocks to 16 nodes requiring 48099840 bytes.\n",
      "[12/14/2022-05:13:52] [TRT] [I] Total Activation Memory: 48099840\n",
      "[12/14/2022-05:13:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 6321, GPU 1580 (MiB)\n",
      "[12/14/2022-05:13:52] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 6322, GPU 1590 (MiB)\n",
      "[12/14/2022-05:13:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +8, GPU +16, now: CPU 8, GPU 16 (MiB)\n",
      "Succesfully build the TensorRT engine\n"
     ]
    }
   ],
   "source": [
    "# build the TensorRT engine based on dlrm_with_hps.onnx\n",
    "import tensorrt as trt\n",
    "import ctypes\n",
    "\n",
    "plugin_lib_name = \"/usr/local/hps_trt/lib/libhps_plugin.so\"\n",
    "handle = ctypes.CDLL(plugin_lib_name, mode=ctypes.RTLD_GLOBAL)\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "def build_engine_from_onnx(onnx_model_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(EXPLICIT_BATCH) as network, trt.OnnxParser(network, TRT_LOGGER) as parser, builder.create_builder_config() as builder_config:        \n",
    "        model = open(onnx_model_path, 'rb')\n",
    "        parser.parse(model.read())\n",
    "\n",
    "        profile = builder.create_optimization_profile()        \n",
    "        profile.set_shape(\"categorical_features\", (1, 26), (1024, 26), (1024, 26))    \n",
    "        profile.set_shape(\"numerical_features\", (1, 13), (1024, 13), (1024, 13))\n",
    "        builder_config.add_optimization_profile(profile)\n",
    "        engine = builder.build_serialized_network(network, builder_config)\n",
    "        return engine\n",
    "\n",
    "serialized_engine = build_engine_from_onnx(\"dlrm_hugectr_with_hps.onnx\")\n",
    "with open(\"dlrm_hugectr_with_hps.trt\", \"wb\") as fout:\n",
    "    fout.write(serialized_engine)\n",
    "print(\"Succesfully build the TensorRT engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fab1f",
   "metadata": {},
   "source": [
    "## Deploy HPS-integrated TensorRT engine on Triton\n",
    "\n",
    "In order to deploy the TensorRT engine with the Triton TensorRT backend, we need to create the model repository and define the `config.pbtxt` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b09ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repo/dlrm_hugectr_with_hps/1\n",
    "!mv dlrm_hugectr_with_hps.trt model_repo/dlrm_hugectr_with_hps/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cbeeb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_repo/dlrm_hugectr_with_hps/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_repo/dlrm_hugectr_with_hps/config.pbtxt\n",
    "\n",
    "platform: \"tensorrt_plan\"\n",
    "default_model_filename: \"dlrm_hugectr_with_hps.trt\"\n",
    "backend: \"tensorrt\"\n",
    "max_batch_size: 0\n",
    "input [\n",
    "  {\n",
    "    name: \"categorical_features\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [-1,26]\n",
    "  },\n",
    "  {\n",
    "    name: \"numerical_features\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1,13]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "      name: \"label\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [-1,1]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "    gpus:[0]\n",
    "\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0c411c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmodel_repo/dlrm_hugectr_with_hps\u001b[00m\r\n",
      "├── \u001b[01;34m1\u001b[00m\r\n",
      "│   └── dlrm_hugectr_with_hps.trt\r\n",
      "└── config.pbtxt\r\n",
      "\r\n",
      "1 directory, 2 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree model_repo/dlrm_hugectr_with_hps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd011a",
   "metadata": {},
   "source": [
    "We can then launch the Triton inference server using the TensorRT backend. Please note that `LD_PRELOAD` is utilized to load the custom TensorRT plugin (i.e., HPS TensorRT plugin) into Triton.\n",
    "\n",
    "Note: `Since Background processes not supported by Jupyter, please launch the Triton Server according to the following command independently in the background`.\n",
    "\n",
    "> **LD_PRELOAD=/usr/local/hps_trt/lib/libhps_plugin.so tritonserver --model-repository=/hugectr/hps_trt/notebooks/model_repo/ --load-model=dlrm_hugectr_with_hps --model-control-mode=explicit**\n",
    "\n",
    "If you successfully started tritonserver, you should see a log similar to following:\n",
    "\n",
    "\n",
    "```bash\n",
    "+----------+--------------------------------+--------------------------------+\n",
    "| Backend  | Path                           | Config                         |\n",
    "+----------+--------------------------------+--------------------------------+\n",
    "| tensorrt | /opt/tritonserver/backends/ten | {\"cmdline\":{\"auto-complete-con |\n",
    "|          | sorrt/libtriton_tensorrt.so    | fig\":\"true\",\"min-compute-capab |\n",
    "|          |                                | ility\":\"6.000000\",\"backend-dir |\n",
    "|          |                                | ectory\":\"/opt/tritonserver/bac |\n",
    "|          |                                | kends\",\"default-max-batch-size |\n",
    "|          |                                | \":\"4\"}}                        |\n",
    "|          |                                |                                |\n",
    "+----------+--------------------------------+--------------------------------+\n",
    "\n",
    "+-----------------------+---------+--------+\n",
    "| Model                 | Version | Status |\n",
    "+-----------------------+---------+--------+\n",
    "| dlrm_hugectr_with_hps | 1       | READY  |\n",
    "+-----------------------+---------+--------+\n",
    "```\n",
    "\n",
    "We can then send the requests to the Triton inference server using the HTTP client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16be5f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction result is \n",
      "[[1.        ]\n",
      " [0.49642828]\n",
      " [0.52846366]\n",
      " ...\n",
      " [0.99999994]\n",
      " [0.9999992 ]\n",
      " [0.9999905 ]]\n",
      "Response details:\n",
      "{'model_name': 'dlrm_hugectr_with_hps', 'model_version': '1', 'outputs': [{'name': 'label', 'datatype': 'FP32', 'shape': [1024, 1], 'parameters': {'binary_data_size': 4096}}]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import *\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "categorical_feature = np.random.randint(0,260000,size=(BATCH_SIZE,26)).astype(np.int32)\n",
    "numerical_feature = np.random.random((BATCH_SIZE, 13)).astype(np.float32)\n",
    "\n",
    "inputs = [\n",
    "    httpclient.InferInput(\"categorical_features\", \n",
    "                          categorical_feature.shape,\n",
    "                          np_to_triton_dtype(np.int32)),\n",
    "    httpclient.InferInput(\"numerical_features\", \n",
    "                          numerical_feature.shape,\n",
    "                          np_to_triton_dtype(np.float32)),                          \n",
    "]\n",
    "inputs[0].set_data_from_numpy(categorical_feature)\n",
    "inputs[1].set_data_from_numpy(numerical_feature)\n",
    "\n",
    "\n",
    "outputs = [\n",
    "    httpclient.InferRequestedOutput(\"label\")\n",
    "]\n",
    "\n",
    "model_name = \"dlrm_hugectr_with_hps\"\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            outputs=outputs)\n",
    "    result = response.get_response()\n",
    "    \n",
    "    print(\"Prediction result is \\n{}\".format(response.as_numpy(\"label\")))\n",
    "    print(\"Response details:\\n{}\".format(result))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
