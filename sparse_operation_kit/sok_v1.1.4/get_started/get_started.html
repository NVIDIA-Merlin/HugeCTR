<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Get Started With SparseOperationKit &mdash; SparseOperationKit  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Examples and Tutorials" href="../examples/index.html" />
    <link rel="prev" title="SparseOperationKit" href="../intro_link.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> SparseOperationKit
          </a>
              <div class="version">
                1.1.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro_link.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Get Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#import-sparseoperationkit">Import SparseOperationKit</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tensorflow-2-x">TensorFlow 2.x</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#define-a-model-with-tensorflow">Define a model with TensorFlow</a></li>
<li class="toctree-l3"><a class="reference internal" href="#use-sparseoperationkit-with-tf-distribute-strategy">Use SparseOperationKit with tf.distribute.Strategy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#with-tf-distribute-mirroredstrategy">with tf.distribute.MirroredStrategy</a></li>
<li class="toctree-l4"><a class="reference internal" href="#with-tf-distribute-multiworkermirroredstrategy">With tf.distribute.MultiWorkerMirroredStrategy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#use-sparseoperationkit-with-horovod">Use SparseOperationKit with Horovod</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tensorflow-1-15">TensorFlow 1.15</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#using-sparseoperationkit-with-horovod">Using SparseOperationKit with Horovod</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#experimental-features">Experimental Features</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance/index.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Docs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes/release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_vars/env_vars.html">Environment Variables</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SparseOperationKit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Get Started With SparseOperationKit</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/get_started/get_started.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="get-started-with-sparseoperationkit">
<h1>Get Started With SparseOperationKit<a class="headerlink" href="#get-started-with-sparseoperationkit" title="Permalink to this heading"></a></h1>
<p>This document will walk you through simple demos to get you familiar with SparseOperationKit.</p>
<div class="admonition note">
<p class="admonition-title">See also</p>
<p>For experts or more examples, please refer to Examples section</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>We strongly recommend using the new version of SOK under <code class="docutils literal notranslate"><span class="pre">sok.experiment</span></code>. After this new version is stable, the old SOK will be deprecated. See the Experimental Features section on this page to get started with it.</p>
</div>
<p>Refer to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/intro_link.html#installation"><em>Installation</em> section</a> to install SparseOperationKit on your system.</p>
<section id="import-sparseoperationkit">
<h2>Import SparseOperationKit<a class="headerlink" href="#import-sparseoperationkit" title="Permalink to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sparse_operation_kit</span> <span class="k">as</span> <span class="nn">sok</span>
</pre></div>
</div>
<p>SOK supports TensorFlow 1.15 and 2.x, and automatically detects the version of TensorFlow from your program. The SOK API signatures for TensorFlow 2.x and TensorFlow 1.15 are identical.</p>
</section>
<section id="tensorflow-2-x">
<h2>TensorFlow 2.x<a class="headerlink" href="#tensorflow-2-x" title="Permalink to this heading"></a></h2>
<section id="define-a-model-with-tensorflow">
<h3>Define a model with TensorFlow<a class="headerlink" href="#define-a-model-with-tensorflow" title="Permalink to this heading"></a></h3>
<p>The structure of this demo model is depicted in Fig 1.</p>
<p><br><img alt="../_images/demo_model_structure1.png" src="../_images/demo_model_structure1.png" /></br></p>
<center><b>Fig 1. The structure of demo model</b></center>
<br>
<p>To define the model, you can use either use <em>subclassing</em> or the <em>functional API</em>.</p>
<p><strong>Subclassing approach</strong>. The following code sample shows how this demo model can be created by subclassing <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code>. Additional information about <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> and its customization options is available <a class="reference external" href="https://tensorflow.google.cn/guide/keras/custom_layers_and_models">here</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="k">class</span> <span class="nc">DemoModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">max_vocabulary_size_per_gpu</span><span class="p">,</span>
                 <span class="n">slot_num</span><span class="p">,</span>
                 <span class="n">nnz_per_slot</span><span class="p">,</span>
                 <span class="n">embedding_vector_size</span><span class="p">,</span>
                 <span class="n">num_of_dense_layers</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DemoModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_vocabulary_size_per_gpu</span> <span class="o">=</span> <span class="n">max_vocabulary_size_per_gpu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span> <span class="o">=</span> <span class="n">slot_num</span>            <span class="c1"># the number of feature-fileds per sample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nnz_per_slot</span> <span class="o">=</span> <span class="n">nnz_per_slot</span>    <span class="c1"># the number of valid keys per feature-filed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_vector_size</span> <span class="o">=</span> <span class="n">embedding_vector_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_of_dense_layers</span> <span class="o">=</span> <span class="n">num_of_dense_layers</span>

        <span class="c1"># this embedding layer will concatenate each key&#39;s embedding vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">All2AllDenseEmbedding</span><span class="p">(</span>
                    <span class="n">max_vocabulary_size_per_gpu</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_vocabulary_size_per_gpu</span><span class="p">,</span>
                    <span class="n">embedding_vec_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_vector_size</span><span class="p">,</span>
                    <span class="n">slot_num</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span><span class="p">,</span>
                    <span class="n">nnz_per_slot</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nnz_per_slot</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dense_layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_of_dense_layers</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dense_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># its shape is [batchsize, slot_num, nnz_per_slot, embedding_vector_size]</span>
        <span class="n">emb_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># reshape this tensor, so that it can be processed by Dense layer</span>
        <span class="n">emb_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">emb_vector</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz_per_slot</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_vector_size</span><span class="p">])</span>

        <span class="n">hidden</span> <span class="o">=</span> <span class="n">emb_vector</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_layers</span><span class="p">:</span>
            <span class="n">hidden</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>

        <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logit</span>
</pre></div>
</div>
<p><strong>Functional API approach</strong>. The following code sample shows how to create a model with the <code class="docutils literal notranslate"><span class="pre">TensorFlow</span> <span class="pre">functional</span> <span class="pre">API</span></code>. For information about the API, see the <a class="reference external" href="https://tensorflow.google.cn/guide/keras/functional">TensorFlow functional API</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="k">def</span> <span class="nf">create_DemoModel</span><span class="p">(</span><span class="n">max_vocabulary_size_per_gpu</span><span class="p">,</span>
                     <span class="n">slot_num</span><span class="p">,</span>
                     <span class="n">nnz_per_slot</span><span class="p">,</span>
                     <span class="n">embedding_vector_size</span><span class="p">,</span>
                     <span class="n">num_of_dense_layers</span><span class="p">):</span>
    <span class="c1"># config the placeholder for embedding layer</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span>
                <span class="n">type_spec</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">slot_num</span><span class="p">,</span> <span class="n">nnz_per_slot</span><span class="p">),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>

    <span class="c1"># create embedding layer and produce embedding vector</span>
    <span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">All2AllDenseEmbedding</span><span class="p">(</span>
                <span class="n">max_vocabulary_size_per_gpu</span><span class="o">=</span><span class="n">max_vocabulary_size_per_gpu</span><span class="p">,</span>
                <span class="n">embedding_vec_size</span><span class="o">=</span><span class="n">embedding_vector_size</span><span class="p">,</span>
                <span class="n">slot_num</span><span class="o">=</span><span class="n">slot_num</span><span class="p">,</span>
                <span class="n">nnz_per_slot</span><span class="o">=</span><span class="n">nnz_per_slot</span><span class="p">)</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>

    <span class="c1"># create dense layers and produce logit</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span>
                <span class="n">target_shape</span><span class="o">=</span><span class="p">(</span><span class="n">slot_num</span> <span class="o">*</span> <span class="n">nnz_per_slot</span> <span class="o">*</span> <span class="n">embedding_vector_size</span><span class="p">,))(</span><span class="n">embedding</span><span class="p">)</span>

    <span class="n">hidden</span> <span class="o">=</span> <span class="n">embedding</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_of_dense_layers</span><span class="p">):</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">hidden</span><span class="p">)</span>
    <span class="n">logit</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">logit</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</section>
<section id="use-sparseoperationkit-with-tf-distribute-strategy">
<h3>Use SparseOperationKit with tf.distribute.Strategy<a class="headerlink" href="#use-sparseoperationkit-with-tf-distribute-strategy" title="Permalink to this heading"></a></h3>
<p>SparseOperationKit is compatible with <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code>. More specificly, <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.distribute.MultiWorkerMirroredStrategy</span></code>.</p>
<section id="with-tf-distribute-mirroredstrategy">
<h4>with tf.distribute.MirroredStrategy<a class="headerlink" href="#with-tf-distribute-mirroredstrategy" title="Permalink to this heading"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code> class enables data-parallel synchronized training on a machine with multiple GPUs. For more information, see the TensorFlow documentation for the <a class="reference external" href="https://tensorflow.google.cn/api_docs/python/tf/distribute/MirroredStrategy">MirroredStrategy</a> class.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The programming model for MirroredStrategy is single-process &amp; multiple-threads. CPython is prone to Global Interpreter Lock (GIL). GIL makes it hard to fully leverage all available CPU cores, which might impact the end-to-end training / inference performance. Therefore, the MirroredStrategy is not recommended for synchronized training using multiple GPUs.</p>
</div>
<p><em><strong>create MirroredStrategy</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By default, MirroredStrategy will use all available GPUs in one machine. You can select which GPUs should be used for synchronized training by specifying either <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.config.set_visible_devices</span></code>.</p>
</div>
<p><em><strong>create model instance under MirroredStrategy.scope</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">65536</span>
<span class="n">use_tf_opt</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
    <span class="n">sok</span><span class="o">.</span><span class="n">Init</span><span class="p">(</span><span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">DemoModel</span><span class="p">(</span>
        <span class="n">max_vocabulary_size_per_gpu</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">slot_num</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">nnz_per_slot</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">embedding_vector_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">num_of_dense_layers</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_tf_opt</span><span class="p">:</span>
        <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">dense_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p>Prior to using a DNN model that is built with SOK, you must call <code class="docutils literal notranslate"><span class="pre">sok.Init</span></code> to initalize SOK. Please refer to its <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/init.html#module-sparse_operation_kit.core.initialize">API document</a> for further information.</p>
<p><em><strong>define training step</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">NONE</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_replica_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">compute_average_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">_train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">_replica_loss</span><span class="p">(</span><span class="n">lables</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="n">emb_var</span><span class="p">,</span> <span class="n">other_var</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">split_embedding_variable_from_others</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">grads</span><span class="p">,</span> <span class="n">emb_grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">other_var</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">use_tf_opt</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">sok</span><span class="o">.</span><span class="n">OptimizerScope</span><span class="p">(</span><span class="n">emb_var</span><span class="p">):</span>
            <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">),</span>
                                    <span class="n">experimental_aggregate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">),</span>
                                <span class="n">experimental_aggregate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">dense_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">other_var</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you are using native TensorFlow optimizers, such as <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>, then <code class="docutils literal notranslate"><span class="pre">sok.OptimizerScope</span></code> must be used. Please refer to its <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/utils/opt_scope.html#sparseoperationkit-optimizer-scope">API document</a> for further information.</p>
<p><em><strong>start training</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="n">replica_loss</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">_train_step</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">replica_loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[SOK INFO]: Iteration: </span><span class="si">{}</span><span class="s2">, loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">))</span>
</pre></div>
</div>
<p>After these steps, the <code class="docutils literal notranslate"><span class="pre">DemoModel</span></code> will be successfully trained.</p>
</section>
<section id="with-tf-distribute-multiworkermirroredstrategy">
<h4>With tf.distribute.MultiWorkerMirroredStrategy<a class="headerlink" href="#with-tf-distribute-multiworkermirroredstrategy" title="Permalink to this heading"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.MultiWorkerMirroredStrategy</span></code> allows data-parallel synchronized training across multiple machines with multiple GPUs in each machine. Its <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy">documentation</a> can be found <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy">here</a>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The programming model for the MultiWorkerMirroredStrategy is multiple processes plus multi-threading. Hence, each process owns multiple threads to control the indidvidual GPUs in each machine. GILs in the CPython interpreter can make it hard to fully leverage all available CPU cores in each machine, which might impact the end-to-end training / inference performance. Therefore, it is recommended to use multiple processes in each machine, and each process controls one GPU.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>By default, MultiWorkerMirroredStrategy will use all available GPUs in each process. You can limit GPU access for each process by setting either <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.config.set_visible_devices</span></code>.</p>
</div>
<p><em><strong>create MultiWorkerMirroredStrategy</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">json</span>

<span class="n">worker_num</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># how many GPUs are used</span>
<span class="n">task_id</span> <span class="o">=</span> <span class="mi">0</span>    <span class="c1"># this process controls which GPU</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">task_id</span><span class="p">)</span> <span class="c1"># this procecss only controls this GPU</span>

<span class="n">port</span> <span class="o">=</span> <span class="mi">12345</span> <span class="c1"># could be arbitrary unused port on this machine</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TF_CONFIG&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span>
    <span class="s2">&quot;cluster&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;localhost:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">port</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">worker_num</span><span class="p">)]},</span>
    <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;worker&quot;</span><span class="p">,</span> <span class="s2">&quot;index&quot;</span><span class="p">:</span> <span class="n">task_id</span><span class="p">}</span>
<span class="p">})</span>
<span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>
</pre></div>
</div>
<p><em><strong>Other Steps</strong></em><br>
The steps <em><strong>create model instance under MultiWorkerMirroredStrategy.scope</strong></em>, <em><strong>define training step</strong></em> and <em><strong>start training</strong></em> are the same as those described in <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/get_started/get_started.html#with-tf-distribute-mirroredstrategy">with tf.distribute.MirroredStrategy</a>. Please check that section.</p>
<p><em><strong>launch training program</strong></em><br>
Because multiple CPU processes are used in each machine for synchronized training, MPI can be used to launch this program. For example using:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$ mpiexec -np <span class="m">8</span> <span class="o">[</span>mpi-args<span class="o">]</span> python3 main.py <span class="o">[</span>python-args<span class="o">]</span>
</pre></div>
</div>
</section>
</section>
<section id="use-sparseoperationkit-with-horovod">
<h3>Use SparseOperationKit with Horovod<a class="headerlink" href="#use-sparseoperationkit-with-horovod" title="Permalink to this heading"></a></h3>
<p>SparseOperationKit is also compatible with <a class="reference external" href="https://horovod.ai">Horovod</a>, which is similar to <code class="docutils literal notranslate"><span class="pre">tf.distribute.MultiWorkerMirroredStrategy</span></code>.</p>
<p><em><strong>initialize horovod for tensorflow</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">horovod.tensorflow</span> <span class="k">as</span> <span class="nn">hvd</span>
<span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">())</span> <span class="c1"># this process only controls one GPU</span>
</pre></div>
</div>
<p><em><strong>create model instance</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">65536</span>
<span class="n">use_tf_opt</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">sok</span><span class="o">.</span><span class="n">Init</span><span class="p">(</span><span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DemoModel</span><span class="p">(</span><span class="n">max_vocabulary_size_per_gpu</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
                  <span class="n">slot_num</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                  <span class="n">nnz_per_slot</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">embedding_vector_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                  <span class="n">num_of_dense_layers</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">use_tf_opt</span><span class="p">:</span>
    <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">dense_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p>Prior to using a DNN model built with SOK, <code class="docutils literal notranslate"><span class="pre">sok.Init</span></code> must be called to perform certain initilization steps. Please refer to its <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/init.html#module-sparse_operation_kit.core.initialize">API document</a> for further information.</p>
<p><em><strong>define training step</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">NONE</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_replica_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">compute_average_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">_train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">first_batch</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">_replica_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="n">emb_var</span><span class="p">,</span> <span class="n">other_var</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">split_embedding_variable_from_others</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">emb_grads</span><span class="p">,</span> <span class="n">other_grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">emb_var</span><span class="p">,</span> <span class="n">other_var</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">use_tf_opt</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">sok</span><span class="o">.</span><span class="n">OptimizerScope</span><span class="p">(</span><span class="n">emb_var</span><span class="p">):</span>
            <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">),</span>
                                    <span class="n">experimental_aggregate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">),</span>
                                <span class="n">experimental_aggregate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">other_grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span> <span class="k">for</span> <span class="n">grads</span> <span class="ow">in</span> <span class="n">other_grads</span><span class="p">]</span>
    <span class="n">dense_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">other_grads</span><span class="p">,</span> <span class="n">other_var</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">first_batch</span><span class="p">:</span>
        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">other_var</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">dense_opt</span><span class="o">.</span><span class="n">variables</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you use native TensorFlow optimizers, such as <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>, then <code class="docutils literal notranslate"><span class="pre">sok.OptimizerScope</span></code> must be used. Please see its <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/utils/opt_scope.html#sparseoperationkit-optimizer-scope">API document</a> for further information.</p>
<p><em><strong>start training</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="n">replica_loss</span> <span class="o">=</span> <span class="n">_train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="mi">0</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">replica_loss</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[SOK INFO]: Iteration: </span><span class="si">{}</span><span class="s2">, loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">))</span>
</pre></div>
</div>
<p><em><strong>launch training program</strong></em><br>
You can use <code class="docutils literal notranslate"><span class="pre">horovodrun</span></code> or <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code> to launch multiple processes in each machine for synchronized training. For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$ horovodrun -np <span class="m">8</span> -H localhost:8 python3 main.py <span class="o">[</span>python-args<span class="o">]</span>
</pre></div>
</div>
</section>
</section>
<section id="tensorflow-1-15">
<h2>TensorFlow 1.15<a class="headerlink" href="#tensorflow-1-15" title="Permalink to this heading"></a></h2>
<p>SOK is compatible with TensorFlow 1.15. But due to some restrictions in TF 1.15, only Horovod can be used as the communication protocol.</p>
<section id="using-sparseoperationkit-with-horovod">
<h3>Using SparseOperationKit with Horovod<a class="headerlink" href="#using-sparseoperationkit-with-horovod" title="Permalink to this heading"></a></h3>
<p><em><strong>initialize horovod for tensorflow</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">horovod.tensorflow</span> <span class="k">as</span> <span class="nn">hvd</span>
<span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">())</span> <span class="c1"># this process only controls one GPU</span>
</pre></div>
</div>
<p><em><strong>create model instance</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">65536</span>
<span class="n">use_tf_opt</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">sok_init_op</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">Init</span><span class="p">(</span><span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DemoModel</span><span class="p">(</span><span class="n">max_vocabulary_size_per_gpu</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
                  <span class="n">slot_num</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                  <span class="n">nnz_per_slot</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">embedding_vector_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                  <span class="n">num_of_dense_layers</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">use_tf_opt</span><span class="p">:</span>
    <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">dense_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p>Prior to using a DNN model built with SOK, <code class="docutils literal notranslate"><span class="pre">sok.Init</span></code> must be called to perform certain initilization steps. Please refer to its <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/init.html#module-sparse_operation_kit.core.initialize">API document</a> for further information.</p>
<p><em><strong>define training step</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_replica_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">compute_average_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">_replica_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logit</span><span class="p">)</span>
    <span class="n">emb_var</span><span class="p">,</span> <span class="n">other_var</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">split_embedding_variable_from_others</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">emb_var</span> <span class="o">+</span> <span class="n">other_var</span><span class="p">,</span> <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">emb_grads</span><span class="p">,</span> <span class="n">other_grads</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">emb_var</span><span class="p">)],</span> <span class="n">grads</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">emb_var</span><span class="p">):]</span>

    <span class="k">if</span> <span class="n">use_tf_opt</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">sok</span><span class="o">.</span><span class="n">OptimizerScope</span><span class="p">(</span><span class="n">emb_var</span><span class="p">):</span>
            <span class="n">emb_train_op</span> <span class="o">=</span> <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">emb_train_op</span> <span class="o">=</span> <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">))</span>

    <span class="n">other_grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">other_grads</span><span class="p">]</span>
    <span class="n">other_train_op</span> <span class="o">=</span> <span class="n">dense_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">other_grads</span><span class="p">,</span> <span class="n">other_var</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">emb_train_op</span><span class="p">,</span> <span class="n">other_train_op</span><span class="p">]):</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">total_loss</span>
</pre></div>
</div>
<p>If you are using native TensorFlow optimizers, such as <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>, then <code class="docutils literal notranslate"><span class="pre">sok.OptimizerScope</span></code> must be used. Please see its <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/utils/opt_scope.html#sparseoperationkit-optimizer-scope">API document</a> for further information.</p>
<p><em><strong>start training</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">(),</span>
                   <span class="n">tf</span><span class="o">.</span><span class="n">local_variables_initializer</span><span class="p">())</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">sok_init_op</span><span class="p">)</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">loss_v</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[SOK INFO]: Iteration: </span><span class="si">{}</span><span class="s2">, loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">loss_v</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Please be noted that <code class="docutils literal notranslate"><span class="pre">sok_init_op</span></code> must be the first step in <code class="docutils literal notranslate"><span class="pre">sess.run</span></code>, even before variables initialization.</strong></p>
<p><em><strong>launch training program</strong></em>
You can use <code class="docutils literal notranslate"><span class="pre">horovodrun</span></code> or <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code> to launch multiple processes in each machine for synchronized training. For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$ horovodrun -np <span class="m">8</span> -H localhost:8 python main.py <span class="o">[</span>args<span class="o">]</span>
</pre></div>
</div>
</section>
</section>
<section id="experimental-features">
<h2>Experimental Features<a class="headerlink" href="#experimental-features" title="Permalink to this heading"></a></h2>
<p>Currently, we use horovod for communication. So in the beginning, you need to import horovod and correctly bind a GPU to each process like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">horovod.tensorflow</span> <span class="k">as</span> <span class="nn">hvd</span>

<span class="kn">from</span> <span class="nn">sparse_operation_kit</span> <span class="kn">import</span> <span class="n">experiment</span> <span class="k">as</span> <span class="n">sok</span>


<span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()],</span> <span class="s2">&quot;GPU&quot;</span><span class="p">)</span>  <span class="c1"># nopep8</span>

<span class="n">sok</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
</pre></div>
</div>
<p>Next, in order to use the distributed embedding op, you need to create a variable on each process that represents a portion of the entire embedding table, whose shape is also a subset of the full embedding table. We provide a tensorflow variable wrapper to help you simplify this process.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Default mode of sok.Variable is Distributed mode</span>
<span class="c1"># If there are 2 GPUs in total, the shape of v1 on GPU0 will be [9, 3] and the shape</span>
<span class="c1"># on GPU1 will be [8, 3]</span>
<span class="n">v1</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">17</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">v2</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">7</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;v1:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;v2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, create the indices for the embedding lookup. This step is no different from the normal tensorflow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">indices1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span>
    <span class="n">indices</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dense_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;indices1:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">indices1</span><span class="p">)</span>
<span class="c1"># indices1: batch_size=2, max_hotness=3</span>
<span class="c1"># [[1, 1]</span>
<span class="c1">#  [3, 4, 5]]</span>

<span class="n">indices2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span>
    <span class="n">indices</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dense_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;indices2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">indices2</span><span class="p">)</span>
<span class="c1"># indices2: batch_size=2, max_hotness=2</span>
<span class="c1"># [[1]</span>
<span class="c1">#  [2, 3]]</span>
</pre></div>
</div>
<p>Then, use sok’s embedding op to do the lookup. Note that here we pass two embedding variables and two indices into the lookup at the same time through a list, this fused operation will bring performance gain for us.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">lookup_sparse</span><span class="p">(</span>
        <span class="p">[</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">],</span> <span class="p">[</span><span class="n">indices1</span><span class="p">,</span> <span class="n">indices2</span><span class="p">],</span> <span class="n">hotness</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">combiners</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">embeddings</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;embedding</span><span class="si">%d</span><span class="s2">:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">embedding</span><span class="p">)</span>
    <span class="c1"># embedding1: [[6,  8,  10]</span>
    <span class="c1">#              [36, 39, 42]]</span>
    <span class="c1"># embedding2: [[5,  6,  7,  8,  9</span>
    <span class="c1">#              [25, 27, 29, 31, 33]]</span>
</pre></div>
</div>
<p>Finally, update the variable like normal tensorflow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If there are 2 GPUs in total</span>
<span class="c1"># GPU0:</span>
<span class="c1">#   In Distributed mode: shape of grad of v1 will be [1, 3], shape of grad of v2 will be [1, 5]</span>
<span class="c1">#   In Localized mode: shape of grad of v1 will be [4, 3], grad of v2 will None</span>
<span class="c1"># GPU1:</span>
<span class="c1">#   In Distributed mode: shape of grad of v1 will be [3, 3], shape of grad of v2 will be [2, 5]</span>
<span class="c1">#   In Localized mode: grad of v1 will be None, shape of grad of v2 will be [3, 5]</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grads</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;grad</span><span class="si">%d</span><span class="s2">:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">grad</span><span class="p">)</span>

<span class="c1"># Use tf.keras.optimizer to optimize the sok.Variable</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="p">[</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;v1:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;v2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>
</pre></div>
</div>
<p>For more examples and API descriptions see the Example section and API section.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../intro_link.html" class="btn btn-neutral float-left" title="SparseOperationKit" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../examples/index.html" class="btn btn-neutral float-right" title="Examples and Tutorials" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    master: sok_v1.1.4
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../sok_v1.0.0/get_started/get_started.html">sok_v1.0.0</a></dd>
      <dd><a href="../../sok_v1.1.0/get_started/get_started.html">sok_v1.1.0</a></dd>
      <dd><a href="../../sok_v1.1.1/get_started/get_started.html">sok_v1.1.1</a></dd>
      <dd><a href="../../sok_v1.1.2/get_started/get_started.html">sok_v1.1.2</a></dd>
      <dd><a href="get_started.html">sok_v1.1.4</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>