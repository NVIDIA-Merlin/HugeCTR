# DCN CTR SAMPLE #
A sample of building and training Deep & Cross Network with HugeCTR [(link)](https://arxiv.org/pdf/1708.05123.pdf).

## Dataset and preprocess ##
The data is provided by CriteoLabs (http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/).
The original training set contains 45,840,617 examples.
Each example contains a label (1 if the ad was clicked, otherwise 0) and 39 features (13 integer features and 26 categorical features).
The dataset also has the significant amounts of missing values across the feature columns, which should be preprocessed accordingly.
The original test set doesn't contain labels, so it's not used.

### Requirements ###
* Python >= 3.6.9
* Pandas 1.0.1
* Sklearn 0.22.1

### 1. Download the dataset and preprocess

Go to [(link)](http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/)
and download the kaggle-display dataset into the folder "${project_home}/tools/criteo_script/".
The script `preprocess.sh` fills the missing values by mapping them to the unused unique integer or category.
It also replaces unique values which appear less than six times across the entire dataset with the unique value for missing values.
Its purpose is to reduce the vocabulary size of each column while not losing too much information.
In addition, it normalizes the integer feature values to the range [0, 1],
but it doesn't create any feature crosses. Please choose one of the following two methods for data preprocessing.

#### Preprocessing by Pandas ####
```shell
# The preprocessing can take 40 minutes to 1 hour based on the system configuration.
$ cd ../../tools/criteo_script/
$ bash preprocess.sh dcn 1 0
$ cd ../../samples/dcn/
```
Convert the dataset to HugeCTR format
```shell
$ cp ../../build/bin/criteo2hugectr ./
$ ./criteo2hugectr ../../tools/criteo_script/dcn_data/train criteo/sparse_embedding file_list.txt
$ ./criteo2hugectr ../../tools/criteo_script/dcn_data/val criteo_test/sparse_embedding file_list_test.txt
```

#### Preprocessing by NVTabular ####

HugeCTR supports data processing by NVTabular since version 2.2.1. Please make sure NVTabular docker environment has been set up successfully according to [NVTAbular github](https://github.com/NVIDIA/NVTabular). Make sure to use the latest version(0.2) of NVTabular.
And bind mount HugeCTR ${project_home} volume to NVTabular docker. Run NVTabular docker and execute the following preprocessing commands.
Go to [(link)](http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/)
download kaggle-display dataset into the folder "${project_home}/samples/dcn/". 
```shell
$ tar zxvf dac.tar.gz 
$ mkdir -p dcn_data/train
$ mkdir -p dcn_data/val 
$ head -n 36672493 train.txt > dcn_data/train/train.txt 
$ tail -n 9168124 train.txt > dcn_data/val/test.txt 
$ cp ../../tools/criteo_script/preprocess_nvt.py ./
#--help:show help message and explan usage of each parameters.
#--parquet_format=1 The default output of NVTabular is the parquet format, if need the norm binary format, please add argument with 0
#--device_limit_frac：Worker device-memory limit as a fraction of GPU capacity, which should be determined by the gpu with the leatest memory
#--device_pool_frac：The RMM pool frac is the same for all GPUs, make sure each one has enough memory size
#--num_io_threads: Number of threads to use when writing output data.
$ python3 preprocess_nvt.py --data_path dcn_data/train/train.txt --out_path dcn_data/train/ --freq_limit 6 --device_limit_frac 0.2 --device_pool_frac 0.2 --out_files_per_proc 8  --devices "0" --num_io_threads 2 

$ python3 preprocess_nvt.py --data_path dcn_data/val/test.txt --out_path dcn_data/val/ --freq_limit 6 --device_limit_frac 0.2 --device_pool_frac 0.2 --out_files_per_proc 8  --devices "0" --num_io_threads 2
```
- **NOTE**: If you want to generate a `Raw` format data, use `--parquet_format=0`. Otherwise, the parquet format data is generated by default.
- **NOTE**: You may want to change `--out_path` in case where multiple datasets, e.g., with different formats, must be generated.
- **NOTE**: If you change `--out_path`, don't forget to change `source` and `eval_source` in your JSON config file as well.

Exit from the NVTabular docker environment and then run HugeCTR docker with interaction mode under home directory again.

### 2. Build HugeCTR with the instructions on README.md under home directory.


## Training with HugeCTR ##

1. Copy huge_ctr to samples/dcn
```shell
$ cp ../../build/bin/huge_ctr ./
```

2. Run huge_ctr

#### For Pandas Preprocessing ####
```shell
$ ./huge_ctr --train ./dcn.json
```

#### For NVTabular Preprocessing ####

Parquet output
```shell
$ ./huge_ctr --train ./dcn_parquet.json
```

Binary output
```shell
$ ./huge_ctr --train ./dcn_bin.json
```

## Training with localized slot embedding ##

1. Plan file generation

If gossip communication library is used, a plan file is needed to be generated first as below. If NCCL communication library is used, there is no need to generate a plan file, just go to step 2. 
```shell
$ export CUDA_DEVICE_ORDER=PCI_BUS_ID
$ python3 ../../tools/plan_generation_no_mpi/plan_generator_no_mpi.py dcn_localized_embedding.json
```

2. Run huge_ctr
```shell
$ ./huge_ctr --train dcn_localized_embedding.json
```

