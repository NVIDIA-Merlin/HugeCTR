<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Hierarchical Parameter Server Plugin for TensorRT &mdash; Merlin HugeCTR  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />

  
    <link rel="canonical" href="https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_trt_user_guide.html" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="HPS Plugin for TensorRT API" href="hps_trt_api/index.html" />
    <link rel="prev" title="HPS Layers" href="hps_tf_api/layers.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Since the HugeCTR <code>v23.09</code>, the offline inference has been deprecated.
      Since the HugeCTR <code>v24.06</code>, the HPS has been deprecated.
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">HUGECTR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Hierarchical Parameter Server</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="hps_database_backend.html">HPS Database Backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_tf_user_guide.html">HPS Plugin for TensorFlow</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">HPS Plugin for TensorRT</a><ul>
<li class="toctree-l3"><a class="reference internal" href="hps_trt_api/index.html">API Documentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hps_torch_user_guide.html">HPS Plugin for Torch</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_dlrm_benchmark.html">Benchmark HPS-integrated DLRM</a></li>
<li class="toctree-l2"><a class="reference internal" href="profiling_hps.html">Profiling HPS</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../sparse_operation_kit.html">Sparse Operation Kit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Hierarchical Parameter Server</a></li>
      <li class="breadcrumb-item active">Hierarchical Parameter Server Plugin for TensorRT</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="hierarchical-parameter-server-plugin-for-tensorrt">
<h1>Hierarchical Parameter Server Plugin for TensorRT<a class="headerlink" href="#hierarchical-parameter-server-plugin-for-tensorrt" title="Permalink to this heading"></a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction-to-the-hps-plugin-for-tensorrt" id="id1">Introduction to the HPS Plugin for TensorRT</a></p></li>
<li><p><a class="reference internal" href="#workflow" id="id2">Workflow</a></p></li>
<li><p><a class="reference internal" href="#installation" id="id3">Installation</a></p>
<ul>
<li><p><a class="reference internal" href="#compute-capability" id="id4">Compute Capability</a></p></li>
<li><p><a class="reference internal" href="#installing-hps-using-ngc-containers" id="id5">Installing HPS Using NGC Containers</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#example-notebooks" id="id6">Example Notebooks</a></p></li>
<li><p><a class="reference internal" href="#benchmark" id="id7">Benchmark</a></p></li>
</ul>
</nav>
<section id="introduction-to-the-hps-plugin-for-tensorrt">
<h2>Introduction to the HPS Plugin for TensorRT<a class="headerlink" href="#introduction-to-the-hps-plugin-for-tensorrt" title="Permalink to this heading"></a></h2>
<p>Hierarchical Parameter Server (HPS) is a distributed inference framework that is dedicated to deploying large embedding tables and realizing the low-latency retrieval of embeddings.
The framework combines a high-performance GPU embedding cache with a hierarchical storage architecture that encompasses different types of database backends.
The HPS plugin for TensorRT can be integrated into the TensorRT network as a custom layer to build the engine. The TensorRT engine with HPS Plugin for TensorRT can perform low-latency embedding lookup for large tables and accelerated forward propagation for dense network at the same time.</p>
</section>
<section id="workflow">
<h2>Workflow<a class="headerlink" href="#workflow" title="Permalink to this heading"></a></h2>
<a class="reference internal image-reference" href="../_images/workflow1.png"><img alt="../_images/workflow1.png" class="align-center" src="../_images/workflow1.png" style="width: 1080px;" /></a>
<div align=center>Fig. 1: Workflow of using HPS plugin for TensorRT </div>
<p><br></br></p>
<p>The workflow to leverage the HPS plugin for TensorRT is shown in Fig. 1:</p>
<ul class="simple">
<li><p><strong>Convert trained models to ONNX</strong>: The models trained with different frameworks are converted to ONNX using the popular tools <a class="reference external" href="https://github.com/onnx/tensorflow-onnx">tf2onnx</a>, <a class="reference external" href="https://github.com/pytorch/pytorch/tree/master/torch/onnx">torch.onnx</a>, <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/main/onnx_converter">hugectr2onnx</a>, and so on.</p></li>
<li><p><strong>Perform ONNX graph surgery</strong>: The node for embedding lookup in the ONNX graph is replaced by the placeholder of HPS plugin for TensorRT using the tool <a class="reference external" href="https://github.com/NVIDIA/TensorRT/tree/main/tools/onnx-graphsurgeon">ONNX GraphSurgeon</a>, as shown in Fig. 2.</p></li>
<li><p><strong>Build the TensorRT engine with HPS Plugin for TensorRT</strong>: We can build the TensorRT engine based on the modified ONNX graph where the HPS can leveraged as a custom plugin layer.</p></li>
<li><p><strong>Deploy the engine on the Triton backend for TensorRT</strong>: The TensorRT engine with HPS Plugin for TensorRT is deployed on the Triton backend for TensorRT. Set the <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD=/usr/local/hps_trt/lib/libhps_plugin.so</span></code> environment variable to load the plugin shared library when you start Triton Inference Server.</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/graph_surgeon.png"><img alt="Logical diagram of using ONNX GraphSurgeon to set the embedding lookup to the HPS plugin for TensorRT" src="../_images/graph_surgeon.png" style="width: 720px;" /></a>
<div align=center>Fig. 2: ONNX Graph Surgery </div>
<p><br></br></p>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading"></a></h2>
<section id="compute-capability">
<h3>Compute Capability<a class="headerlink" href="#compute-capability" title="Permalink to this heading"></a></h3>
<p>The plugin supports the following compute capabilities:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Compute Capability</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>SM</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>7.0</p></td>
<td><p>NVIDIA V100 (Volta)</p></td>
<td><p>70</p></td>
</tr>
<tr class="row-odd"><td><p>7.5</p></td>
<td><p>NVIDIA T4 (Turing)</p></td>
<td><p>75</p></td>
</tr>
<tr class="row-even"><td><p>8.0</p></td>
<td><p>NVIDIA A100 (Ampere)</p></td>
<td><p>80</p></td>
</tr>
<tr class="row-odd"><td><p>9.0</p></td>
<td><p>NVIDIA H100 (Hopper)</p></td>
<td><p>90</p></td>
</tr>
</tbody>
</table>
</section>
<section id="installing-hps-using-ngc-containers">
<h3>Installing HPS Using NGC Containers<a class="headerlink" href="#installing-hps-using-ngc-containers" title="Permalink to this heading"></a></h3>
<p>All NVIDIA Merlin components are available as open source projects. However, a more convenient way to use these components is by using our Merlin NGC containers. These containers allow you to package your software application, libraries, dependencies, and runtime compilers in a self-contained environment. When installing HPS using NGC containers, the application environment remains portable, consistent, reproducible, and agnostic to the underlying host system’s software configuration.</p>
<p>HPS is included in the Merlin Docker containers that are available from the NVIDIA GPU Cloud (NGC) catalog.
Access the catalog of containers at <a class="reference external" href="https://catalog.ngc.nvidia.com/containers">https://catalog.ngc.nvidia.com/containers</a>.
To use these Docker containers, you must install the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> to provide GPU support for Docker.</p>
<p>The following sample commands pull and start the Merlin TensorFlow container, Merlin PyTorch container, or Merlin HugeCTR container:</p>
<p>Merlin TensorFlow</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run the container in interactive mode</span>
$<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>--gpus<span class="o">=</span>all<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--cap-add<span class="w"> </span>SYS_NICE<span class="w"> </span>nvcr.io/nvidia/merlin/merlin-tensorflow:23.02
</pre></div>
</div>
<p>Merlin PyTorch</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run the container in interactive mode</span>
$<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>--gpus<span class="o">=</span>all<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--cap-add<span class="w"> </span>SYS_NICE<span class="w"> </span>nvcr.io/nvidia/merlin/merlin-pytorch:23.02
</pre></div>
</div>
<p>Merlin HugeCTR</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run the container in interactive mode</span>
$<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>--gpus<span class="o">=</span>all<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--cap-add<span class="w"> </span>SYS_NICE<span class="w"> </span>nvcr.io/nvidia/merlin/merlin-hugectr:23.02
</pre></div>
</div>
<p>You can check the existence of the HPS plugin for TensorRT after launching the container by running the following Python statements:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ctypes</span>
<span class="n">handle</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="s2">&quot;/usr/local/hps_trt/lib/libhps_plugin.so&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">ctypes</span><span class="o">.</span><span class="n">RTLD_GLOBAL</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="example-notebooks">
<h2>Example Notebooks<a class="headerlink" href="#example-notebooks" title="Permalink to this heading"></a></h2>
<p>We provide a collection of examples as <a class="reference internal" href="#../hps_trt/notebooks/index.md"><span class="xref myst">Jupyter Notebooks</span></a> that demonstrate how to build the TensorRT engine with HPS Plugin for TensorRT for models trained with TensorFlow, PyTorch, or HugeCTR.</p>
</section>
<section id="benchmark">
<h2>Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this heading"></a></h2>
<p>We benchmark the DLRM TensorRT engine with HPS Plugin for TensorRT in <a class="reference internal" href="hps_dlrm_benchmark.html"><span class="std std-doc">hps_dlrm_benchmark.md</span></a>.</p>
</section>
<div class="toctree-wrapper compound">
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hps_tf_api/layers.html" class="btn btn-neutral float-left" title="HPS Layers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hps_trt_api/index.html" class="btn btn-neutral float-right" title="HPS Plugin for TensorRT API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v24.04.00
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../v23.08.00/hierarchical_parameter_server/hps_trt_user_guide.html">v23.08.00</a></dd>
      <dd><a href="../../v23.09.00/hierarchical_parameter_server/hps_trt_user_guide.html">v23.09.00</a></dd>
      <dd><a href="../../v23.12.00/hierarchical_parameter_server/hps_trt_user_guide.html">v23.12.00</a></dd>
      <dd><a href="hps_trt_user_guide.html">v24.04.00</a></dd>
      <dd><a href="../../v24.06.00/hierarchical_parameter_server/hps_trt_user_guide.html">v24.06.00</a></dd>
      <dd><a href="../../v25.03.00/index.html">v25.03.00</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../main/index.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>