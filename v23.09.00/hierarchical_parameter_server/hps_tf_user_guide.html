<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Hierarchical Parameter Server Plugin for TensorFlow &mdash; Merlin HugeCTR  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />

  
    <link rel="canonical" href="https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_tf_user_guide.html" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hierarchical Parameter Server API" href="hps_tf_api/index.html" />
    <link rel="prev" title="Hierarchical Parameter Server Database Backend" href="hps_database_backend.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Since the HugeCTR <code>v23.09</code>, the offline inference has been deprecated.
      Since the HugeCTR <code>v24.06</code>, the HPS has been deprecated.
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">HUGECTR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_embedding_training_cache.html">Embedding Training Cache</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Hierarchical Parameter Server</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="hps_database_backend.html">HPS Database Backend</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">HPS Plugin for TensorFlow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="hps_tf_api/index.html">API Documentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hps_trt_user_guide.html">HPS Plugin for TensorRT</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_torch_user_guide.html">HPS Plugin for Torch</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_dlrm_benchmark.html">Benchmark HPS-integrated DLRM</a></li>
<li class="toctree-l2"><a class="reference internal" href="profiling_hps.html">Profiling HPS</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../sparse_operation_kit.html">Sparse Operation Kit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Hierarchical Parameter Server</a></li>
      <li class="breadcrumb-item active">Hierarchical Parameter Server Plugin for TensorFlow</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="hierarchical-parameter-server-plugin-for-tensorflow">
<h1>Hierarchical Parameter Server Plugin for TensorFlow<a class="headerlink" href="#hierarchical-parameter-server-plugin-for-tensorflow" title="Permalink to this heading"></a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction-to-the-hps-plugin-for-tensorflow" id="id1">Introduction to the HPS Plugin for TensorFlow</a></p></li>
<li><p><a class="reference internal" href="#benefits-of-the-plugin-for-tensorflow" id="id2">Benefits of the Plugin for TensorFlow</a></p></li>
<li><p><a class="reference internal" href="#workflow" id="id3">Workflow</a></p></li>
<li><p><a class="reference internal" href="#installation" id="id4">Installation</a></p>
<ul>
<li><p><a class="reference internal" href="#compute-capability" id="id5">Compute Capability</a></p></li>
<li><p><a class="reference internal" href="#installing-hps-using-ngc-containers" id="id6">Installing HPS Using NGC Containers</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#example-notebooks" id="id7">Example Notebooks</a></p></li>
<li><p><a class="reference internal" href="#benchmark" id="id8">Benchmark</a></p></li>
</ul>
</nav>
<section id="introduction-to-the-hps-plugin-for-tensorflow">
<h2>Introduction to the HPS Plugin for TensorFlow<a class="headerlink" href="#introduction-to-the-hps-plugin-for-tensorflow" title="Permalink to this heading"></a></h2>
<p>Hierarchical Parameter Server (HPS) is a distributed inference framework that is dedicated to deploying large embedding tables and realizing the low-latency retrieval of embeddings.
The framework combines a high-performance GPU embedding cache with a hierarchical storage architecture that encompasses different types of database backends.
The plugin is provided as a Python toolkit that you can integrate easily into the TensorFlow (TF) model graph.
Integration with the graph facilitates the TensorFlow model deployment of large embedding tables.</p>
</section>
<section id="benefits-of-the-plugin-for-tensorflow">
<h2>Benefits of the Plugin for TensorFlow<a class="headerlink" href="#benefits-of-the-plugin-for-tensorflow" title="Permalink to this heading"></a></h2>
<p>When you deploy deep learning models with large embedding tables in TensorFlow, you are faced with the following challenges:</p>
<ul class="simple">
<li><p><strong>Large Embedding Tables</strong>: Trained embedding tables of hundreds of gigabytes cannot fit into the GPU memory.</p></li>
<li><p><strong>Low Latency Requirement</strong>: Online inference requires that the latency of embedding lookup should be low to maintain the quality of experience and the user engagement.</p></li>
<li><p><strong>Scalability on multiple GPUs</strong>: Dozens of models need to be deployed on multiple GPUs and each model can have several embedding tables.</p></li>
<li><p><strong>Pre-trained embeddings</strong>: Large embedding tables need to be loaded as pre-trained embeddings for tasks like transfer learning.</p></li>
</ul>
<p>The HPS plugin for TensorFlow mitigates these challenges and helps in the following ways:</p>
<ul class="simple">
<li><p>Extend the GPU memory by utilizing other memory resources available within the cluster, such as CPU-accessible RAM and non-volatile memory such as HDDs and SDDs, as shown in Fig. 1.</p></li>
<li><p>Use the GPU embedding cache to exploit the long-tail characteristics of the keys. The cache automatically stores the embeddings for hot keys as queries are constantly received, providing the low-latency lookup service.</p></li>
<li><p>Manage the embedding tables of multiple models in a structured manner across the whole memory hierarchy of GPUs, CPUs, and SSDs.</p></li>
<li><p>Make the lookup service subscribable through custom TensorFlow layers, enabling transfer learning with large embedding tables.</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/memory_hierarchy.png"><img alt="../_images/memory_hierarchy.png" class="align-center" src="../_images/memory_hierarchy.png" style="width: 1080px;" /></a>
<div align=center>Fig. 1: HPS Memory Hierarchy </div>
<p><br></br></p>
</section>
<section id="workflow">
<h2>Workflow<a class="headerlink" href="#workflow" title="Permalink to this heading"></a></h2>
<p>The workflow of leveraging HPS for deployment of TensorFlow models is illustrated in Fig. 2.</p>
<a class="reference internal image-reference" href="../_images/workflow.png"><img alt="../_images/workflow.png" class="align-center" src="../_images/workflow.png" style="width: 1080px;" /></a>
<div align=center>Fig. 2: Workflow of deploying TF models with HPS </div>
<p><br></br></p>
<p>The steps in the workflow can be summarized as:</p>
<ul class="simple">
<li><p><strong>Train</strong>: The model graph should be created with native TensorFlow embedding layers (e.g., <code class="docutils literal notranslate"><span class="pre">tf.nn.embedding_lookup_sparse</span></code>) or model parallelism enabled <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/index.html">SOK</a> embedding layers (e,g., <code class="docutils literal notranslate"><span class="pre">sok.DistributedEmbedding</span></code>). There is no restriction on the usage of dense layers or the topology of the model graph as long as the model can be successfully trained with TensorFlow.</p></li>
<li><p><strong>Dissect the training graph</strong>: The subgraph composided of only dense layers should be extracted from the trained graph, and then saved separately. For native TensorFlow embedding layers, the trained embedding weights should be obtained and converted to the HPS-compatible formats. For <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/index.html">SOK</a> embedding layers, <code class="docutils literal notranslate"><span class="pre">sok.Saver.dump_to_file</span></code> can be utilized to derive the desired formats. Basically, each embedding table should be stored in a directory with two binary files, i.e., <code class="docutils literal notranslate"><span class="pre">key</span></code> (int64) and <code class="docutils literal notranslate"><span class="pre">emb_vector</span></code> (float32). For example, if there are totally 1000 trained keys and the embedding vector size is 16, then the size of <code class="docutils literal notranslate"><span class="pre">key</span></code> file and the <code class="docutils literal notranslate"><span class="pre">emb_vector</span></code> file should be 1000*8 bytes and 1000*16*4 bytes respectively.</p></li>
<li><p><strong>Create and save the inference graph</strong>: The inference graph should be created with HPS layers (e.g., <code class="docutils literal notranslate"><span class="pre">hps.SparseLookupLayer</span></code>) and the saved subgraph of dense layers. It can be then saved as a whole so as to be deployed in the production environment.</p></li>
<li><p><strong>Deploy the inference graph with HPS</strong>: The configurations for the models to be deployed should be specified in a JSON file and the HPS should be started via <code class="docutils literal notranslate"><span class="pre">hps.Init</span></code> before any executions. The saved inference graph can be deployed to perform online inference leveraging the benefits of the HPS embedding lookup. Please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_parameter_server.html#configuration">HPS Configuration</a> for more information.</p></li>
</ul>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading"></a></h2>
<section id="compute-capability">
<h3>Compute Capability<a class="headerlink" href="#compute-capability" title="Permalink to this heading"></a></h3>
<p>We support the following compute capabilities:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Compute Capability</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>SM</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>7.0</p></td>
<td><p>NVIDIA V100 (Volta)</p></td>
<td><p>70</p></td>
</tr>
<tr class="row-odd"><td><p>7.5</p></td>
<td><p>NVIDIA T4 (Turing)</p></td>
<td><p>75</p></td>
</tr>
<tr class="row-even"><td><p>8.0</p></td>
<td><p>NVIDIA A100 (Ampere)</p></td>
<td><p>80</p></td>
</tr>
<tr class="row-odd"><td><p>9.0</p></td>
<td><p>NVIDIA H100 (Hopper)</p></td>
<td><p>90</p></td>
</tr>
</tbody>
</table>
</section>
<section id="installing-hps-using-ngc-containers">
<h3>Installing HPS Using NGC Containers<a class="headerlink" href="#installing-hps-using-ngc-containers" title="Permalink to this heading"></a></h3>
<p>All NVIDIA Merlin components are available as open source projects. However, a more convenient way to utilize these components is by using our Merlin NGC containers. These containers allow you to package your software application, libraries, dependencies, and runtime compilers in a self-contained environment. When installing HPS using NGC containers, the application environment remains portable, consistent, reproducible, and agnostic to the underlying host system’s software configuration.</p>
<p>HPS is included in the Merlin Docker containers that are available from the <a class="reference external" href="https://catalog.ngc.nvidia.com/containers">NVIDIA container repository</a>. To use these Docker containers, you’ll first need to install the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> to provide GPU support for Docker. You can use the NGC links referenced in the table above to obtain more information about how to launch and run these containers.</p>
<p>The following sample command pulls and starts the Merlin TensorFlow container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run the container in interactive mode</span>
$<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>--gpus<span class="o">=</span>all<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--cap-add<span class="w"> </span>SYS_NICE<span class="w"> </span>nvcr.io/nvidia/merlin/merlin-tensorflow:23.02
</pre></div>
</div>
<p>You can check the existence of the HPS Python toolkit after launching this container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>python3<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import hierarchical_parameter_server as hps&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="example-notebooks">
<h2>Example Notebooks<a class="headerlink" href="#example-notebooks" title="Permalink to this heading"></a></h2>
<p>We provide a collection of examples as <a class="reference internal" href="#../hps_tf/notebooks"><span class="xref myst">Jupyter Notebooks</span></a> that cover the following topics:</p>
<ul class="simple">
<li><p>Basic workflow of HPS deployment for TensorFlow models</p></li>
<li><p>Migrating from SOK training to HPS inference</p></li>
<li><p>Leveraging HPS to load pre-trained embeddings</p></li>
</ul>
</section>
<section id="benchmark">
<h2>Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this heading"></a></h2>
<p>We benchmark the DLRM TensorFlow model with HPS Plugin for TensorFlow in <a class="reference internal" href="hps_dlrm_benchmark.html"><span class="std std-doc">hps_dlrm_benchmark.md</span></a>.</p>
</section>
<div class="toctree-wrapper compound">
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hps_database_backend.html" class="btn btn-neutral float-left" title="Hierarchical Parameter Server Database Backend" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hps_tf_api/index.html" class="btn btn-neutral float-right" title="Hierarchical Parameter Server API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v23.09.00
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../v23.08.00/hierarchical_parameter_server/hps_tf_user_guide.html">v23.08.00</a></dd>
      <dd><a href="hps_tf_user_guide.html">v23.09.00</a></dd>
      <dd><a href="../../v23.12.00/hierarchical_parameter_server/hps_tf_user_guide.html">v23.12.00</a></dd>
      <dd><a href="../../v24.04.00/hierarchical_parameter_server/hps_tf_user_guide.html">v24.04.00</a></dd>
      <dd><a href="../../v24.06.00/hierarchical_parameter_server/hps_tf_user_guide.html">v24.06.00</a></dd>
      <dd><a href="../../v25.03.00/index.html">v25.03.00</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../main/index.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>