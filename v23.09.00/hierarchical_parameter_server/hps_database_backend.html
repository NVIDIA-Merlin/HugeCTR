<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Hierarchical Parameter Server Database Backend &mdash; Merlin HugeCTR  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />

  
    <link rel="canonical" href="https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_database_backend.html" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hierarchical Parameter Server Plugin for TensorFlow" href="hps_tf_user_guide.html" />
    <link rel="prev" title="Hierarchical Parameter Server" href="index.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Since the HugeCTR <code>v23.09</code>, the offline inference has been deprecated.
      Since the HugeCTR <code>v24.06</code>, the HPS has been deprecated.
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">HUGECTR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_embedding_training_cache.html">Embedding Training Cache</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Hierarchical Parameter Server</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">HPS Database Backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_tf_user_guide.html">HPS Plugin for TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_trt_user_guide.html">HPS Plugin for TensorRT</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_torch_user_guide.html">HPS Plugin for Torch</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_dlrm_benchmark.html">Benchmark HPS-integrated DLRM</a></li>
<li class="toctree-l2"><a class="reference internal" href="profiling_hps.html">Profiling HPS</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../sparse_operation_kit.html">Sparse Operation Kit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Hierarchical Parameter Server</a></li>
      <li class="breadcrumb-item active">Hierarchical Parameter Server Database Backend</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="hierarchical-parameter-server-database-backend">
<h1>Hierarchical Parameter Server Database Backend<a class="headerlink" href="#hierarchical-parameter-server-database-backend" title="Permalink to this heading"></a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction-to-the-hps-database-backend" id="id1">Introduction to the HPS Database Backend</a></p></li>
<li><p><a class="reference internal" href="#background" id="id2">Background</a></p></li>
<li><p><a class="reference internal" href="#architecture" id="id3">Architecture</a></p></li>
<li><p><a class="reference internal" href="#training-and-iterative-model-updates" id="id4">Training and Iterative Model Updates</a></p></li>
<li><p><a class="reference internal" href="#execution" id="id5">Execution</a></p>
<ul>
<li><p><a class="reference internal" href="#inference" id="id6">Inference</a></p></li>
<li><p><a class="reference internal" href="#training" id="id7">Training</a></p></li>
<li><p><a class="reference internal" href="#lookup-optimization" id="id8">Lookup Optimization</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#configuration" id="id9">Configuration</a></p>
<ul>
<li><p><a class="reference internal" href="#inference-parameters-and-embedding-cache-configuration" id="id10">Inference Parameters and Embedding Cache Configuration</a></p></li>
<li><p><a class="reference internal" href="#volatile-database-configuration" id="id11">Volatile Database Configuration</a></p></li>
<li><p><a class="reference internal" href="#persistent-database-configuration" id="id12">Persistent Database Configuration</a></p></li>
<li><p><a class="reference internal" href="#update-source-configuration" id="id13">Update Source Configuration</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="introduction-to-the-hps-database-backend">
<h2>Introduction to the HPS Database Backend<a class="headerlink" href="#introduction-to-the-hps-database-backend" title="Permalink to this heading"></a></h2>
<p>The Hierarchical Parameter Server database backend (HPS database backend) allows HugeCTR to use models with huge embedding tables by extending HugeCTRs storage space beyond the constraints of GPU memory through utilizing various memory resources across you cluster. Further, it grants the ability to permanently store embedding tables in a structured manner. For an end-to-end demo on how to use the HPS database backend, please refer to <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/main/samples/hierarchical_deployment">samples</a>.</p>
</section>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this heading"></a></h2>
<p>GPU clusters offer superior compute power, compared to their CPU-only counterparts. However, although modern data-center GPUs by NVIDIA are equipped with increasing amounts of memory, new and more powerful AI algorithms come into existence that require more memory. Recommendation models with their huge embedding tables are spearheading these developments. The HPS database backend allows you to efficiently perform inference with models that rely on embedding tables that vastly exceed the available GPU device storage space.</p>
<p>This is achieved through utilizing other memory resources, available within your clsuter, such as CPU accessible RAM and non-volatile memory. Aside from general advantages of non-volatile memory with respect to retaining stored information, storage devices such as HDDs and SDDs offer orders of magnitude more storage space than DDR memory and HBM (High Bandwidth Memory), at significantly lower cost. However, their throughout is lower and latency is higher than that of DRR and HBM.</p>
<p>The HPS database backend acts as an intermediate layer between your GPU and non-volatile memory to store all embeddings of your model. Thereby, available local RAM and/or RAM resources available across the cluster can be used as a cache to improve response times.</p>
</section>
<section id="architecture">
<h2>Architecture<a class="headerlink" href="#architecture" title="Permalink to this heading"></a></h2>
<p>As of version 3.3, the HugeCTR hierarchical parameter server database backend defines 3 storage layers.</p>
<ol class="arabic">
<li><p>The <strong>CPU Memory Database</strong> layer utilizes volatile CPU addressable RAM memory to cache embeddings.
This database is created and maintained separately by each machine that runs HugeCTR in your cluster.</p></li>
<li><p>The <strong>Distributed Database</strong> layer allows utilizing Redis cluster deployments to store and retrieve embeddings in and from the RAM memory that is available in your cluster.
The HugeCTR distributed database layer is designed for compatibility with Redis <a class="reference external" href="https://redis.io/topics/persistence">persistence features</a> such as RDB and AOF to allow seamless continued operation across device restart.
This kind of database is shared by all nodes that participate in the training or inference of a HugeCTR model.</p>
<p><strong>Note</strong>: Many products claim Redis compatibility.
We cannot guarantee or make any statements regarding the suitability of these with our distributed database layer.
However, we note that Redis alternatives are likely to be compatible with the Redis cluster distributed database layer as long as they are compatible with <a class="reference external" href="https://github.com/redis/hiredis">hiredis</a>.
We would love to hear about your experiences.
Please let us know if you successfully or unsuccessfully deployed such Redis alternatives as storage targets with HugeCTR.</p>
</li>
<li><p>The <strong>Persistent Database</strong> layer links HugeCTR with a persistent database.
Each node that has such a persistent storage layer configured retains a separate copy of all embeddings in its locally available non-volatile memory.
This layer is best considered as a compliment to the distributed database to further expand storage capabilities and to provide high availability.
As a result, if your model exceeds even the total RAM capacity of your entire cluster or if—for whatever reason—the Redis cluster becomes unavailable, all nodes that are configured with a persistent database are still able to respond to inference requests, though likely with increased latency.</p></li>
</ol>
<p>The following table provides an overview of the typical properties for the different parameter database layers and the embedding cache.
We emphasize that this table provides rough guidelines.
Properties for production deployments are often different.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>GPU Embedding Cache</p></th>
<th class="head"><p>CPU Memory Database</p></th>
<th class="head"><p>Distributed Database (InfiniBand)</p></th>
<th class="head"><p>Distributed Database (Ethernet)</p></th>
<th class="head"><p>Persistent Database</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mean Latency</p></td>
<td><p>ns ~ us</p></td>
<td><p>us ~ ms</p></td>
<td><p>us ~ ms</p></td>
<td><p>several ms</p></td>
<td><p>ms ~ s</p></td>
</tr>
<tr class="row-odd"><td><p>Capacity (relative)</p></td>
<td><p>++</p></td>
<td><p>+++</p></td>
<td><p>+++++</p></td>
<td><p>+++++</p></td>
<td><p>+++++++</p></td>
</tr>
<tr class="row-even"><td><p>Capacity (range in practice)</p></td>
<td><p>10 GBs ~ few TBs</p></td>
<td><p>100 GBs ~ several TBs</p></td>
<td><p>several TBs</p></td>
<td><p>several TBs</p></td>
<td><p>up to 100s of TBs</p></td>
</tr>
<tr class="row-odd"><td><p>Cost / Capacity</p></td>
<td><p>++++</p></td>
<td><p>+++</p></td>
<td><p>++++</p></td>
<td><p>++++</p></td>
<td><p>+</p></td>
</tr>
<tr class="row-even"><td><p>Volatile</p></td>
<td><p>yes</p></td>
<td><p>yes</p></td>
<td><p>configuration dependent</p></td>
<td><p>configuration dependent</p></td>
<td><p>no</p></td>
</tr>
<tr class="row-odd"><td><p>Configuration / maintenance complexity</p></td>
<td><p>low</p></td>
<td><p>low</p></td>
<td><p>high</p></td>
<td><p>high</p></td>
<td><p>low</p></td>
</tr>
</tbody>
</table>
</section>
<section id="training-and-iterative-model-updates">
<h2>Training and Iterative Model Updates<a class="headerlink" href="#training-and-iterative-model-updates" title="Permalink to this heading"></a></h2>
<p>Models that are deployed with the HugeCTR HPS database backend allow streaming model parameter updates from external sources through <a class="reference external" href="https://kafka.apache.org">Apache Kafka</a>.
This ability provides zero-downtime online model retraining.</p>
</section>
<section id="execution">
<h2>Execution<a class="headerlink" href="#execution" title="Permalink to this heading"></a></h2>
<section id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this heading"></a></h3>
<p>With respect to embedding lookups from the HugeCTR GPU embedding cache and HPS database backend, the following logic applies:</p>
<ul class="simple">
<li><p>Whenever the HugeCTR inference engine receives a batch of model input parameters for inference, the inference engine first determines the associated unique embedding keys and tries to resolve these embeddings using the embedding cache.</p></li>
<li><p>When there is a cache miss, the inference engine then turns to the HPS database backend to determine the embedding representations.</p></li>
<li><p>The HPS database backend queries its configured backends in the following order to fill in the missing embeddings:</p>
<ol class="arabic simple">
<li><p>Local and remote CPU memory locations.</p></li>
<li><p>Persistent storage.</p></li>
</ol>
</li>
</ul>
<p>HugeCTR first tries to look up missing embeddings in either the CPU memory database or the distributed database.
If, and only if, there are still missing embedding representations after that, HugeCTR tries the non-volatile memory from the persistent database to find the corresponding embedding representations.
The persistent database contains a copy of all existing embeddings.</p>
</section>
<section id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this heading"></a></h3>
<p>After a training iteration, model updates for updated embeddings are published through Kafka by the HugeCTR training process.
The HPS database backend can be configured to listen automatically to change requests for certain models and then ingest these updates in its various database stages.</p>
</section>
<section id="lookup-optimization">
<h3>Lookup Optimization<a class="headerlink" href="#lookup-optimization" title="Permalink to this heading"></a></h3>
<p>If the volatile memory resources—the CPU memory database and distributed database—are not sufficient to retain the entire model, HugeCTR attempts to minimize the average latency for lookup through managing these resources like a cache by using a least recently used (LRU) algorithm.</p>
</section>
</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this heading"></a></h2>
<p>The HugeCTR HPS database backend and iterative update can be configured using three separate configuration objects.
The <code class="docutils literal notranslate"><span class="pre">VolatileDatabaseParams</span></code> and <code class="docutils literal notranslate"><span class="pre">PersistentDatabaseParams</span></code> objects are used to configure the database backends of each HPS database backend instance.
If you want iterative or online model updating, you must also provide the <code class="docutils literal notranslate"><span class="pre">UpdateSourceParams</span></code> object to link the HPS database backend instance with your Kafka deployment.
These objects are part of the <a class="reference internal" href="../api/python_interface.html#inference-api"><span class="std std-ref">hugectr.inference</span></a> Python package.</p>
<p>If you deploy HugeCTR as a backend for NVIDIA <a class="reference external" href="https://developer.nvidia.com/nvidia-triton-inference-server">Triton Inference Server</a>, you can also provide these configuration options by extending your Triton deployment’s JSON configuration:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;supportlonglong&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;fuse_embedding_table&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">  </span><span class="c1">// ...</span>
<span class="w">  </span><span class="nt">&quot;volatile_db&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// ...</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;persistent_db&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// ...</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;update_source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// ...</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="c1">// ...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Set the <code class="docutils literal notranslate"><span class="pre">supportlonglong</span></code> field to <code class="docutils literal notranslate"><span class="pre">True</span></code> when you need to use a 64-bit integer input key.
You must set this field to <code class="docutils literal notranslate"><span class="pre">true</span></code> if you specify <code class="docutils literal notranslate"><span class="pre">True</span></code> for the <code class="docutils literal notranslate"><span class="pre">i64_input_key</span></code> parameter.
The default value is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>Set the <code class="docutils literal notranslate"><span class="pre">fuse_embedding_table</span></code> field to <code class="docutils literal notranslate"><span class="pre">True</span></code> when you want to fuse embedding tables. The tables with the same embedding vector size will be fused in storage during HPS initialization. At each iteration, original lookup queries are packed into one via CPU multi-thread synchronization and the packed query is forward to the fused embedding table. To use this feature, please ensure that key values in different tables have no overlap and the embedding lookup layers have no dependency to each other in the model graph. This is valid for <a class="reference internal" href="hps_tf_user_guide.html"><span class="std std-doc">HPS Plugin for TensorFlow</span></a>,  <a class="reference internal" href="hps_torch_user_guide.html"><span class="std std-doc">HPS Plugin for Torch</span></a> and <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/main/hps_backend">HPS Backend for Triton Inference Server</a>. The default value is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>The following sections describe the configuration parameters.
Generally speaking, each node in your HugeCTR cluster should deploy the same configuration.
In rare cases, it might make sense to vary some parameters.
The most common reason to vary the configuration by node is for heterogeneous clusters.</p>
<section id="inference-parameters-and-embedding-cache-configuration">
<h3>Inference Parameters and Embedding Cache Configuration<a class="headerlink" href="#inference-parameters-and-embedding-cache-configuration" title="Permalink to this heading"></a></h3>
<section id="inference-params-syntax">
<h4>Inference Params Syntax<a class="headerlink" href="#inference-params-syntax" title="Permalink to this heading"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">InferenceParams</span><span class="p">(</span>
  <span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span>
  <span class="n">max_batchsize</span> <span class="o">=</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">hit_rate_threshold</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
  <span class="n">dense_model_file</span> <span class="o">=</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span>
  <span class="n">network_file</span> <span class="o">=</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span>
  <span class="n">sparse_model_files</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;string-1&quot;</span><span class="p">,</span> <span class="s2">&quot;string-2&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
  <span class="n">use_gpu_embedding_cache</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
  <span class="n">cache_size_percentage</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
  <span class="n">i64_input_key</span> <span class="o">=</span> <span class="o">&lt;</span><span class="kc">True</span><span class="o">|</span><span class="kc">False</span><span class="o">&gt;</span><span class="p">,</span>
  <span class="n">use_mixed_precision</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
  <span class="n">scaler</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
  <span class="n">use_algorithm_search</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
  <span class="n">use_cuda_graph</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
  <span class="n">number_of_worker_buffers_in_pool</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
  <span class="n">number_of_refresh_buffers_in_pool</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
  <span class="n">thread_pool_size</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
  <span class="n">cache_refresh_percentage_per_iteration</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
  <span class="n">deployed_devices</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
  <span class="n">default_value_for_each_table</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">float</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
  <span class="n">volatile_db</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">volatile</span><span class="o">-</span><span class="n">database</span><span class="o">-</span><span class="n">configuration</span><span class="o">&gt;</span><span class="p">,</span>
  <span class="n">persistent_db</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">persistent</span><span class="o">-</span><span class="n">database</span><span class="o">-</span><span class="n">configuration</span><span class="o">&gt;</span><span class="p">,</span>
  <span class="n">update_source</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">update</span><span class="o">-</span><span class="n">source</span><span class="o">-</span><span class="n">parameters</span><span class="o">&gt;</span><span class="p">,</span>
  <span class="n">maxnum_des_feature_per_sample</span> <span class="o">=</span> <span class="mi">26</span><span class="p">,</span>
  <span class="n">embedding_cache_type</span> <span class="o">=</span> <span class="s2">&quot;dynamic&quot;</span><span class="p">,</span>
  <span class="n">refresh_delay</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
  <span class="n">refresh_interval</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
  <span class="n">maxnum_catfeature_query_per_table_per_sample</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
  <span class="n">embedding_vecsize_per_table</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
  <span class="n">embedding_table_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;string-1&quot;</span><span class="p">,</span> <span class="s2">&quot;string-2&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">InferenceParams</span></code> object specifies the parameters related to the inference.
An <code class="docutils literal notranslate"><span class="pre">InferenceParams</span></code> object is required to initialize the <code class="docutils literal notranslate"><span class="pre">InferenceModel</span></code> instance.</p>
</section>
<section id="inference-parameters">
<h4>Inference Parameters<a class="headerlink" href="#inference-parameters" title="Permalink to this heading"></a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code>: String, specifies the name of the model to use for inference.
It should be consistent with the <code class="docutils literal notranslate"><span class="pre">model_name</span></code> that you specified during training.
This parameter has no default value and you must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_batchsize</span></code>: Integer, the maximum batch size for inference.
The specific value is the global batch size and should be divisible by the length of <code class="docutils literal notranslate"><span class="pre">deployed_devices</span></code>.
This parameter has no default value and you must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hit_rate_threshold</span></code>: Float, the real hit rate of GPU embedding cache during inference.
When the real hit rate of the GPU embedding cache is higher than the specified threshold, the GPU embedding cache performs an asynchronous insertion of missing embedding keys.
Otherwise, the GPU embedding cache inserts the keys synchronously.
Specify a value between 0 and 1.
The default value is <code class="docutils literal notranslate"><span class="pre">0.9</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dense_model_file</span></code>: String, the dense model file to load for inference.
This parameter has no default value and you must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">network_file</span></code>: String, specifies a file that includes the model network structure in JSON format.
This file is exported after model training and is used for the initialization of the network structure of the dense part of the model.
This parameter has no default value and you must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sparse_model_files</span></code>: List[str], the sparse model files to load for inference.
This parameter has no default value and you must specify a value. Remote file systems(HDFS, S3, and GCS) are also supported. For example, for HDFS, the prefix can be <code class="docutils literal notranslate"><span class="pre">hdfs://localhost:9000/dir/to/model</span></code>. For S3, the prefix should be either virtual-hosted-style or path-style and contains the region information. For examples, take a look at the AWS official <a class="reference external" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-bucket-intro.html">documentation</a>. For GCS, both URI (<code class="docutils literal notranslate"><span class="pre">gs://bucket/object</span></code>) and URL (<code class="docutils literal notranslate"><span class="pre">https://https://storage.googleapis.com/bucket/object</span></code>) are supported.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device_id</span></code>: Integer, is scheduled to be deprecated and replaced by <code class="docutils literal notranslate"><span class="pre">devicelist</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_gpu_embedding_cache</span></code>: Boolean, whether to employ the features of GPU embedding cache.
When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the embedding vector look up goes to the GPU embedding cache.
Otherwise, the look up attempts to use the CPU HPS database backend directly.
The default value is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embedding_cache_type</span></code>: String, specify the type of embedding cache. Three types are supported: <code class="docutils literal notranslate"><span class="pre">&quot;dynamic&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;static&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;uvm&quot;</span></code>. The lookup performance can be ranked from low to high as <code class="docutils literal notranslate"><span class="pre">&quot;dynamic&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;uvm&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;static&quot;</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">&quot;dynamic&quot;</span></code>. The functional differences between the three types of embedding cache are shown in the following table</p></li>
</ul>
<center>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Type</p></th>
<th class="head text-center"><p>Support Dynamic Update</p></th>
<th class="head text-center"><p>Offload Embeddings to CPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">&quot;dynamic&quot;</span></code></p></td>
<td class="text-center"><p>Yes</p></td>
<td class="text-center"><p>Yes</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">&quot;static&quot;</span></code></p></td>
<td class="text-center"><p>No</p></td>
<td class="text-center"><p>No</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">&quot;uvm&quot;</span></code></p></td>
<td class="text-center"><p>No</p></td>
<td class="text-center"><p>Yes</p></td>
</tr>
</tbody>
</table>
</center>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cache_size_percentage</span></code>: Float, the percentage of cached embeddings on the GPU, relative to all the embedding tables on the CPU.
The default value is <code class="docutils literal notranslate"><span class="pre">0.2</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">i64_input_key</span></code>: Boolean, this value should be set to <code class="docutils literal notranslate"><span class="pre">True</span></code> when you need to use an Int64 input key.
This parameter has no default value and you must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_mixed_precision</span></code>: Boolean, whether to enable mixed precision inference.
The default value is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scaler</span></code>: Float, the scaler to use when mixed precision training is enabled.
The function supports <code class="docutils literal notranslate"><span class="pre">128</span></code>, <code class="docutils literal notranslate"><span class="pre">256</span></code>, <code class="docutils literal notranslate"><span class="pre">512</span></code>, and <code class="docutils literal notranslate"><span class="pre">1024</span></code> scalers only for mixed precision training.
The default value is <code class="docutils literal notranslate"><span class="pre">1.0</span></code> and corresponds to no mixed precision training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_algorithm_search</span></code>: Boolean, whether to use algorithm search for <code class="docutils literal notranslate"><span class="pre">cublasGemmEx</span></code> within the fully connected layer.
The default value is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_cuda_graph</span></code>: Boolean, whether to enable CUDA graph for dense-network forward propagation.
The default value is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">number_of_worker_buffers_in_pool</span></code>: Integer, specifies the number of worker buffers to allocate in the embedded cache memory pool.
Specify a value such as two times the number of model instances to avoid resource exhaustion.
An alternative to specifying a larger value while still avoiding resource exhaustion is to disable asynchronous updates by setting the <code class="docutils literal notranslate"><span class="pre">hit_rate_threshold</span></code> parameter to greater than <code class="docutils literal notranslate"><span class="pre">1</span></code>.
The default value is <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">number_of_refresh_buffers_in_pool</span></code>: Integer, specifies the number of refresh buffers to allocate in the embedded cache memory pool.
HPS uses the refresh memory pool to support online updates of incremental models.
Specify larger values if model updates occur at a high-frequency or you have a large volume of incremental model updates.
The default value is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">thread_pool_size</span></code>: Integer, specifies the size of the thread pool. The thread pool is used by the GPU embedding cache to perform asynchronous insertion of missing keys.
The actual thread pool size is set to the maximum of the value that you specify and the value returned by <code class="docutils literal notranslate"><span class="pre">std::thread::hardware_concurrency()</span></code>.
The default value is <code class="docutils literal notranslate"><span class="pre">16</span></code>.</p></li>
</ul>
<p>The actual thread pool size will be set as the maximum value of this configured one and <code class="docutils literal notranslate"><span class="pre">std::thread::hardware_concurrency()</span></code>.
The default value is <code class="docutils literal notranslate"><span class="pre">16</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cache_refresh_percentage_per_iteration</span></code>: Float, specifies the percentage of the embedding cache to refresh during each iteration.
To avoid reducing the performance of the GPU cache during online updating, you can configure the update percentage of GPU embedding cache.
For example, if you specify <code class="docutils literal notranslate"><span class="pre">cache_refresh_percentage_per_iteration</span> <span class="pre">=</span> <span class="pre">0.2</span></code>, the entire GPU embedding cache is refreshed during 5 iterations.
Specify a smaller value if model updates occur at a high-frequency or you have a large volume of incremental model updates.
The default value is <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">deployed_devices</span></code>: List[Integer], specifies a list of the device IDs of your GPUs.
The offline inference is executed concurrently on the specified GPUs.
The default value is <code class="docutils literal notranslate"><span class="pre">[0]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">default_value_for_each_table</span></code>:List[Float], specifies a default value when an embedding key cannot be returned.
When an embedding key can not be queried in the GPU cache or volatile and persistent databases, the default value is returned.
For models with multiple embedding tables, each embedding table has a default value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">volatile_db</span></code>: See the <a class="reference internal" href="#volatile-database-configuration">Volatile Database Configuration</a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">persistent_db</span></code>: See the <a class="reference internal" href="#persistent-database-configuration">Persistent Database Configuration</a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">update_source</span></code>: See the <a class="reference internal" href="#update-source-configuration">Update Source Configuration</a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">maxnum_des_feature_per_sample</span></code>: Integer, specifies the maximum number of dense features in each sample.
Because each sample can contain a varying number of numeric (dense) features, use this parameter to specify the maximum number of dense feature in each sample.
The specified value determines the pre-allocated memory size on the host and device.
The default value is <code class="docutils literal notranslate"><span class="pre">26</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">refresh_delay</span></code>: Float, specifies an initial delay, in seconds, to wait before beginning to refresh the embedding cache.
The timer begins when the service launches.
The default value is <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">refresh_interval</span></code>: Float, specifies the interval, in seconds, for the periodic refresh of the embedding keys in the GPU embedding cache.
The embedding keys are refreshed from volatile and persistent data sources based on the specified number of seconds.
The default value is <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">maxnum_catfeature_query_per_table_per_sample</span></code>: List[Int], this parameter determines the pre-allocated memory size on the host and device.
We assume that for each input sample, there is a maximum number of embedding keys per sample in each embedding table that need to be looked up.
Specify this parameter as [max(the number of embedding keys that need to be queried from embedding table 1 in each sample), max(the number of embedding keys that need to be queried from embedding table 2 in each sample), …]
This parameter has no default value and you must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embedding_vecsize_per_table</span></code>:List[Int], this parameter determines the pre-allocated memory size on the host and device.
For the case of multiple embedding tables, we assume that the size of the embedding vector in each embedding table is different.
Specify the maximum vector size for each embedding table.
This parameter has no default value and you must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embedding_table_names</span></code>: List[String], specifies the name of each embedding table.
The names are used to name the data partition and data table in the hierarchical database backend.
The default value is <code class="docutils literal notranslate"><span class="pre">[&quot;sparse_embedding1&quot;,</span> <span class="pre">&quot;sparse_embedding2&quot;,</span> <span class="pre">...]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">label_dim</span></code>: Int, each model can contain a varying size of prediction result, such as a multi-task model.
Specify the maximum size of prediction result in each sample.
The specified value determines the pre-allocated memory size on the host and device.
The default value is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">slot_num</span></code>: Int, each model can contain a fixed size of feature fields.
Specify the number of feature fields (the number of slots).
The specified value determines the pre-allocated memory size on the host and device.
The default value is <code class="docutils literal notranslate"><span class="pre">10</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embedding_cache_type</span></code>: String, specify the type of embedding cache. Three types are supported: <code class="docutils literal notranslate"><span class="pre">&quot;dynamic&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;static&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;uvm&quot;</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">&quot;dynamic&quot;</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_context_stream</span></code>: Boolean, whether to use context stream of TensorFlow or TensorRT for HPS embedding lookup. This is only valid for <a class="reference internal" href="hps_tf_user_guide.html"><span class="std std-doc">HPS Plugin for TensorFlow</span></a> and <a class="reference internal" href="hps_trt_user_guide.html"><span class="std std-doc">HPS Plugin for TensorRT</span></a>. The default value is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</section>
<section id="parameter-server-configuration-models">
<h4>Parameter Server Configuration: Models<a class="headerlink" href="#parameter-server-configuration-models" title="Permalink to this heading"></a></h4>
<p>The following JSON shows a sample configuration for the <code class="docutils literal notranslate"><span class="pre">models</span></code> key in a parameter server configuration file.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="nt">&quot;supportlonglong&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="nt">&quot;fuse_embedding_table&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="nt">&quot;models&quot;</span><span class="p">:[</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="s2">&quot;wdl&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;sparse_files&quot;</span><span class="p">:[</span><span class="s2">&quot;/wdl_infer/model/wdl/1/wdl0_sparse_20000.model&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;/wdl_infer/model/wdl/1/wdl1_sparse_20000.model&quot;</span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;dense_file&quot;</span><span class="p">:</span><span class="s2">&quot;/wdl_infer/model/wdl/1/wdl_dense_20000.model&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;network_file&quot;</span><span class="p">:</span><span class="s2">&quot;/wdl_infer/model/wdl/1/wdl.json&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;num_of_worker_buffer_in_pool&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;num_of_refresher_buffer_in_pool&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;deployed_device_list&quot;</span><span class="p">:[</span><span class="mi">0</span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;max_batch_size&quot;</span><span class="p">:</span><span class="mi">64</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;default_value_for_each_table&quot;</span><span class="p">:[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;maxnum_des_feature_per_sample&quot;</span><span class="p">:</span><span class="mi">26</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;maxnum_catfeature_query_per_table_per_sample&quot;</span><span class="p">:[</span><span class="mi">2</span><span class="p">,</span><span class="mi">26</span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;embedding_vecsize_per_table&quot;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">15</span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;embedding_table_names&quot;</span><span class="p">:[</span><span class="s2">&quot;table1&quot;</span><span class="p">,</span><span class="s2">&quot;table2&quot;</span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;refresh_delay&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;refresh_interval&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;hit_rate_threshold&quot;</span><span class="p">:</span><span class="mf">0.9</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;gpucacheper&quot;</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;embedding_cache_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;dynamic&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;gpucache&quot;</span><span class="p">:</span><span class="kc">true</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;cache_refresh_percentage_per_iteration&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;label_dim&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;slot_num&quot;</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;use_context_stream&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="volatile-database-configuration">
<h3>Volatile Database Configuration<a class="headerlink" href="#volatile-database-configuration" title="Permalink to this heading"></a></h3>
<p>For HugeCTR, the volatile database implementations are grouped into two categories:</p>
<ul>
<li><p><strong>CPU memory databases</strong> have an instance on each machine and only use the locally available RAM memory as backing storage.
As a result, you can indvidually vary their configuration parameters for each machine.</p></li>
<li><p><strong>Distributed CPU memory databases</strong> are typically shared by all machines in your HugeCTR deployment.
They enable you to use the combined memory capacity of your cluster machines.
The configuration parameters for this kind of database should be identical across all machines in your deployment.</p>
<p>Distributed databases are shared by all your HugeCTR nodes.
These nodes collaborate to inject updates into the underlying database.
The assignment of which nodes update specific partition can change at runtime.</p>
</li>
</ul>
<section id="volatile-database-params-syntax">
<h4>Volatile Database Params Syntax<a class="headerlink" href="#volatile-database-params-syntax" title="Permalink to this heading"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">VolatileDatabaseParams</span><span class="p">(</span>
  <span class="nb">type</span> <span class="o">=</span> <span class="s2">&quot;redis_cluster&quot;</span><span class="p">,</span>
  <span class="n">address</span> <span class="o">=</span> <span class="s2">&quot;127.0.0.1:7000&quot;</span><span class="p">,</span>
  <span class="n">user_name</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span>
  <span class="n">password</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
  <span class="n">num_partitions</span> <span class="o">=</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">allocation_rate</span> <span class="o">=</span> <span class="mi">268435456</span><span class="p">,</span>  <span class="c1"># 256 MiB</span>
  <span class="n">shared_memory_size</span> <span class="o">=</span> <span class="mi">17179869184</span><span class="p">,</span>  <span class="c1"># 16 GiB</span>
  <span class="n">shared_memory_name</span> <span class="o">=</span> <span class="s2">&quot;hctr_mp_hash_map_database&quot;</span><span class="p">,</span>
  <span class="n">shared_memory_auto_remove</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
  <span class="n">max_batch_size</span> <span class="o">=</span> <span class="mi">65536</span><span class="p">,</span>
  <span class="n">enable_tls</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
  <span class="n">tls_ca_certificate</span> <span class="o">=</span> <span class="s2">&quot;cacertbundle.crt&quot;</span><span class="p">,</span>
  <span class="n">tls_client_certificate</span> <span class="o">=</span> <span class="s2">&quot;client_cert.pem&quot;</span><span class="p">,</span>
  <span class="n">tls_client_key</span> <span class="o">=</span> <span class="s2">&quot;client_key.pem&quot;</span><span class="p">,</span>
  <span class="n">tls_server_name_identification</span> <span class="o">=</span> <span class="s2">&quot;redis.localhost&quot;</span><span class="p">,</span>
  <span class="n">overflow_margin</span> <span class="o">=</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">overflow_policy</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">DatabaseOverflowPolicy_t</span><span class="o">.&lt;</span><span class="n">enum_value</span><span class="o">&gt;</span><span class="p">,</span>
  <span class="n">overflow_resolution_target</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span>
  <span class="n">initialize_after_startup</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
  <span class="n">initial_cache_rate</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
  <span class="n">cache_missed_embeddings</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
  <span class="n">update_filters</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;filter-0&quot;</span><span class="p">,</span> <span class="s2">&quot;filter-1&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="parameter-server-configuration-volatile-database">
<h4>Parameter Server Configuration: Volatile Database<a class="headerlink" href="#parameter-server-configuration-volatile-database" title="Permalink to this heading"></a></h4>
<p>The following JSON shows a sample configuration for the <code class="docutils literal notranslate"><span class="pre">volatile_db</span></code> key in a parameter server configuration file.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="nt">&quot;volatile_db&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;redis_cluster&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;address&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;127.0.0.1:7003,127.0.0.1:7004,127.0.0.1:7005&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;user_name&quot;</span><span class="p">:</span><span class="w">  </span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;password&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;num_partitions&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;allocation_rate&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">268435456</span><span class="p">,</span><span class="w">  </span><span class="c1">// 256 MiB</span>
<span class="w">  </span><span class="nt">&quot;shared_memory_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">17179869184</span><span class="p">,</span><span class="w">  </span><span class="c1">// 16 GiB</span>
<span class="w">  </span><span class="nt">&quot;shared_memory_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;hctr_mp_hash_map_database&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;shared_memory_auto_remove&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;max_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">65536</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;enable_tls&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;tls_ca_certificate&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cacertbundle.crt&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;tls_client_certificate&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;client_cert.pem&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;tls_client_key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;client_key.pem&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;tls_server_name_identification&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;redis.localhost&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;overflow_margin&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">10000000</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;overflow_policy&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;evict_random&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;overflow_resolution_target&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.8</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;initialize_after_startup&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;initial_cache_rate&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;cache_missed_embeddings&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;update_filters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;.+&quot;</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="volatile-database-parameters">
<h4>Volatile Database Parameters<a class="headerlink" href="#volatile-database-parameters" title="Permalink to this heading"></a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">type</span></code>: specifies the volatile database implementation.
Specify one of the following:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">hash_map</span></code>: Hash-map based CPU memory database implementation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">multi_process_hash_map</span></code>: A hash-map that can be shared by multiple processes. This hash map lives in your operating system’s shared memory (i.e., <code class="docutils literal notranslate"><span class="pre">/dev/shm</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">parallel_hash_map</span></code>: Hash-map based CPU memory database implementation with multi threading support. This is the default value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">redis_cluster</span></code>: Connect to an existing Redis cluster deployment (Distributed CPU memory database implementation).</p></li>
</ul>
</li>
</ul>
<p>The following parameters apply when you set <code class="docutils literal notranslate"><span class="pre">type=&quot;hash_map&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">type=&quot;parallel_hash_map&quot;</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_partitions</span></code>: Integer, specifies the number of partitions for embedding tables and controls the degree of parallelism.
Parallel hashmap implementations split your embedding tables into approximately evenly-sized partitions and parallelizes look up and insert operations.
The default value is calculated as <code class="docutils literal notranslate"><span class="pre">min(number_of_cpu_cores,</span> <span class="pre">16)</span></code> of the system that you used to build the HugeCTR binaries.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">allocation_rate</span></code>: Integer, specifies the maximum number of bytes to allocate for each memory allocation request.
The default value is <code class="docutils literal notranslate"><span class="pre">268435456</span></code> bytes, 256 MiB.</p></li>
</ul>
<p>The following parameters apply when you set <code class="docutils literal notranslate"><span class="pre">type=&quot;multi_process_hash_map&quot;</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">shared_memory_size</span></code>: Integer, denotes the amount of shared memory that should be reserved in the operating system. In other words, this value determines the size of the memory mapped file that will be created in <code class="docutils literal notranslate"><span class="pre">/dev/shm</span></code>. The upper bound size of <code class="docutils literal notranslate"><span class="pre">/dev/shm</span></code> is determined by your hardware and operating system  configuration. The latter of which may need to be adjusted to share large embedding tables between processes. This is particularly true when running HugeCTR in a Docker image. By default, Docker will only allocate 64 MiB for <code class="docutils literal notranslate"><span class="pre">/dev/shm</span></code>, which is insufficient for most recommendation models. You can try starting your docker deployment with <code class="docutils literal notranslate"><span class="pre">--shm-size=...</span></code> to reserve more shared memory of the native OS for the respective docker container (see also <a class="reference external" href="https://docs.docker.com/engine/reference/run">docs.docker.com/engine/reference/run</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shared_memory_name</span></code>: String, the symbolic name of the shared memory. System-unique, and must be the same for all processes that attach to the same shared memory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shared_memory_auto_remove</span></code>: Boolean, disables removal of the shared memory when the last process disconnects. If this is flag is set to <code class="docutils literal notranslate"><span class="pre">False</span></code> (<code class="docutils literal notranslate"><span class="pre">True</span></code> by default), the state of the shared memory is retained across program restarts.</p></li>
</ul>
<p>The following parameters apply when you set <code class="docutils literal notranslate"><span class="pre">type=&quot;redis_cluster&quot;</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">address</span></code>: String, specifies the address of one of servers of the Redis cluster.
Use the pattern <code class="docutils literal notranslate"><span class="pre">&quot;host-1[:port],host-2[:port],...&quot;</span></code>.
The default value is <code class="docutils literal notranslate"><span class="pre">&quot;127.0.0.1:7000&quot;</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">user_name</span></code>: String, specifies the user name of the Redis cluster.
The default value is <code class="docutils literal notranslate"><span class="pre">&quot;default&quot;</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">password</span></code>: String, specifies the password of your account.
The default value is <code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code> and corresponds to no password.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_partitions</span></code>: Integer, specifies the number of partitions for embedding tables.
Each embedding table is divided into <code class="docutils literal notranslate"><span class="pre">num_partitions</span></code> of approximately evenly-sized partitions.
Each partition is assigned a storage location in your Redis cluster.
HugeCTR does not provide any guarantees regarding the placement of partitions.
As a result, multiple partitions can be stored the same node for some models and deployments.
In most cases, to take advantage of your cluster resources, set <code class="docutils literal notranslate"><span class="pre">num_partitions</span></code> to at least equal to the number of Redis nodes.
For optimal performance, set <code class="docutils literal notranslate"><span class="pre">num_parititions</span></code> to be strictly larger than the number of machines.
However, each partition incurs a small processing overhead so do not specify a value that is too large.
A typical value that retains high performance and provides good cluster utilization is 2-5x the number of machines in your Redis cluster.
The default value is <code class="docutils literal notranslate"><span class="pre">8</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code>: Integer, specifies optimization parameters. Mass lookup and insert requests to distributed endpoints are chunked into <code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code>-sized batches. For maximum performance, this parameters should be large. However, if the available memory for buffering requests in your endpoints is limited or you experience transmission stability issues, specifying smaller values can help. The default value is <code class="docutils literal notranslate"><span class="pre">65536</span></code>. With high-performance networking and endpoint hardware, try setting the values to <code class="docutils literal notranslate"><span class="pre">1000000</span></code>.</p>
<p><em>Note: when using the Redis backend (<code class="docutils literal notranslate"><span class="pre">type</span> <span class="pre">=</span> <span class="pre">&quot;redis_cluster&quot;</span></code>) is used in conjunction with certain open source versions of Redis, setting a maximum batch size above <code class="docutils literal notranslate"><span class="pre">262143</span></code> (2^18 - 1) can lead to obscure errors and, therefore, should be avoided.</em></p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_tls</span></code>: Boolean, allows enabling TLS/SSL secured connections with Redis clusters. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code> (=disable TLS/SSL). Enabling encryption may slightly increase latency and decrease the overall throughput when communicating with the Redis cluster.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tls_ca_certificate</span></code>: String, allows you specify the filesystem path to the certificate(s) of the CA for TLS/SSL secured connections. If the provided path denotes a directory, all valid certificates in the directory will be considered. Default value: <code class="docutils literal notranslate"><span class="pre">cacertbundle.crt</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tls_client_certificate</span></code>: String, filesystem path of the client certificate to use for TLS/SSL secured connections. Default value: <code class="docutils literal notranslate"><span class="pre">client_cert.pem</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tls_client_key</span></code>: String, file system path of the private key to use for TLS/SSL secured connections. Default value: <code class="docutils literal notranslate"><span class="pre">client_key.pem</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tls_server_name_identification</span></code>: String, SNI used by the server. Can be different from the actual connection address. Default value: <code class="docutils literal notranslate"><span class="pre">redis.localhost</span></code>.</p></li>
</ul>
</section>
<section id="overflow-parameters">
<h4>Overflow Parameters<a class="headerlink" href="#overflow-parameters" title="Permalink to this heading"></a></h4>
<p>To maximize performance and avoid instabilities that can be caused by sporadic high memory usage, such as an out of memory situations, HugeCTR provides an overflow handling mechanism.
This mechanism enables you to limit the maximum amount of embeddings to store for each partition.
The limit acts as an upper bound for the memory consumption of your distributed database.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">overflow_margin</span></code>: Integer, specifies the maximum amount of embeddings to store for each partition.
Inserting more than <code class="docutils literal notranslate"><span class="pre">overflow_margin</span></code> embeddings into the database triggers the  configured <code class="docutils literal notranslate"><span class="pre">overflow_policy</span></code>.
This parameter sets the upper bound for the maximum amount of memory that your CPU memory database can occupy.
Larger values for this parameter result in higher hit rates but also consume more memory.
The default value is <code class="docutils literal notranslate"><span class="pre">2^64</span> <span class="pre">-</span> <span class="pre">1</span></code> and indicates no limit.</p>
<p>When you use a CPU memory database in conjunction with a persistent database, the ideal value for <code class="docutils literal notranslate"><span class="pre">overflow_margin</span></code> can vary.
In practice, a value in the range <code class="docutils literal notranslate"><span class="pre">[1000000,</span> <span class="pre">100000000]</span></code> provides reliable performance and throughput.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">overflow_policy</span></code>: specifies how to respond to an overflow condition (i.e., which embeddings should be pruned first). Pruning is conducted per-partition in <code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code>-sized batches until the respective partition contains at most <code class="docutils literal notranslate"><span class="pre">overflow_margin</span> <span class="pre">*</span> <span class="pre">overflow_resolution_target</span></code> embeddings.
Specify one of the following:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">evict_random</span></code> <em>(default)</em>: Embeddings for pruning are chosen at random.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">evict_least_used</span></code>: Prune the least-frequently used (LFU) embeddings. This is a best effort. For performance reasons, we implement different algorithms. Identical behavior across backends is not guaranteed.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">evict_oldest</span></code>: Prune the least-recently used (LRU) embeddings.</p></li>
</ul>
<p>Unlike <code class="docutils literal notranslate"><span class="pre">evict_least_used</span></code> and <code class="docutils literal notranslate"><span class="pre">evict_oldest</span></code>, the <code class="docutils literal notranslate"><span class="pre">evict_random</span></code> policy does not require complicated comparisons and can be faster. However, <code class="docutils literal notranslate"><span class="pre">evict_least_used</span></code> and <code class="docutils literal notranslate"><span class="pre">evict_oldest</span></code> are likely to deliver better performance over time because these policies evict embeddings based on the access statistics.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">overflow_resolution_target</span></code>: Double, specifies the fraction of the embeddings to keep when embeddings must be evicted.
Specify a value between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>, but not exactly <code class="docutils literal notranslate"><span class="pre">0</span></code> or <code class="docutils literal notranslate"><span class="pre">1</span></code>.
The default value is <code class="docutils literal notranslate"><span class="pre">0.8</span></code> and indicates to evict embeddings from a partition until it is shrunk to 80% of its maximum size.
In other words, when the partition size surpasses <code class="docutils literal notranslate"><span class="pre">overflow_margin</span></code> embeddings, 20% of the embeddings are evicted according to the specified <code class="docutils literal notranslate"><span class="pre">overflow_policy</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">initialize_after_startup</span></code>: Boolean,when set to <code class="docutils literal notranslate"><span class="pre">True</span></code> <em>(default)</em>, the contents of the sparse model files are used to initialize this database. This is useful if multiple processes should connect to the same database, or if restarting processes connect to a previously-initialized database that retains its state between inference process restarts. For example, if you reconnect to an existing RocksDB or Redis deployment, or an already materialized multi-process hashmap.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">initial_cache_rate</span></code>: Double, specifies the fraction of the embeddings to initially attempt to cache.
Specify a value in the range <code class="docutils literal notranslate"><span class="pre">[0.0,</span> <span class="pre">1.0]</span></code>.
HugeCTR attempts to cache the specified fraction of the dataset immediately upon startup of the HPS database backend.
For example, a value of <code class="docutils literal notranslate"><span class="pre">0.5</span></code> causes the HugeCTR HPS database backend to attempt to cache up to 50% of your dataset using the volatile database after initialization.
The default value is <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
</ul>
</section>
<section id="common-volatile-database-parameters">
<h4>Common Volatile Database Parameters<a class="headerlink" href="#common-volatile-database-parameters" title="Permalink to this heading"></a></h4>
<p>The following parameters are common to all volatile database types.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">cache_missed_embeddings</span></code>: Bool, when set to <code class="docutils literal notranslate"><span class="pre">True</span></code> and an embedding could not be retrieved from the volatile database, but could be retrieved from the persistent database, the embedding is inserted into the volatile database.
The insert operation could replace another value.
The default value is <code class="docutils literal notranslate"><span class="pre">False</span></code> and disables this functionality.</p>
<p>This setting optimizes the volatile database in response to the queries that are received in inference mode.
In training mode, updated embeddings are automatically written back to the database after each training step.
As a result, setting the value to <code class="docutils literal notranslate"><span class="pre">True</span></code> during training is likely to increase the number of writes to the database and degrade performance without providing significant improvements.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">update_filters</span></code>: List[str], specifies regular expressions that are used to control sending model updates from Kafka to the CPU memory database backend.
The default value is <code class="docutils literal notranslate"><span class="pre">[&quot;^hps_.+$&quot;]</span></code> and processes updates for all HPS models because the filter matches all HPS model names.</p>
<p>The functionality of this parameter might change in future versions.</p>
</li>
</ul>
</section>
</section>
<section id="persistent-database-configuration">
<h3>Persistent Database Configuration<a class="headerlink" href="#persistent-database-configuration" title="Permalink to this heading"></a></h3>
<p>Persistent databases have an instance on each machine and use the locally available non-volatile memory as backing storage.
As a result, some configuration parameters can vary according to the specifications of the machine.</p>
<section id="persistent-database-params-syntax">
<h4>Persistent Database Params Syntax<a class="headerlink" href="#persistent-database-params-syntax" title="Permalink to this heading"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">PersistentDatabaseParams</span><span class="p">(</span>
  <span class="nb">type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">DatabaseType_t</span><span class="o">.&lt;</span><span class="n">enum_value</span><span class="o">&gt;</span><span class="p">,</span>
  <span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/rocksdb&quot;</span><span class="p">,</span>
  <span class="n">num_threads</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
  <span class="n">read_only</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
  <span class="n">max_batch_size</span> <span class="o">=</span> <span class="mi">65536</span><span class="p">,</span>
  <span class="n">update_filters</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;filter-0&quot;</span><span class="p">,</span> <span class="s2">&quot;filter-1&quot;</span><span class="p">,</span> <span class="o">...</span> <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="parameter-server-configuration-persistent-database">
<h4>Parameter Server Configuration: Persistent Database<a class="headerlink" href="#parameter-server-configuration-persistent-database" title="Permalink to this heading"></a></h4>
<p>The following JSON shows a sample configuration for the <code class="docutils literal notranslate"><span class="pre">persistent_db</span></code> key in a parameter server configuration file.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="nt">&quot;persistent_db&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;rocks_db&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/tmp/rocksdb&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;num_threads&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;read_only&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;max_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">65536</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;update_filters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;.+&quot;</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="persistent-database-parameters">
<h4>Persistent Database Parameters<a class="headerlink" href="#persistent-database-parameters" title="Permalink to this heading"></a></h4>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">type</span></code>: specifies the persistent datatabase implementation.
Specify one of the following:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">disabled</span></code> <em>(default)</em>: Prevents the use of a persistent database.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rocks_db</span></code>: Create or connect to a RocksDB database.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">path</span></code>: String, specifies the directory on each machine where the RocksDB database can be found.
If the directory does not contain a RocksDB database, HugeCTR creates a database for you.
Be aware that this behavior can overwrite files that are stored in the directory.
For best results, make sure that <code class="docutils literal notranslate"><span class="pre">path</span></code> specifies an existing RocksDB database or an empty directory.
The default value is <code class="docutils literal notranslate"><span class="pre">/tmp/rocksdb</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_threads</span></code>: Integer, specifies the number of threads for the RocksDB driver.
The default value is <code class="docutils literal notranslate"><span class="pre">16</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">read_only</span></code>: Bool, when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the database is opened in read-only mode.
Read-only mode is suitable for use with inference if the model is static and the database is shared by multiple machines, such as with NFS.
The default value is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code>: Integer, specifies the batch size for lookup and insert requests. Mass lookup and insert requests to RocksDB are chunked into batches. For maximum performance this parameter should be large. However, if the available memory for buffering requests in your endpoints is limited, lowering this value might improve performance. The default value is <code class="docutils literal notranslate"><span class="pre">65536</span></code>. With high-performance hardware, you can attempt to set these parameters to <code class="docutils literal notranslate"><span class="pre">1000000</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">update_filters</span></code>: List[str], specifies regular expressions that are used to control sending model updates from Kafka to the CPU memory database backend.
The default value is <code class="docutils literal notranslate"><span class="pre">[&quot;^hps_.+$&quot;]</span></code> and processes updates for all HPS models because the filter matches all HPS model names.</p>
<p>The functionality of this parameter might change in future versions.</p>
</li>
</ul>
</section>
</section>
<section id="update-source-configuration">
<h3>Update Source Configuration<a class="headerlink" href="#update-source-configuration" title="Permalink to this heading"></a></h3>
<p>The real-time update source is the origin for model updates during online retraining.
To ensure that all database layers are kept in sync, configure all the nodes in your HugeCTR deployment identically.</p>
<section id="update-source-params-syntax">
<h4>Update Source Params Syntax<a class="headerlink" href="#update-source-params-syntax" title="Permalink to this heading"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">UpdateSourceParams</span><span class="p">(</span>
  <span class="nb">type</span> <span class="o">=</span> <span class="s2">&quot;kafka_message_queue&quot;</span><span class="p">,</span>
  <span class="n">brokers</span> <span class="o">=</span> <span class="s2">&quot;host-1[:port][;host-2[:port]...]&quot;</span><span class="p">,</span>
  <span class="n">metadata_refresh_interval_ms</span> <span class="o">=</span> <span class="mi">30000</span><span class="p">,</span>
  <span class="n">poll_timeout_ms</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
  <span class="n">receive_buffer_size</span> <span class="o">=</span> <span class="mi">262144</span><span class="p">,</span>
  <span class="n">max_batch_size</span> <span class="o">=</span> <span class="mi">8192</span><span class="p">,</span>
  <span class="n">failure_backoff_ms</span> <span class="o">=</span> <span class="mi">50</span>
  <span class="n">max_commit_interval</span> <span class="o">=</span> <span class="mi">32</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="parameter-server-configuration-update-source">
<h4>Parameter Server Configuration: Update Source<a class="headerlink" href="#parameter-server-configuration-update-source" title="Permalink to this heading"></a></h4>
<p>The following JSON shows a sample configuration for the <code class="docutils literal notranslate"><span class="pre">update_source</span></code> key in a parameter server configuration file.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="nt">&quot;update_source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kafka_message_queue&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;brokers&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;127.0.0.1:9092&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;metadata_refresh_interval_ms&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">30000</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;poll_timeout_ms&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">500</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;receive_buffer_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">262144</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;max_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">8192</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;failure_backoff_ms&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;max_commit_interval&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">32</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="update-source-parameters">
<h4>Update Source Parameters<a class="headerlink" href="#update-source-parameters" title="Permalink to this heading"></a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">type</span></code>: String, specifies the update source implementation.
Specify one of the following:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">null</span></code>: Prevents the use of an update source. This is the default value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kafka_message_queue</span></code>: Connect to an existing Apache Kafka message queue.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">brokers</span></code>: String, specifies a semicolon-delimited list of host name or IP address and port pairs.
You must specify  at least one host name and port of a Kafka broker node.
The default value is <code class="docutils literal notranslate"><span class="pre">127.0.0.1:9092</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">metadata_refresh_interval_ms</span></code>: Int, specifies the frequency at which the topic metadata downloaded from the Kafka broker.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">receive_buffer_size</span></code> Int, specifies the size of the buffer, in bytes, that stores data that is received from Kafka.
The best value to specify is equal to <code class="docutils literal notranslate"><span class="pre">send_buffer_size</span></code> of the KafkaMessageSink that is used to push updates to Kafka.
The <code class="docutils literal notranslate"><span class="pre">message.max.bytes</span></code> setting of the Kafka broker must be at least <code class="docutils literal notranslate"><span class="pre">receive_buffer_size</span> <span class="pre">+</span> <span class="pre">1024</span></code> bytes.
The default value is <code class="docutils literal notranslate"><span class="pre">262144</span></code> bytes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">poll_timeout_ms</span></code>: Int, specifies the maximum time to wait, in milliseconds, for additional updates before dispatching updates to the database layers.
The default value is <code class="docutils literal notranslate"><span class="pre">500</span></code> ms.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code>: Int, specifies the maximum number of keys and values from messages to consume before dispatching updates to the database.
HugeCTR dispatches the updates in chunks.
The maximum size of these chunks is set with this parameter.
The default value is <code class="docutils literal notranslate"><span class="pre">8192</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">failure_backoff_ms</span></code>: Int, specifies a delay, in milliseconds, to wait after failing to dispatch updates to the database successfully.
In some situations, there can be issues that prevent the successful dispatch such as if a Redis node is temporarily unreachable.
After the delay, HugeCTR retries dispatching a set of updates.
The default value is <code class="docutils literal notranslate"><span class="pre">50</span></code> ms.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_commit_interval</span></code>: Int, specifies the maximum number of messages to hold before delivering and committing the messages to Kafka.
This parameter is evaluated independent of any other conditions or parameters.
Any received data is forwarded and committed if at most <code class="docutils literal notranslate"><span class="pre">max_commit_interval</span></code> were processed since the previous commit.
The default value is <code class="docutils literal notranslate"><span class="pre">32</span></code>.</p></li>
</ul>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Hierarchical Parameter Server" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hps_tf_user_guide.html" class="btn btn-neutral float-right" title="Hierarchical Parameter Server Plugin for TensorFlow" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v23.09.00
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../v23.08.00/hierarchical_parameter_server/hps_database_backend.html">v23.08.00</a></dd>
      <dd><a href="hps_database_backend.html">v23.09.00</a></dd>
      <dd><a href="../../v23.12.00/hierarchical_parameter_server/hps_database_backend.html">v23.12.00</a></dd>
      <dd><a href="../../v24.04.00/hierarchical_parameter_server/hps_database_backend.html">v24.04.00</a></dd>
      <dd><a href="../../v24.06.00/hierarchical_parameter_server/hps_database_backend.html">v24.06.00</a></dd>
      <dd><a href="../../v25.03.00/index.html">v25.03.00</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../main/index.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>