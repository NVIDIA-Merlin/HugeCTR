<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Benchmark the HPS Plugin for TensorFlow &mdash; Merlin HugeCTR  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hierarchical Parameter Server Notebooks" href="notebooks/index.html" />
    <link rel="prev" title="Hierarchical Parameter Server Plugin for TensorFlow" href="hps_tf_user_guide.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">HUGECTR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_embedding_training_cache.html">Embedding Training Cache</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Hierarchical Parameter Server</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../hugectr_parameter_server.html">HPS Database Backend</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="hps_tf_user_guide.html">HPS Plugin for TensorFlow</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="notebooks/index.html">Notebooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/index.html">API Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Hierarchical Parameter Server</a> &raquo;</li>
          <li><a href="hps_tf_user_guide.html">Hierarchical Parameter Server Plugin for TensorFlow</a> &raquo;</li>
      <li>Benchmark the HPS Plugin for TensorFlow</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="tex2jax_ignore mathjax_ignore section" id="benchmark-the-hps-plugin-for-tensorflow">
<h1>Benchmark the HPS Plugin for TensorFlow<a class="headerlink" href="#benchmark-the-hps-plugin-for-tensorflow" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#benchmark-setup" id="id1">Benchmark Setup</a></p></li>
<li><p><a class="reference internal" href="#results-and-analysis" id="id2">Results and Analysis</a></p></li>
<li><p><a class="reference internal" href="#resources" id="id3">Resources</a></p></li>
</ul>
</div>
<div class="section" id="benchmark-setup">
<h2>Benchmark Setup<a class="headerlink" href="#benchmark-setup" title="Permalink to this headline"></a></h2>
<p>The inference SavedModel that leverages HPS can be deployed with the Triton TensorFlow Backend, which is demonstrated in <a class="reference internal" href="notebooks/hps_tensorflow_triton_deployment_demo.html"><span class="doc std std-doc">hps_tensorflow_triton_deployment_demo.ipynb</span></a>. The inference performance of this deployment method needs to be investigated using Triton Performance Analyzer to verify the effectiveness of the HPS integration into TensorFlow.</p>
<p>We train a DLRM model following <a class="reference external" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow2/Recommendation/DLRM">JoC TensorFlow2 DLRM</a> on the Criteo 1TB dataset filtered with the frequency threshold of 15. The trained model is in the SavedModel format and the size is about 16GB, which is almost the size of embedding weights because the size of dense layer weights is small. We compare three deployment methods on the Triton Infrence Server:</p>
<ul class="simple">
<li><p>Deploy the trained SavedModel directly with the TensorFlow backend.</p></li>
<li><p>Deploy the inference SavedModel that leverages HPS with the TensorFlow backend. The workflow of deriving the inference SavedModel is illustrated in <a class="reference internal" href="hps_tf_user_guide.html#workflow"><span class="std std-doc">HPS TensorFlow WorkFlow</span></a>.</p></li>
<li><p>Deploy the ensemble model that combines the Triton HPS backend and the Triton TensorFlow backend. The demo for this deployment method can be found at the <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/main/hps_backend/samples/hps-triton-ensemble">HPS Triton Ensemble</a> notebook.</p></li>
</ul>
<p>The benchmark is conducted on DGX A100, and only one GPU is utilized with one Triton model instance on it. The best-case performance for HPS is studied by sending the same batch of inference data repeatedly. In this case, the embedding lookup is served by the GPU embedding cache of HPS. As for the method of deploying the trained SavedModel, the whole embedding weights (~16GB) can fit into the 80GB GPU memory of A100, which can be regarded as the baseline of the benchmark.</p>
</div>
<div class="section" id="results-and-analysis">
<h2>Results and Analysis<a class="headerlink" href="#results-and-analysis" title="Permalink to this headline"></a></h2>
<p>The per-batch latency, in milliseconds, measured at the server side is shown in the following table and Fig. 1. The Y-axis is logarithmic.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Batch size</p></th>
<th class="head"><p>Native TF Model</p></th>
<th class="head"><p>HPS Plugin TF Model</p></th>
<th class="head"><p>Ensemble Model</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>32</p></td>
<td><p>0.636</p></td>
<td><p>0.612</p></td>
<td><p>0.854</p></td>
</tr>
<tr class="row-odd"><td><p>64</p></td>
<td><p>0.754</p></td>
<td><p>0.651</p></td>
<td><p>1.008</p></td>
</tr>
<tr class="row-even"><td><p>256</p></td>
<td><p>0.756</p></td>
<td><p>0.669</p></td>
<td><p>1.802</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>0.922</p></td>
<td><p>0.870</p></td>
<td><p>4.420</p></td>
</tr>
<tr class="row-even"><td><p>2048</p></td>
<td><p>1.176</p></td>
<td><p>1.100</p></td>
<td><p>8.087</p></td>
</tr>
<tr class="row-odd"><td><p>4096</p></td>
<td><p>1.447</p></td>
<td><p>1.552</p></td>
<td><p>23.383</p></td>
</tr>
<tr class="row-even"><td><p>8192</p></td>
<td><p>2.443</p></td>
<td><p>2.524</p></td>
<td><p>42.303</p></td>
</tr>
<tr class="row-odd"><td><p>16384</p></td>
<td><p>3.901</p></td>
<td><p>4.402</p></td>
<td><p>82.310</p></td>
</tr>
<tr class="row-even"><td><p>32768</p></td>
<td><p>7.681</p></td>
<td><p>8.320</p></td>
<td><p>291.335</p></td>
</tr>
<tr class="row-odd"><td><p>65536</p></td>
<td><p>13.665</p></td>
<td><p>15.577</p></td>
<td><p>617.968</p></td>
</tr>
<tr class="row-even"><td><p>131072</p></td>
<td><p>29.636</p></td>
<td><p>29.952</p></td>
<td><p>1190.025</p></td>
</tr>
</tbody>
</table>
<a class="reference internal image-reference" href="../_images/latency.png"><img alt="../_images/latency.png" class="align-center" src="../_images/latency.png" style="width: 720px;" /></a>
<div align=center>Fig. 1: JoC DLRM inference benchmark - latency </div>
<p><br></br></p>
<p>The throughput, in K samples per second, measured at the server side is shown in the following table and Fig. 2. The Y-axis is logarithmic.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Batch size</p></th>
<th class="head"><p>Native TF Model</p></th>
<th class="head"><p>HPS Plugin TF Model</p></th>
<th class="head"><p>Ensemble Model</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>32</p></td>
<td><p>50.314</p></td>
<td><p>52.287</p></td>
<td><p>37.470</p></td>
</tr>
<tr class="row-odd"><td><p>64</p></td>
<td><p>84.880</p></td>
<td><p>98.310</p></td>
<td><p>63.492</p></td>
</tr>
<tr class="row-even"><td><p>256</p></td>
<td><p>338.624</p></td>
<td><p>382.660</p></td>
<td><p>142.064</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>1110.629</p></td>
<td><p>1177.011</p></td>
<td><p>231.674</p></td>
</tr>
<tr class="row-even"><td><p>2048</p></td>
<td><p>1741.496</p></td>
<td><p>1861.818</p></td>
<td><p>253.245</p></td>
</tr>
<tr class="row-odd"><td><p>4096</p></td>
<td><p>2830.684</p></td>
<td><p>2639.175</p></td>
<td><p>175.169</p></td>
</tr>
<tr class="row-even"><td><p>8192</p></td>
<td><p>3353.254</p></td>
<td><p>3245.641</p></td>
<td><p>193.650</p></td>
</tr>
<tr class="row-odd"><td><p>16384</p></td>
<td><p>4199.948</p></td>
<td><p>3721.944</p></td>
<td><p>199.052</p></td>
</tr>
<tr class="row-even"><td><p>32768</p></td>
<td><p>4266.111</p></td>
<td><p>3938.461</p></td>
<td><p>112.475</p></td>
</tr>
<tr class="row-odd"><td><p>65536</p></td>
<td><p>4795.901</p></td>
<td><p>4207.228</p></td>
<td><p>106.050</p></td>
</tr>
<tr class="row-even"><td><p>131072</p></td>
<td><p>4422.729</p></td>
<td><p>4376.068</p></td>
<td><p>110.142</p></td>
</tr>
</tbody>
</table>
<a class="reference internal image-reference" href="../_images/throughput.png"><img alt="../_images/throughput.png" class="align-center" src="../_images/throughput.png" style="width: 720px;" /></a>
<div align=center>Fig. 2: JoC DLRM inference benchmark - throughput </div>
<p><br></br></p>
<p>As the benchmark results indicate, the performance of inference SavedModel is comparable to that of trained SavedModel. The results show that the best-case performance of HPS embedding lookup, when serviced by the GPU embedding cache, is on par with that of native TensorFlow GPU embedding lookup. Often, large embedding weights cannot fit in GPU memory. The native TensorFlow GPU embedding lookup does not support this condition. However, the HPS plugin for TensorFlow can handle embedding tables that exceed GPU memory with a hierarchical memory storage and provide a low-latency embedding lookup service with an efficient GPU caching mechanism.</p>
<p>Among the three deployment methods, the performance of the Triton ensemble model is much worse than the other two, which can be attributed to the overhead of data transfer between two backends. Specifically, the embedding vectors output by the HPS backend will be fed to the TensorFlow backend, and the size of embedding vectors increase linearly with the batch size (attaining 131072*128*4 bytes for the batch size 131072). Therefore, the latency is orders of magnitude higher than the other two methods.</p>
</div>
<div class="section" id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/triton-inference-server/tensorflow_backend">Triton TensorFlow Backend</a></p></li>
<li><p><a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/perf_analyzer.md">Triton Performance Analyzer</a></p></li>
<li><p><a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/main/hps_backend">Triton HPS backend</a></p></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hps_tf_user_guide.html" class="btn btn-neutral float-left" title="Hierarchical Parameter Server Plugin for TensorFlow" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="notebooks/index.html" class="btn btn-neutral float-right" title="Hierarchical Parameter Server Notebooks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: master
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Branches</dt>
      <dd><a href="hps_tf_benchmark.html">master</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>