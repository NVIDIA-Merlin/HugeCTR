{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Python Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 2.3, we've integrated the Python interface, which supports setting data source and model oversubscribing during training. This notebook explains how to access and use the HugeCTR Python interface.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Build the HugeCTR Python Interface](#1)\n",
    "1. [Wide & Deep Demo](#2)\n",
    "1. [API Signatures](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Build the HugeCTR Python Interface\n",
    "\n",
    "To build the HugeCTR Python interface: \n",
    "\n",
    "1. Enter the HugeCTR docker container and run the following commands:\n",
    "   ```bash\n",
    "   $ cd hugectr\n",
    "   $ mkdir -p build && cd build\n",
    "   $ cmake -DCMAKE_BUILD_TYPE=Release -DSM=70 .. # Target is NVIDIA V100\n",
    "   $ make -j\n",
    "   ```\n",
    "\n",
    "   A dynamic link to the `hugectr.so` library is generated in the `hugectr/build/lib/` folder as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /hugectr/build/lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Copy `hugectr.so` to the folder where you want to use the Python interface. \n",
    "   You can also install it to /usr/local/hugectr/lib and set the environment variable export to `PYTHONPATH=/usr/local/hugectr/lib:$PYTHONPATH` if you want to use the Python interface within the docker container environment.\n",
    "\n",
    "3. Import HugeCTR and train your model using with Python as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hugectr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Wide & Deep Demo\n",
    "\n",
    "### 2.1 Download and Preprocess Data\n",
    "1. Download the Kaggle Criteo dataset using the following command:\n",
    "   ```shell\n",
    "   $ wget https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz\n",
    "   ```\n",
    "\n",
    "   For additional information, see [http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/](http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/).\n",
    "   \n",
    "2. Extract the dataset using the following command:\n",
    "   ```shell\n",
    "   $ tar zxvf dac.tar.gz\n",
    "   ```\n",
    "\n",
    "3. Preprocess the data using  the following commands:\n",
    "   ```shell\n",
    "   $ mkdir wdl_data\n",
    "   $ shuf train.txt > train.shuf.txt\n",
    "   $ python3 /hugectr/tools/criteo_script/preprocess.py --src_csv_path=train.shuf.txt --dst_csv_path=wdl_data/train.out.txt --normalize_dense=1 --feature_cross=1\n",
    "   ```\n",
    "\n",
    "4. Split the dataset using the following commands:\n",
    "   ```shell\n",
    "   head -n 36672493 wdl_data/train.out.txt > wdl_data/train && \\\\\n",
    "   tail -n 9168124 wdl_data/train.out.txt > wdl_data/valtest && \\\\\n",
    "   head -n 4584062 wdl_data/valtest > wdl_data/val && \\\\\n",
    "   tail -n 4584062 wdl_data/valtest > wdl_data/test\n",
    "   ```\n",
    "5. Convert the dataset to the HugeCTR Norm dataset format by generating a `file_list.*.txt` and `file_list.*.keyset` as well as all training data (`*.data`) so that the features of the designated source can be employed during training such as model prefetch during training using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile criteo2hugectr.sh\n",
    "mkdir -p wdl_data_hugectr/wdl_data_bin && \\\n",
    "cd wdl_data_hugectr && \\\n",
    "cp /hugectr/build/bin/criteo2hugectr ./ &&\n",
    "./criteo2hugectr /wdl_data/train wdl_data_bin/ file_list.txt 2 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash criteo2hugectr.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train from scratch\n",
    "\n",
    "We can train fom scratch and store the trained dense model and embedding tables in model files by doing the following: \n",
    "\n",
    "1. Create a JSON file for the W&D model. \n",
    "   **NOTE**: Please note that the solver clause no longer needs to be added to the JSON file when using the Python interface. Instead, you can configure the parameters using `hugectr.solver_parser_helper()` directly in the Python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_1gpu.json\n",
    "{\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"update_type\": \"Global\",\n",
    "    \"adam_hparam\": {\n",
    "      \"learning_rate\": 0.001,\n",
    "      \"beta1\": 0.9,\n",
    "      \"beta2\": 0.999,\n",
    "      \"epsilon\": 0.0000001\n",
    "    }\n",
    "  },\n",
    "  \"layers\": [\n",
    "    {\n",
    "      \"name\": \"data\",\n",
    "      \"type\": \"Data\",\n",
    "      \"source\": \"./file_list.0.txt\",\n",
    "      \"eval_source\": \"./file_list.5.txt\",\n",
    "      \"check\": \"Sum\",\n",
    "      \"label\": {\n",
    "        \"top\": \"label\",\n",
    "        \"label_dim\": 1\n",
    "      },\n",
    "      \"dense\": {\n",
    "        \"top\": \"dense\",\n",
    "        \"dense_dim\": 13\n",
    "      },\n",
    "      \"sparse\": [\n",
    "        {\n",
    "          \"top\": \"wide_data\",\n",
    "          \"type\": \"DistributedSlot\",\n",
    "          \"max_feature_num_per_sample\": 30,\n",
    "          \"slot_num\": 1\n",
    "        },\n",
    "        {\n",
    "          \"top\": \"deep_data\",\n",
    "          \"type\": \"DistributedSlot\",\n",
    "          \"max_feature_num_per_sample\": 30,\n",
    "          \"slot_num\": 26\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sparse_embedding2\",\n",
    "      \"type\": \"DistributedSlotSparseEmbeddingHash\",\n",
    "      \"bottom\": \"wide_data\",\n",
    "      \"top\": \"sparse_embedding2\",\n",
    "      \"sparse_embedding_hparam\": {\n",
    "        \"max_vocabulary_size_per_gpu\": 2322444,\n",
    "        \"embedding_vec_size\": 1,\n",
    "        \"combiner\": 0\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sparse_embedding1\",\n",
    "      \"type\": \"DistributedSlotSparseEmbeddingHash\",\n",
    "      \"bottom\": \"deep_data\",\n",
    "      \"top\": \"sparse_embedding1\",\n",
    "      \"sparse_embedding_hparam\": {\n",
    "        \"max_vocabulary_size_per_gpu\": 2322444,\n",
    "        \"embedding_vec_size\": 16,\n",
    "        \"combiner\": 0\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"reshape1\",\n",
    "      \"type\": \"Reshape\",\n",
    "      \"bottom\": \"sparse_embedding1\",\n",
    "      \"top\": \"reshape1\",\n",
    "      \"leading_dim\": 416\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"reshape2\",\n",
    "      \"type\": \"Reshape\",\n",
    "      \"bottom\": \"sparse_embedding2\",\n",
    "      \"top\": \"reshape2\",\n",
    "      \"leading_dim\": 1\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"concat1\",\n",
    "      \"type\": \"Concat\",\n",
    "      \"bottom\": [\n",
    "        \"reshape1\",\n",
    "        \"dense\"\n",
    "      ],\n",
    "      \"top\": \"concat1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc1\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"concat1\",\n",
    "      \"top\": \"fc1\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1024\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu1\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc1\",\n",
    "      \"top\": \"relu1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"dropout1\",\n",
    "      \"type\": \"Dropout\",\n",
    "      \"rate\": 0.5,\n",
    "      \"bottom\": \"relu1\",\n",
    "      \"top\": \"dropout1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc2\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"dropout1\",\n",
    "      \"top\": \"fc2\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1024\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu2\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc2\",\n",
    "      \"top\": \"relu2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"dropout2\",\n",
    "      \"type\": \"Dropout\",\n",
    "      \"rate\": 0.5,\n",
    "      \"bottom\": \"relu2\",\n",
    "      \"top\": \"dropout2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc4\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"dropout2\",\n",
    "      \"top\": \"fc4\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"add1\",\n",
    "      \"type\": \"Add\",\n",
    "      \"bottom\": [\n",
    "        \"fc4\",\n",
    "        \"reshape2\"\n",
    "      ],\n",
    "      \"top\": \"add1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"loss\",\n",
    "      \"type\": \"BinaryCrossEntropyLoss\",\n",
    "      \"bottom\": [\n",
    "        \"add1\",\n",
    "        \"label\"\n",
    "      ],\n",
    "      \"top\": \"loss\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write the Python script. \n",
    "   Ensure that the `repeat_dataset` parameter is set to `False` within the script, which indicates that the file list needs to be specified before submitting the sess.train() or sess.evaluation() calls. Additionally, be sure to create a write-enabled directory for storing the temporary files for model oversubscribing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_from_scratch.py\n",
    "from hugectr import Session, solver_parser_helper\n",
    "import sys\n",
    "\n",
    "def train_from_scratch(json_file):\n",
    "  dataset = [(\"./file_list.\"+str(i)+\".txt\", \"./file_list.\"+str(i)+\".keyset\") for i in range(5)]\n",
    "  solver_config = solver_parser_helper(seed = 0,\n",
    "                                     batchsize = 16384,\n",
    "                                     batchsize_eval =16384,\n",
    "                                     model_file = \"\",\n",
    "                                     embedding_files = [],\n",
    "                                     vvgpu = [[0]],\n",
    "                                     use_mixed_precision = False,\n",
    "                                     scaler = 1.0,\n",
    "                                     i64_input_key = False,\n",
    "                                     use_algorithm_search = True,\n",
    "                                     use_cuda_graph = True,\n",
    "                                     repeat_dataset = False\n",
    "                                    )\n",
    "  sess = Session(solver_config, json_file, True, \"./temp_embedding\")\n",
    "  data_reader_train = sess.get_data_reader_train()\n",
    "  data_reader_eval = sess.get_data_reader_eval()\n",
    "  data_reader_eval.set_file_list_source(\"./file_list.5.txt\")\n",
    "  model_oversubscriber = sess.get_model_oversubscriber()\n",
    "  iteration = 0\n",
    "  for file_list, keyset_file in dataset:\n",
    "    data_reader_train.set_file_list_source(file_list)\n",
    "    model_oversubscriber.update(keyset_file)\n",
    "    while True:\n",
    "      good = sess.train()\n",
    "      if good == False:\n",
    "        break\n",
    "      if iteration % 100 == 0:\n",
    "        sess.check_overflow()\n",
    "        sess.copy_weights_for_evaluation()\n",
    "        data_reader_eval = sess.get_data_reader_eval()\n",
    "        good_eval = True\n",
    "        j = 0\n",
    "        while good_eval:\n",
    "          if j >= solver_config.max_eval_batches:\n",
    "            break\n",
    "          good_eval = sess.eval()\n",
    "          j += 1\n",
    "        if good_eval == False:\n",
    "          data_reader_eval.set_file_list_source()\n",
    "        metrics = sess.get_eval_metrics()\n",
    "        print(\"[HUGECTR][INFO] iter: {}, metrics: {}\".format(iteration, metrics))\n",
    "      iteration += 1\n",
    "    print(\"[HUGECTR][INFO] trained with data in {}\".format(file_list))\n",
    "  sess.download_params_to_files(\"./\", iteration)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  json_file = sys.argv[1]\n",
    "  train_from_scratch(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_from_scratch.sh\n",
    "cd wdl_data_hugectr && \\\n",
    "mkdir -p temp_embedding && \\\n",
    "python3 ../wdl_from_scratch.py ../wdl_1gpu.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23d09h55m09s][HUGECTR][INFO]: Initial seed is 228832826\n",
      "[23d09h55m13s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: GeForce RTX 2080 Ti\n",
      "[23d09h55m14s][HUGECTR][INFO]: cache_eval_data is not specified using default: 0\n",
      "[23d09h55m14s][HUGECTR][INFO]: max_nnz is not specified using default: 30\n",
      "[23d09h55m14s][HUGECTR][INFO]: max_nnz is not specified using default: 30\n",
      "[23d09h55m14s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[23d09h55m14s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[23d09h55m14s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=2322444\n",
      "[23d09h55m14s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=2322444\n",
      "[23d09h55m15s][HUGECTR][INFO]: Traning from scratch, no snapshot file specified\n",
      "[23d09h55m15s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h55m15s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h55m15s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 0\n",
      "[23d09h55m15s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 0\n",
      "[HUGECTR][INFO] iter: 0, metrics: [('AUC', 0.5151771903038025)]\n",
      "[HUGECTR][INFO] iter: 100, metrics: [('AUC', 0.7732792496681213)]\n",
      "[HUGECTR][INFO] iter: 200, metrics: [('AUC', 0.7761018872261047)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.0.txt\n",
      "[23d09h55m35s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h55m35s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h55m35s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h55m35s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h55m37s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 237\n",
      "[23d09h55m38s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 896\n",
      "[HUGECTR][INFO] iter: 300, metrics: [('AUC', 0.7701977491378784)]\n",
      "[HUGECTR][INFO] iter: 400, metrics: [('AUC', 0.7870358228683472)]\n",
      "[HUGECTR][INFO] iter: 500, metrics: [('AUC', 0.7913110852241516)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.1.txt\n",
      "[23d09h55m56s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h55m56s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h55m57s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h55m57s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h55m58s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 292\n",
      "[23d09h55m59s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1084\n",
      "[HUGECTR][INFO] iter: 600, metrics: [('AUC', 0.7919529676437378)]\n",
      "[HUGECTR][INFO] iter: 700, metrics: [('AUC', 0.7915957570075989)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.2.txt\n",
      "[23d09h56m18s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h56m18s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h56m18s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h56m18s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h56m20s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 318\n",
      "[23d09h56m21s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1165\n",
      "[HUGECTR][INFO] iter: 800, metrics: [('AUC', 0.7918049097061157)]\n",
      "[HUGECTR][INFO] iter: 900, metrics: [('AUC', 0.7956733703613281)]\n",
      "[HUGECTR][INFO] iter: 1000, metrics: [('AUC', 0.7857438921928406)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.3.txt\n",
      "[23d09h56m40s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h56m40s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h56m41s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h56m41s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h56m42s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 329\n",
      "[23d09h56m44s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1203\n",
      "[HUGECTR][INFO] iter: 1100, metrics: [('AUC', 0.7933880090713501)]\n",
      "[HUGECTR][INFO] iter: 1200, metrics: [('AUC', 0.7943921089172363)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.4.txt\n",
      "[23d09h57m02s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[23d09h57m02s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[23d09h57m02s][HUGECTR][INFO]: Done\n",
      "[23d09h57m02s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[23d09h57m02s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[23d09h57m02s][HUGECTR][INFO]: Done\n"
     ]
    }
   ],
   "source": [
    "!bash wdl_from_scratch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train from stored model\n",
    "\n",
    "Check the stored model files that will be used in the training. Dense model file embeddings should be passed to the respective model_file and embedding_files when calling `sess.solver_parser_helper()`. We will use the same JSON file and training data as the previous section. Also, all the other configurations for `solver_parser_helper` will also be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdl_data_hugectr/0_sparse_1260.model  wdl_data_hugectr/_dense_1260.model\r\n",
      "wdl_data_hugectr/1_sparse_1260.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls wdl_data_hugectr/*.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_from_stored.py\n",
    "from hugectr import Session, solver_parser_helper\n",
    "import sys\n",
    "\n",
    "def train_from_stored(json_file):\n",
    "  dataset = [(\"./file_list.\"+str(i)+\".txt\", \"./file_list.\"+str(i)+\".keyset\") for i in range(5)]\n",
    "  solver_config = solver_parser_helper(seed = 0,\n",
    "                                     batchsize = 16384,\n",
    "                                     batchsize_eval =16384,\n",
    "                                     model_file = \"_dense_1260.model\",\n",
    "                                     embedding_files = [\"0_sparse_1260.model\", \"1_sparse_1260.model\"],\n",
    "                                     vvgpu = [[0]],\n",
    "                                     use_mixed_precision = False,\n",
    "                                     scaler = 1.0,\n",
    "                                     i64_input_key = False,\n",
    "                                     use_algorithm_search = True,\n",
    "                                     use_cuda_graph = True,\n",
    "                                     repeat_dataset = False\n",
    "                                    )\n",
    "  sess = Session(solver_config, json_file, True, \"./temp_embedding\")\n",
    "  data_reader_train = sess.get_data_reader_train()\n",
    "  data_reader_eval = sess.get_data_reader_eval()\n",
    "  data_reader_eval.set_file_list_source(\"./file_list.5.txt\")\n",
    "  model_oversubscriber = sess.get_model_oversubscriber()\n",
    "  iteration = 1260\n",
    "  for file_list, keyset_file in dataset:\n",
    "    data_reader_train.set_file_list_source(file_list)\n",
    "    model_oversubscriber.update(keyset_file)\n",
    "    while True:\n",
    "      good = sess.train()\n",
    "      if good == False:\n",
    "        break\n",
    "      if iteration % 100 == 0:\n",
    "        sess.check_overflow()\n",
    "        sess.copy_weights_for_evaluation()\n",
    "        data_reader_eval = sess.get_data_reader_eval()\n",
    "        good_eval = True\n",
    "        j = 0\n",
    "        while good_eval:\n",
    "          if j >= solver_config.max_eval_batches:\n",
    "            break\n",
    "          good_eval = sess.eval()\n",
    "          j += 1\n",
    "        if good_eval == False:\n",
    "          data_reader_eval.set_file_list_source()\n",
    "        metrics = sess.get_eval_metrics()\n",
    "        print(\"[HUGECTR][INFO] iter: {}, metrics: {}\".format(iteration, metrics))\n",
    "      iteration += 1\n",
    "    print(\"[HUGECTR][INFO] trained with data in {}\".format(file_list))\n",
    "  sess.download_params_to_files(\"./\", iteration)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  json_file = sys.argv[1]\n",
    "  train_from_stored(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_from_stored.sh\n",
    "cd wdl_data_hugectr && \\\n",
    "mkdir -p temp_embedding && \\\n",
    "python3 ../wdl_from_stored.py ../wdl_1gpu.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23d09h57m48s][HUGECTR][INFO]: Initial seed is 1639765225\n",
      "[23d09h57m49s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: GeForce RTX 2080 Ti\n",
      "[23d09h57m49s][HUGECTR][INFO]: cache_eval_data is not specified using default: 0\n",
      "[23d09h57m49s][HUGECTR][INFO]: max_nnz is not specified using default: 30\n",
      "[23d09h57m49s][HUGECTR][INFO]: max_nnz is not specified using default: 30\n",
      "[23d09h57m49s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[23d09h57m49s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[23d09h57m49s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=2322444\n",
      "[23d09h57m49s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=2322444\n",
      "Loading dense model: _dense_1260.model\n",
      "[23d09h57m50s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h57m50s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h57m50s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 250\n",
      "[23d09h57m51s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 941\n",
      "[HUGECTR][INFO] iter: 1300, metrics: [('AUC', 0.7935425639152527)]\n",
      "[HUGECTR][INFO] iter: 1400, metrics: [('AUC', 0.8011876940727234)]\n",
      "[HUGECTR][INFO] iter: 1500, metrics: [('AUC', 0.7951088547706604)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.0.txt\n",
      "[23d09h57m57s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h57m57s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h57m57s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h57m57s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h57m58s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 302\n",
      "[23d09h57m59s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1116\n",
      "[HUGECTR][INFO] iter: 1600, metrics: [('AUC', 0.7907765507698059)]\n",
      "[HUGECTR][INFO] iter: 1700, metrics: [('AUC', 0.8003458976745605)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.1.txt\n",
      "[23d09h58m05s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h58m05s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h58m05s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h58m05s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h58m06s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 324\n",
      "[23d09h58m08s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1186\n",
      "[HUGECTR][INFO] iter: 1800, metrics: [('AUC', 0.79911869764328)]\n",
      "[HUGECTR][INFO] iter: 1900, metrics: [('AUC', 0.8008772134780884)]\n",
      "[HUGECTR][INFO] iter: 2000, metrics: [('AUC', 0.8000816106796265)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.2.txt\n",
      "[23d09h58m13s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h58m13s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h58m14s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h58m14s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h58m15s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 335\n",
      "[23d09h58m16s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1217\n",
      "[HUGECTR][INFO] iter: 2100, metrics: [('AUC', 0.8004366159439087)]\n",
      "[HUGECTR][INFO] iter: 2200, metrics: [('AUC', 0.8021687865257263)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.3.txt\n",
      "[23d09h58m22s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h58m22s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h58m22s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[23d09h58m22s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[23d09h58m23s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 348\n",
      "[23d09h58m25s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1236\n",
      "[HUGECTR][INFO] iter: 2300, metrics: [('AUC', 0.789278507232666)]\n",
      "[HUGECTR][INFO] iter: 2400, metrics: [('AUC', 0.7973716259002686)]\n",
      "[HUGECTR][INFO] iter: 2500, metrics: [('AUC', 0.7990200519561768)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.4.txt\n",
      "[23d09h58m31s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[23d09h58m31s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[23d09h58m31s][HUGECTR][INFO]: Done\n",
      "[23d09h58m31s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[23d09h58m31s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[23d09h58m31s][HUGECTR][INFO]: Done\n"
     ]
    }
   ],
   "source": [
    "!bash wdl_from_stored.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. API Signatures\n",
    "\n",
    "Here's list of all the API signatures within the HugeCTR Python interface that you need to get familiar with to successfully train your own model. As you can see from the above example, we've included `Session`, `DataReader`, `ModelPrefetcher` and `solver_parser_helper`.\n",
    "\n",
    "**Session**\n",
    "```bash\n",
    "class Session(pybind11_builtins.pybind11_object)\n",
    " |  Method resolution order:\n",
    " |      Session\n",
    " |      pybind11_builtins.pybind11_object\n",
    " |      builtins.object\n",
    " |  \n",
    " |  Methods defined here:\n",
    " |  \n",
    " |  __init__(...)\n",
    " |      __init__(self: hugectr.Session, solver_config: hugectr.SolverParser, config_file: str, use_model_oversubscriber: bool=False, temp_embedding_dir: str='') -> None\n",
    " |  \n",
    " |  check_overflow(...)\n",
    " |      check_overflow(self: hugectr.Session) -> None\n",
    " |  \n",
    " |  download_params_to_files(...)\n",
    " |      download_params_to_files(self: hugectr.Session, prefix: str, iter: int) -> hugectr.Error_t\n",
    " |  \n",
    " |  evaluation(...)\n",
    " |      evaluation(self: hugectr.Session) -> List[Tuple[str, float]]\n",
    " |  \n",
    " |  get_current_loss(...)\n",
    " |      get_current_loss(self: hugectr.Session) -> float\n",
    " |  \n",
    " |  get_data_reader_eval(...)\n",
    " |      get_data_reader_eval(self: hugectr.Session) -> hugectr.IDataReader\n",
    " |  \n",
    " |  get_data_reader_train(...)\n",
    " |      get_data_reader_train(self: hugectr.Session) -> hugectr.IDataReader\n",
    " |  \n",
    " |  get_model_oversubscriber(...)\n",
    " |      get_model_oversubscriber(self: hugectr.Session) -> hugectr.ModelPrefetcher\n",
    " |  \n",
    " |  set_learning_rate(...)\n",
    " |      set_learning_rate(self: hugectr.Session, lr: float) -> hugectr.Error_t\n",
    " |  \n",
    " |  start_data_reading(...)\n",
    " |      start_data_reading(self: hugectr.Session) -> None\n",
    " |  \n",
    " |  train(...)\n",
    " |      train(self: hugectr.Session) -> bool\n",
    " |  \n",
    "```\n",
    "\n",
    "**DataReader**\n",
    "```bash\n",
    "class DataReader32(IDataReader)\n",
    " |  Method resolution order:\n",
    " |      DataReader32\n",
    " |      IDataReader\n",
    " |      pybind11_builtins.pybind11_object\n",
    " |      builtins.object\n",
    " |  \n",
    " |  Methods defined here:\n",
    " |   \n",
    " |  set_file_list_source(...)\n",
    " |      set_file_list_source(self: hugectr.DataReader32, file_list: str='') -> None\n",
    " \n",
    "class DataReader64(IDataReader)\n",
    " |  Method resolution order:\n",
    " |      DataReader64\n",
    " |      IDataReader\n",
    " |      pybind11_builtins.pybind11_object\n",
    " |      builtins.object\n",
    " |  \n",
    " |  Methods defined here:\n",
    " |   \n",
    " |  set_file_list_source(...)\n",
    " |      set_file_list_source(self: hugectr.DataReader64, file_list: str='') -> None\n",
    "```\n",
    "\n",
    "**ModelPrefetcher**\n",
    "```bash\n",
    "class ModelPrefetcher(pybind11_builtins.pybind11_object)\n",
    " |  Method resolution order:\n",
    " |      ModelPrefetcher\n",
    " |      pybind11_builtins.pybind11_object\n",
    " |      builtins.object\n",
    " |  \n",
    " |  Methods defined here:\n",
    " |   \n",
    " |  update(...)\n",
    " |      update(*args, **kwargs)\n",
    " |      Overloaded function.\n",
    " |      \n",
    " |      1. update(self: hugectr.ModelPrefetcher, keyset_file: str) -> None\n",
    " |      \n",
    " |      2. update(self: hugectr.ModelPrefetcher, keyset_file_list: List[str]) -> None\n",
    " ```\n",
    " \n",
    " **solver_parser_helper**\n",
    " ```bash\n",
    " solver_parser_helper(...) method of builtins.PyCapsule instance\n",
    "    solver_parser_helper(seed: int=0, batchsize_eval: int=16384, batchsize: int=16384, model_file: str='', embedding_files: List[str]=[], vvgpu: List[List[int]]=[[0]], use_mixed_precision: bool=False, scaler: float=1.0, i64_input_key: bool=False, use_algorithm_search: bool=True, use_cuda_graph: bool=True, repeat_dataset: bool=False) -> hugectr.SolverParser\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
