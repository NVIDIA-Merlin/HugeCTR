{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924efc61",
   "metadata": {},
   "source": [
    "# Introduction to the HugeCTR Embedding Collection\n",
    "## overview\n",
    "Embedding collection enables users to group multiple embedding lookup operations together, in which vector size / id space / optimizer / table placement strategy of embedding tables can be different so that both flexibility and performance will be enhanced. \n",
    "This notebook includes:\n",
    "1. Introduce the API of the embedding collection.\n",
    "2. Introduce Embedding Table Placement Strategy(ETPS) and how to configure ETPS in embedding collection.\n",
    "3. Use embedding collection in the DLRM model for Criteo dataset as an example to show how to use embedding collection for your model training and evaluation. We provide two different ETPS as reference.\n",
    "\n",
    "## API\n",
    "There are 2 API related with embedding collection:\n",
    "### `hugectr.EmbeddingTablePlaceHolder`\n",
    "A placeholder for users to configure the attribute of the embedding table.\n",
    "Parameter:\n",
    "* `id_space`: Integer, the id space this table belongs to. Typically, it is the existing number of EmbeddingTablePlaceHolder when you create a new embedding table. \n",
    "* `max_vocabulary_size`: Integer, the vocabulary size of this table. If positive, then the number means how many embedding vectors this table contains. And it will cause overflow if you exceed during training or evaluation.  If you do not know the exact size of the embedding table, you can specify -1 which means the dynamic embedding table will be used and its size can be extended dynamically during training or evaluation.\n",
    "* `ev_size`: Integer, the vector size of embedding this embedding consists of.\n",
    "* `min_key`: Integer, the minimum value of input key.\n",
    "* `max_key`: Integer, the maximum value of input key.\n",
    "* `opt_params`: Optional, hugectr.Optimizer, the optimizer you want to use for this embedding table. If not specified, will use the optimizer specified in `hugectr.Model`.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "# create embedding table\n",
    "num_embedding = 26\n",
    "slot_size_array = [203931, 18598, 14092, 7012, 18977, 4, 6385, 1245, 49, 186213, 71328, 67288, 11, 2168, 7338, 61, 4, 932, 15, 204515, 141526, 199433, 60919, 9137, 71, 34]\n",
    "embedding_table_list = []\n",
    "for i in range(num_embedding):\n",
    "  embedding_table_list.append(hugectr.EmbeddingTablePlaceHolder(id_space=i, max_vocabulary_size=slot_size_array[i], ev_size=128, min_key=0, max_key=slot_size_array[i]))\n",
    "```\n",
    "\n",
    "### `hugectr.EmbeddingPlanner`\n",
    "`EmbeddingPlanner` provides `embedding_lookup` for users to specify lookup operations on `hugectr.EmbeddingTablePlaceHolder`. \n",
    "\n",
    "#### `embedding_lookup`\n",
    "Parameter:\n",
    "  * `emb_table` : hugectr.EmbeddingTablePlaceHolder, the embedding table you want to lookup upon.\n",
    "  * `input`: str, the input tensor name. Should be compatible with the `data_reader_sparse_param_array` in `hugectr.Input` in `hugectr.Model`\n",
    "  * `output`: str, the output tensor name. The shape of output tensor will be (batch_size, 1, embedding vector size).\n",
    "  * `combiner`: str, specify the combiner operation. Currently support `mean`, `sum` and `concat`.\n",
    "\n",
    "#### `create_embedding_collection`\n",
    "After finishing all `embedding_lookup`, users can use `EmbeddingPlanner.create_embedding_collection` to create `hugectr.EmbeddingCollection`, which can be added in `hugectr.Model` for training and evaluation.\n",
    "Parameter:\n",
    "  * `plan_file`: str, a json file which describes the table placement strategy. Will be covered in more detail in section `Plan and Embedding Table Placement Strategy`.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "embedding_planner = hugectr.EmbeddingPlanner()\n",
    "emb_vec_list = []\n",
    "for i in range(num_embedding):\n",
    "  embedding_planner.embedding_lookup(embedding_table_list[i], \"data{}\".format(i), \"emb_vec{}\".format(i), \"sum\")\n",
    "embedding_collection = embedding_planner.create_embedding_collection(\"./plan_7000.json\")\n",
    "```\n",
    "\n",
    "## Plan and Embedding Table Placement Strategy(ETPS)\n",
    "### What is ETPS and why is it important?\n",
    "In the recommendation system, the embedding table is usually so large that a single GPU is not able to hold all embedding tables, where sharding is needed to distribute embedding tables across multiple GPUs. We call such sharding strategy as **Embedding Table Placement Strategy**. It will hugely affect the performance of embedding, since different sharding strategies influence the communication between GPUs, and the optimal placement strategy is highly related with your dataset and lookup operation. So it's very important for users to configure a suitable table placement strategy for their own use case instead of providing a fixed one.   \n",
    "### How to configure ETPS in the embedding collection?\n",
    "We introduce a configurable ETPS interface so that users can adjust their table placement strategy according to their own use case. We use a json file to describe the ETPS in all GPUs, which we call a **plan file**. For example, consider you have 4 table and 5 lookup operations, which may like:\n",
    "```python\n",
    "num_embedding = 5\n",
    "slot_size_array = [...]\n",
    "embedding_table_list = []\n",
    "for i in range(num_embedding):\n",
    "  embedding_table_list.append(hugectr.EmbeddingTablePlaceHolder(id_space=i, max_vocabulary_size=slot_size_array[i], ev_size=128, min_key=0, max_key=slot_size_array[i]))\n",
    "\n",
    "embedding_planner = hugectr.EmbeddingPlanner()\n",
    "embedding_planner.embedding_lookup(embedding_table_list[0], \"data0\", \"emb_vec0\", \"sum\") # lookup 0\n",
    "embedding_planner.embedding_lookup(embedding_table_list[1], \"data1\", \"emb_vec1\", \"sum\") # lookup 1\n",
    "embedding_planner.embedding_lookup(embedding_table_list[2], \"data2\", \"emb_vec2\", \"sum\") # lookup 2\n",
    "embedding_planner.embedding_lookup(embedding_table_list[1], \"data3\", \"emb_vec3\", \"sum\") # lookup 3\n",
    "embedding_planner.embedding_lookup(embedding_table_list[3], \"data4\", \"emb_vec4\", \"sum\") # lookup 4\n",
    "```\n",
    "Now you want to configure the ETPS through a plan file. In the plan file, you can group several lookup operations together and do sharding. You can specify which lookup operation / which GPU / which portion of the embedding table in a plan file. The basic principle is one embedding table can only be sharded in a single way. For example, lookup 0 and lookup 3 take place upon the same embedding table. So lookup 0 and lookup 3 should be grouped together and sharded in the same way. \n",
    "If you have 2 GPUs and you want to use data parallel in all 4 embedding tables, you can write plan file like:\n",
    "```json\n",
    "[\n",
    "  [\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              0, 1, 2, 3, 4\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  0, 1, 2, 3, 4\n",
    "              ],\n",
    "              [\n",
    "                  0, 1, 2, 3, 4\n",
    "              ]\n",
    "          ],\n",
    "          \"num_sharding\": 1,\n",
    "          \"sharding_id\": 0,\n",
    "          \"table_placement_strategy\": \"dp\"\n",
    "      }\n",
    "  ],\n",
    "  [\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              0, 1, 2, 3, 4\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  0, 1, 2, 3, 4\n",
    "              ],\n",
    "              [\n",
    "                  0, 1, 2, 3, 4\n",
    "              ]\n",
    "          ],\n",
    "          \"num_sharding\": 1,\n",
    "          \"sharding_id\": 0,\n",
    "          \"table_placement_strategy\": \"dp\"\n",
    "      }\n",
    "  ]\n",
    "]\n",
    "```\n",
    "The plan file consists of a list which describes the table placement strategy in each gpu orderly. In each gpu, we use a list to describe multiple groups of sharded lookup operations. Each group of sharded lookup operation is a dictionary which includes:\n",
    "* `local_embedding_list`: a list of integers, which lookup operations current gpu contains.\n",
    "* `global_embedding_list`: a list of lists of integers, the current group lookup operations in all gpus.\n",
    "* `num_sharding`: an integer, how many shards you want to shard the current group lookup operations.\n",
    "* `sharding_id`: an integer, the index of the current group lookup operations.\n",
    "* `table_placement_strategy`: str, can be `mp` or `dp`. `mp` means model parallel and `dp` means data parallel.\n",
    "\n",
    "You are allowed to apply more complex ways for ETPS. Let's say we want to shard lookup 0, 1, 2, 3 across GPUs while lookup 4 to be data parallel. We can use:\n",
    "```json\n",
    "[\n",
    "  [\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              0,\n",
    "              2\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  0,\n",
    "                  2\n",
    "              ],\n",
    "              [\n",
    "                  1,\n",
    "                  3\n",
    "              ]\n",
    "          ],\n",
    "          \"table_placement_strategy\": \"mp\"\n",
    "      },\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              4\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  4\n",
    "              ],\n",
    "              [\n",
    "                  4\n",
    "              ]\n",
    "          ],\n",
    "          \"table_placement_strategy\": \"dp\"\n",
    "      }\n",
    "  ],\n",
    "  [\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              1,\n",
    "              3\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  0,\n",
    "                  2\n",
    "              ],\n",
    "              [\n",
    "                  1,\n",
    "                  3\n",
    "              ]\n",
    "          ],\n",
    "          \"table_placement_strategy\": \"mp\"\n",
    "      },\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              4\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  4\n",
    "              ],\n",
    "              [\n",
    "                  4\n",
    "              ]\n",
    "          ],\n",
    "          \"table_placement_strategy\": \"dp\"\n",
    "      }\n",
    "  ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a8a29",
   "metadata": {},
   "source": [
    "## DLRM Model\n",
    "### Parepare Data\n",
    "You can follow the instruction under [samples/deepfm/README.md#Preprocess the Dataset Through NVTabular](../samples/deepfm/README.md#Preprocess_the_Dataset_Through_NVTabular) to prepare data.\n",
    "### Prepare Train Script\n",
    "We will use single DGX-1 to run DLRM in this notebook. The GPU info in DGX-1 is as follows. It consists of 8 V100-SXM2 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf29e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 23 00:14:56 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    42W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    45W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    44W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    42W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    44W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    42W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    44W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    41W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0996e7",
   "metadata": {},
   "source": [
    "We build our train script through embedding collection API. And we will use command argument to pass plan file and test different ETPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1ddb6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlrm_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlrm_train.py\n",
    "import os\n",
    "import sys\n",
    "import hugectr\n",
    "\n",
    "plan_file = sys.argv[1]\n",
    "slot_size_array = [203931, 18598, 14092, 7012, 18977, 4, 6385, 1245, 49, 186213, 71328, 67288, 11, 2168, 7338, 61, 4, 932, 15, 204515, 141526, 199433, 60919, 9137, 71, 34]\n",
    "\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 70,\n",
    "                              batchsize_eval = 65536,\n",
    "                              batchsize = 65536,\n",
    "                              lr = 0.5,\n",
    "                              warmup_steps = 300,\n",
    "                              vvgpu = [[0,1,2,3,4,5,6,7]],\n",
    "                              repeat_dataset = True,\n",
    "                              i64_input_key = True,\n",
    "                              metrics_spec = {hugectr.MetricsType.AverageLoss:0.0},\n",
    "                              use_embedding_collection = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                  source = [\"./deepfm_data_nvt/train/_file_list.txt\"],\n",
    "                                  eval_source = \"./deepfm_data_nvt/val/_file_list.txt\",\n",
    "                                  check_type=hugectr.Check_t.Non,\n",
    "                                  slot_size_array = slot_size_array)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.SGD,\n",
    "                                    update_type = hugectr.Update_t.Local,\n",
    "                                    atomic_update = True)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "num_embedding = 26\n",
    "\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data{}\".format(i), 1, False, 1) for i in range(num_embedding)]))\n",
    "\n",
    "# create embedding table\n",
    "embedding_table_list = []\n",
    "for i in range(num_embedding):\n",
    "  embedding_table_list.append(hugectr.EmbeddingTablePlaceHolder(id_space=i, max_vocabulary_size=slot_size_array[i], ev_size=128, min_key=0, max_key=slot_size_array[i]))\n",
    "# create embedding planner and embedding collection\n",
    "embedding_planner = hugectr.EmbeddingPlanner()\n",
    "emb_vec_list = []\n",
    "for i in range(num_embedding):\n",
    "  embedding_planner.embedding_lookup(embedding_table_list[i], \"data{}\".format(i), \"emb_vec{}\".format(i), \"sum\")\n",
    "embedding_collection = embedding_planner.create_embedding_collection(plan_file)\n",
    "\n",
    "model.add(embedding_collection)\n",
    "# need concat\n",
    "model.add(hugectr.DenseLayer(layer_type=hugectr.Layer_t.Concat,\n",
    "                              bottom_names = [\"emb_vec{}\".format(i) for i in range(num_embedding)],\n",
    "                              top_names = [\"sparse_embedding1\"],\n",
    "                              axis = 1))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dense\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))                           \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=256))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))                            \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=128))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc3\"],\n",
    "                            top_names = [\"relu3\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Interaction, # interaction only support 3-D input\n",
    "                            bottom_names = [\"relu3\",\"sparse_embedding1\"],\n",
    "                            top_names = [\"interaction1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"interaction1\"],\n",
    "                            top_names = [\"fc4\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc4\"],\n",
    "                            top_names = [\"relu4\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu4\"],\n",
    "                            top_names = [\"fc5\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc5\"],\n",
    "                            top_names = [\"relu5\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu5\"],\n",
    "                            top_names = [\"fc6\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc6\"],\n",
    "                            top_names = [\"relu6\"]))                               \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu6\"],\n",
    "                            top_names = [\"fc7\"],\n",
    "                            num_output=256))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc7\"],\n",
    "                            top_names = [\"relu7\"]))                                                                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu7\"],\n",
    "                            top_names = [\"fc8\"],\n",
    "                            num_output=1))                                                                                           \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc8\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 1000, display = 100, eval_interval = 100, snapshot = 10000000, snapshot_prefix = \"dlrm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d6f22",
   "metadata": {},
   "source": [
    "### ETPS: Data parallel + Localized\n",
    "We want to put small size table as data parallel while for other tables, each table will be on single GPU and different GPU will hold different table(The same way we we use in `hugectr.LocalizedHashEmbedding`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f53e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plan(slot_size_array, num_gpus, plan_file):\n",
    "  mp_table = [i for i in range(len(slot_size_array)) if slot_size_array[i] > 6000]\n",
    "  dp_table = [i for i in range(len(slot_size_array)) if slot_size_array[i] <= 6000]\n",
    "  \n",
    "  # place table across all gpus\n",
    "  plan = []\n",
    "  for gpu_id in range(num_gpus):\n",
    "    single_gpu_plan = []\n",
    "    mp_plan = {\n",
    "      'local_embedding_list': [table_id for i, table_id in enumerate(mp_table) if i % num_gpus == gpu_id],\n",
    "      'table_placement_strategy': 'mp'\n",
    "    }\n",
    "    dp_plan = {\n",
    "      'local_embedding_list': dp_table,\n",
    "      'table_placement_strategy': 'dp'\n",
    "    }\n",
    "    single_gpu_plan.append(mp_plan)\n",
    "    single_gpu_plan.append(dp_plan)\n",
    "    plan.append(single_gpu_plan)\n",
    "  \n",
    "  # generate global view of table placement\n",
    "  mp_global_embedding_list = []\n",
    "  dp_global_embedding_list = []\n",
    "  for single_gpu_plan in plan:\n",
    "    mp_global_embedding_list.append(single_gpu_plan[0]['local_embedding_list'])\n",
    "    dp_global_embedding_list.append(single_gpu_plan[1]['local_embedding_list'])\n",
    "  for single_gpu_plan in plan:\n",
    "      single_gpu_plan[0]['global_embedding_list'] = mp_global_embedding_list\n",
    "      single_gpu_plan[1]['global_embedding_list'] = dp_global_embedding_list\n",
    "  print('plan is:', plan)\n",
    "  # dump plan file\n",
    "  import json\n",
    "  with open(plan_file, 'w') as f:\n",
    "    json.dump(plan, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bec0981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan is: [[{'local_embedding_list': [0, 11], 'table_placement_strategy': 'mp', 'global_embedding_list': [[0, 11], [1, 14], [2, 19], [3, 20], [4, 21], [6, 22], [9, 23], [10]]}, {'local_embedding_list': [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], 'table_placement_strategy': 'dp', 'global_embedding_list': [[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]]}], [{'local_embedding_list': [1, 14], 'table_placement_strategy': 'mp', 'global_embedding_list': [[0, 11], [1, 14], [2, 19], [3, 20], [4, 21], [6, 22], [9, 23], [10]]}, {'local_embedding_list': [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], 'table_placement_strategy': 'dp', 'global_embedding_list': [[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]]}], [{'local_embedding_list': [2, 19], 'table_placement_strategy': 'mp', 'global_embedding_list': [[0, 11], [1, 14], [2, 19], [3, 20], [4, 21], [6, 22], [9, 23], [10]]}, {'local_embedding_list': [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], 'table_placement_strategy': 'dp', 'global_embedding_list': [[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]]}], [{'local_embedding_list': [3, 20], 'table_placement_strategy': 'mp', 'global_embedding_list': [[0, 11], [1, 14], [2, 19], [3, 20], [4, 21], [6, 22], [9, 23], [10]]}, {'local_embedding_list': [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], 'table_placement_strategy': 'dp', 'global_embedding_list': [[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]]}], [{'local_embedding_list': [4, 21], 'table_placement_strategy': 'mp', 'global_embedding_list': [[0, 11], [1, 14], [2, 19], [3, 20], [4, 21], [6, 22], [9, 23], [10]]}, {'local_embedding_list': [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], 'table_placement_strategy': 'dp', 'global_embedding_list': [[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]]}], [{'local_embedding_list': [6, 22], 'table_placement_strategy': 'mp', 'global_embedding_list': [[0, 11], [1, 14], [2, 19], [3, 20], [4, 21], [6, 22], [9, 23], [10]]}, {'local_embedding_list': [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], 'table_placement_strategy': 'dp', 'global_embedding_list': [[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]]}], [{'local_embedding_list': [9, 23], 'table_placement_strategy': 'mp', 'global_embedding_list': [[0, 11], [1, 14], [2, 19], [3, 20], [4, 21], [6, 22], [9, 23], [10]]}, {'local_embedding_list': [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], 'table_placement_strategy': 'dp', 'global_embedding_list': [[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]]}], [{'local_embedding_list': [10], 'table_placement_strategy': 'mp', 'global_embedding_list': [[0, 11], [1, 14], [2, 19], [3, 20], [4, 21], [6, 22], [9, 23], [10]]}, {'local_embedding_list': [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], 'table_placement_strategy': 'dp', 'global_embedding_list': [[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25], [5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]]}]]\n"
     ]
    }
   ],
   "source": [
    "slot_size_array = [203931, 18598, 14092, 7012, 18977, 4, 6385, 1245, 49, 186213, 71328, 67288, 11, 2168, 7338, 61, 4, 932, 15, 204515, 141526, 199433, 60919, 9137, 71, 34]\n",
    "generate_plan(slot_size_array, 8, \"./dp_and_localized_plan.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24015e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 3.7\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][05:57:36.769][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][05:57:36.769][INFO][RK0][main]: Global seed is 1585076481\n",
      "[HCTR][05:57:37.239][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "  GPU 1 ->  node 0\n",
      "  GPU 2 ->  node 0\n",
      "  GPU 3 ->  node 0\n",
      "  GPU 4 ->  node 1\n",
      "  GPU 5 ->  node 1\n",
      "  GPU 6 ->  node 1\n",
      "  GPU 7 ->  node 1\n",
      "[HCTR][05:57:48.594][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][05:57:48.594][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][05:57:48.783][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][05:57:48.795][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][05:57:48.796][INFO][RK0][main]: Device 0: Tesla V100-SXM2-16GB\n",
      "[HCTR][05:57:48.797][INFO][RK0][main]: Device 1: Tesla V100-SXM2-16GB\n",
      "[HCTR][05:57:48.798][INFO][RK0][main]: Device 2: Tesla V100-SXM2-16GB\n",
      "[HCTR][05:57:48.798][INFO][RK0][main]: Device 3: Tesla V100-SXM2-16GB\n",
      "[HCTR][05:57:48.799][INFO][RK0][main]: Device 4: Tesla V100-SXM2-16GB\n",
      "[HCTR][05:57:48.800][INFO][RK0][main]: Device 5: Tesla V100-SXM2-16GB\n",
      "[HCTR][05:57:48.800][INFO][RK0][main]: Device 6: Tesla V100-SXM2-16GB\n",
      "[HCTR][05:57:48.801][INFO][RK0][main]: Device 7: Tesla V100-SXM2-16GB\n",
      "[HCTR][05:57:48.803][INFO][RK0][main]: num of DataReader workers: 8\n",
      "[HCTR][05:57:48.828][DEBUG][RK0][tid #139904050779904]: file_name_ deepfm_data_nvt/train/4.4f7e95ed8f9b4bcc9b63c5f3278e6905.parquet file_total_rows_ 4580476\n",
      "[HCTR][05:57:48.829][DEBUG][RK0][tid #139900800198400]: file_name_ deepfm_data_nvt/train/6.92133f3ee3664684854969202958122f.parquet file_total_rows_ 4581782\n",
      "[HCTR][05:57:48.830][DEBUG][RK0][tid #139900791805696]: file_name_ deepfm_data_nvt/train/7.9345ade3421b40a5803f518c48ae436f.parquet file_total_rows_ 4589169\n",
      "[HCTR][05:57:48.830][DEBUG][RK0][tid #139901345462016]: file_name_ deepfm_data_nvt/train/1.c7b6f2423fec47ff97a09ec95f6346f9.parquet file_total_rows_ 4585117\n",
      "[HCTR][05:57:48.830][DEBUG][RK0][tid #139904059172608]: file_name_ deepfm_data_nvt/train/3.4b192542e2ad4cc8b745feb142d1878a.parquet file_total_rows_ 4581022\n",
      "[HCTR][05:57:48.831][DEBUG][RK0][tid #139901337069312]: file_name_ deepfm_data_nvt/train/0.1738817c5c5c47dba75a428d0837cbc3.parquet file_total_rows_ 4586722\n",
      "[HCTR][05:57:48.831][DEBUG][RK0][tid #139911906715392]: file_name_ deepfm_data_nvt/train/2.6b134d3f8f0a4f0d9453f1d7c08f74d5.parquet file_total_rows_ 4584304\n",
      "[HCTR][05:57:48.834][INFO][RK0][main]: Vocabulary size: 1221286\n",
      "[HCTR][05:57:48.834][DEBUG][RK0][tid #139900674373376]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][05:57:48.834][DEBUG][RK0][tid #139900665980672]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][05:57:48.834][DEBUG][RK0][tid #139900657587968]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][05:57:48.834][DEBUG][RK0][tid #139900540155648]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][05:57:48.834][DEBUG][RK0][tid #139900531762944]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][05:57:48.835][DEBUG][RK0][tid #139904042387200]: file_name_ deepfm_data_nvt/train/5.c5b89db1e82d4842998d560796eab838.parquet file_total_rows_ 4583901\n",
      "[HCTR][05:57:48.835][DEBUG][RK0][tid #139900523370240]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][05:57:48.835][DEBUG][RK0][tid #139900397545216]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][05:57:48.835][DEBUG][RK0][tid #139900405937920]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][05:57:49.314][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][05:58:26.619][INFO][RK0][main]: label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data0,data1,data2,data3,data4,data5,data6,data7,data8,data9,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19,data20,data21,data22,data23,data24,data25\n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Concat                                  emb_vec0                      sparse_embedding1             (None, 26, 128)               \n",
      "                                        emb_vec1                                                                                  \n",
      "                                        emb_vec2                                                                                  \n",
      "                                        emb_vec3                                                                                  \n",
      "                                        emb_vec4                                                                                  \n",
      "                                        emb_vec5                                                                                  \n",
      "                                        emb_vec6                                                                                  \n",
      "                                        emb_vec7                                                                                  \n",
      "                                        emb_vec8                                                                                  \n",
      "                                        emb_vec9                                                                                  \n",
      "                                        emb_vec10                                                                                 \n",
      "                                        emb_vec11                                                                                 \n",
      "                                        emb_vec12                                                                                 \n",
      "                                        emb_vec13                                                                                 \n",
      "                                        emb_vec14                                                                                 \n",
      "                                        emb_vec15                                                                                 \n",
      "                                        emb_vec16                                                                                 \n",
      "                                        emb_vec17                                                                                 \n",
      "                                        emb_vec18                                                                                 \n",
      "                                        emb_vec19                                                                                 \n",
      "                                        emb_vec20                                                                                 \n",
      "                                        emb_vec21                                                                                 \n",
      "                                        emb_vec22                                                                                 \n",
      "                                        emb_vec23                                                                                 \n",
      "                                        emb_vec24                                                                                 \n",
      "                                        emb_vec25                                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dense                         fc1                           (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu1                         fc2                           (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu2                         fc3                           (None, 128)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc3                           relu3                         (None, 128)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Interaction                             relu3                         interaction1                  (None, 480)                   \n",
      "                                        sparse_embedding1                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            interaction1                  fc4                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc4                           relu4                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu4                         fc5                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc5                           relu5                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu5                         fc6                           (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc6                           relu6                         (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu6                         fc7                           (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc7                           relu7                         (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu7                         fc8                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  fc8                           loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][05:58:26.619][INFO][RK0][main]: Use non-epoch mode with number of iterations: 1000\n",
      "[HCTR][05:58:26.619][INFO][RK0][main]: Training batchsize: 65536, evaluation batchsize: 65536\n",
      "[HCTR][05:58:26.619][INFO][RK0][main]: Evaluation interval: 100, snapshot interval: 10000000\n",
      "[HCTR][05:58:26.619][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][05:58:26.619][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][05:58:26.619][INFO][RK0][main]: lr: 0.500000, warmup_steps: 300, end_lr: 0.000000\n",
      "[HCTR][05:58:26.619][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][05:58:26.619][INFO][RK0][main]: Training source file: ./deepfm_data_nvt/train/_file_list.txt\n",
      "[HCTR][05:58:26.619][INFO][RK0][main]: Evaluation source file: ./deepfm_data_nvt/val/_file_list.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][05:58:34.800][INFO][RK0][main]: Iter: 100 Time(100 iters): 8.11162s Loss: 0.140444 lr:0.168333\n",
      "[HCTR][05:58:36.779][DEBUG][RK0][tid #139900674373376]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][05:58:36.817][DEBUG][RK0][tid #139900665980672]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][05:58:36.855][DEBUG][RK0][tid #139900657587968]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][05:58:36.892][DEBUG][RK0][tid #139900540155648]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][05:58:36.930][DEBUG][RK0][tid #139900531762944]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][05:58:36.971][DEBUG][RK0][tid #139900523370240]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][05:58:37.004][DEBUG][RK0][tid #139900405937920]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][05:58:37.038][DEBUG][RK0][tid #139900397545216]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][05:58:37.280][INFO][RK0][main]: Evaluation, AverageLoss: 0.141579\n",
      "[HCTR][05:58:37.280][INFO][RK0][main]: Eval Time for 70 iters: 2.47942s\n",
      "[HCTR][05:58:45.373][INFO][RK0][main]: Iter: 200 Time(100 iters): 10.5028s Loss: 0.141643 lr:0.335\n",
      "[HCTR][05:58:47.427][DEBUG][RK0][tid #139900674373376]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][05:58:47.466][DEBUG][RK0][tid #139900665980672]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][05:58:47.504][DEBUG][RK0][tid #139900657587968]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][05:58:47.542][DEBUG][RK0][tid #139900540155648]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][05:58:47.580][DEBUG][RK0][tid #139900531762944]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][05:58:47.616][DEBUG][RK0][tid #139900523370240]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][05:58:47.654][DEBUG][RK0][tid #139900405937920]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][05:58:47.698][DEBUG][RK0][tid #139900397545216]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][05:58:47.874][INFO][RK0][main]: Evaluation, AverageLoss: 0.141112\n",
      "[HCTR][05:58:47.874][INFO][RK0][main]: Eval Time for 70 iters: 2.49689s\n",
      "[HCTR][05:58:55.995][INFO][RK0][main]: Iter: 300 Time(100 iters): 10.6218s Loss: 0.143321 lr:0.5\n",
      "[HCTR][05:58:58.129][DEBUG][RK0][tid #139900674373376]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][05:58:58.166][DEBUG][RK0][tid #139900665980672]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][05:58:58.204][DEBUG][RK0][tid #139900657587968]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][05:58:58.237][DEBUG][RK0][tid #139900540155648]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][05:58:58.273][DEBUG][RK0][tid #139900531762944]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][05:58:58.315][DEBUG][RK0][tid #139900523370240]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][05:58:58.352][DEBUG][RK0][tid #139900405937920]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][05:58:58.389][DEBUG][RK0][tid #139900397545216]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][05:58:58.498][INFO][RK0][main]: Evaluation, AverageLoss: 0.140532\n",
      "[HCTR][05:58:58.498][INFO][RK0][main]: Eval Time for 70 iters: 2.50318s\n",
      "[HCTR][05:59:06.615][INFO][RK0][main]: Iter: 400 Time(100 iters): 10.5504s Loss: 0.141971 lr:0.5\n",
      "[HCTR][05:59:08.533][DEBUG][RK0][tid #139900674373376]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][05:59:08.604][DEBUG][RK0][tid #139900657587968]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][05:59:08.643][DEBUG][RK0][tid #139900540155648]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][05:59:08.790][DEBUG][RK0][tid #139900397545216]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][05:59:08.863][DEBUG][RK0][tid #139900665980672]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][05:59:08.971][DEBUG][RK0][tid #139900531762944]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][05:59:09.010][DEBUG][RK0][tid #139900523370240]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][05:59:09.045][DEBUG][RK0][tid #139900405937920]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][05:59:09.116][INFO][RK0][main]: Evaluation, AverageLoss: 0.140388\n",
      "[HCTR][05:59:09.116][INFO][RK0][main]: Eval Time for 70 iters: 2.50056s\n",
      "[HCTR][05:59:17.208][INFO][RK0][main]: Iter: 500 Time(100 iters): 10.5267s Loss: 0.142007 lr:0.5\n",
      "[HCTR][05:59:19.155][DEBUG][RK0][tid #139900674373376]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][05:59:19.192][DEBUG][RK0][tid #139900665980672]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][05:59:19.225][DEBUG][RK0][tid #139900657587968]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][05:59:19.264][DEBUG][RK0][tid #139900540155648]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][05:59:19.297][DEBUG][RK0][tid #139900531762944]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][05:59:19.338][DEBUG][RK0][tid #139900523370240]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][05:59:19.371][DEBUG][RK0][tid #139900405937920]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][05:59:19.406][DEBUG][RK0][tid #139900397545216]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][05:59:19.655][INFO][RK0][main]: Evaluation, AverageLoss: 0.139941\n",
      "[HCTR][05:59:19.655][INFO][RK0][main]: Eval Time for 70 iters: 2.44712s\n",
      "[HCTR][05:59:23.082][DEBUG][RK0][tid #139901337069312]: file_name_ deepfm_data_nvt/train/0.1738817c5c5c47dba75a428d0837cbc3.parquet file_total_rows_ 4586722\n",
      "[HCTR][05:59:23.162][DEBUG][RK0][tid #139901345462016]: file_name_ deepfm_data_nvt/train/1.c7b6f2423fec47ff97a09ec95f6346f9.parquet file_total_rows_ 4585117\n",
      "[HCTR][05:59:23.242][DEBUG][RK0][tid #139911906715392]: file_name_ deepfm_data_nvt/train/2.6b134d3f8f0a4f0d9453f1d7c08f74d5.parquet file_total_rows_ 4584304\n",
      "[HCTR][05:59:23.322][DEBUG][RK0][tid #139904059172608]: file_name_ deepfm_data_nvt/train/3.4b192542e2ad4cc8b745feb142d1878a.parquet file_total_rows_ 4581022\n",
      "[HCTR][05:59:23.404][DEBUG][RK0][tid #139904050779904]: file_name_ deepfm_data_nvt/train/4.4f7e95ed8f9b4bcc9b63c5f3278e6905.parquet file_total_rows_ 4580476\n",
      "[HCTR][05:59:23.486][DEBUG][RK0][tid #139904042387200]: file_name_ deepfm_data_nvt/train/5.c5b89db1e82d4842998d560796eab838.parquet file_total_rows_ 4583901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][05:59:23.566][DEBUG][RK0][tid #139900800198400]: file_name_ deepfm_data_nvt/train/6.92133f3ee3664684854969202958122f.parquet file_total_rows_ 4581782\n",
      "[HCTR][05:59:24.283][DEBUG][RK0][tid #139900791805696]: file_name_ deepfm_data_nvt/train/7.9345ade3421b40a5803f518c48ae436f.parquet file_total_rows_ 4589169\n",
      "[HCTR][05:59:27.744][INFO][RK0][main]: Iter: 600 Time(100 iters): 10.466s Loss: 0.136762 lr:0.5\n",
      "[HCTR][05:59:29.786][DEBUG][RK0][tid #139900674373376]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][05:59:29.823][DEBUG][RK0][tid #139900665980672]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][05:59:29.860][DEBUG][RK0][tid #139900657587968]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][05:59:29.903][DEBUG][RK0][tid #139900540155648]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][05:59:29.945][DEBUG][RK0][tid #139900531762944]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][05:59:29.983][DEBUG][RK0][tid #139900523370240]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][05:59:30.026][DEBUG][RK0][tid #139900405937920]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][05:59:30.069][DEBUG][RK0][tid #139900397545216]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][05:59:30.249][INFO][RK0][main]: Evaluation, AverageLoss: 0.13953\n",
      "[HCTR][05:59:30.249][INFO][RK0][main]: Eval Time for 70 iters: 2.50127s\n",
      "[HCTR][05:59:38.348][INFO][RK0][main]: Iter: 700 Time(100 iters): 10.5305s Loss: 0.141831 lr:0.5\n",
      "[HCTR][05:59:40.470][DEBUG][RK0][tid #139900674373376]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][05:59:40.508][DEBUG][RK0][tid #139900665980672]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][05:59:40.546][DEBUG][RK0][tid #139900657587968]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][05:59:40.583][DEBUG][RK0][tid #139900540155648]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][05:59:40.626][DEBUG][RK0][tid #139900531762944]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][05:59:40.664][DEBUG][RK0][tid #139900523370240]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][05:59:40.706][DEBUG][RK0][tid #139900405937920]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][05:59:40.749][DEBUG][RK0][tid #139900397545216]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][05:59:40.861][INFO][RK0][main]: Evaluation, AverageLoss: 0.139417\n",
      "[HCTR][05:59:40.861][INFO][RK0][main]: Eval Time for 70 iters: 2.51215s\n",
      "[HCTR][05:59:48.978][INFO][RK0][main]: Iter: 800 Time(100 iters): 10.5593s Loss: 0.143233 lr:0.5\n",
      "[HCTR][05:59:50.888][DEBUG][RK0][tid #139900674373376]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][05:59:50.959][DEBUG][RK0][tid #139900657587968]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][05:59:50.997][DEBUG][RK0][tid #139900540155648]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][05:59:51.148][DEBUG][RK0][tid #139900397545216]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][05:59:51.225][DEBUG][RK0][tid #139900665980672]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][05:59:51.330][DEBUG][RK0][tid #139900531762944]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][05:59:51.367][DEBUG][RK0][tid #139900523370240]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][05:59:51.410][DEBUG][RK0][tid #139900405937920]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][05:59:51.490][INFO][RK0][main]: Evaluation, AverageLoss: 0.139185\n",
      "[HCTR][05:59:51.490][INFO][RK0][main]: Eval Time for 70 iters: 2.51201s\n",
      "[HCTR][05:59:59.571][INFO][RK0][main]: Iter: 900 Time(100 iters): 10.5236s Loss: 0.138494 lr:0.5\n",
      "[HCTR][06:00:01.557][DEBUG][RK0][tid #139900674373376]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][06:00:01.594][DEBUG][RK0][tid #139900665980672]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][06:00:01.633][DEBUG][RK0][tid #139900657587968]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][06:00:01.676][DEBUG][RK0][tid #139900540155648]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][06:00:01.713][DEBUG][RK0][tid #139900531762944]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][06:00:01.751][DEBUG][RK0][tid #139900523370240]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][06:00:01.794][DEBUG][RK0][tid #139900405937920]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][06:00:01.836][DEBUG][RK0][tid #139900397545216]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][06:00:02.082][INFO][RK0][main]: Evaluation, AverageLoss: 0.139492\n",
      "[HCTR][06:00:02.082][INFO][RK0][main]: Eval Time for 70 iters: 2.51167s\n",
      "[HCTR][06:00:10.067][INFO][RK0][main]: Finish 1000 iterations with batchsize: 65536 in 103.45s.\n"
     ]
    }
   ],
   "source": [
    "!python3 dlrm_train.py ./dp_and_localized_plan.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ddef90",
   "metadata": {},
   "source": [
    "### ETPS: Distributed \n",
    "We want to distributed all tables across all gpus(the same way we use in  `hugectr.DistributedHashEmbedding`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a64116e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distributed_plan(slot_size_array, num_gpus, plan_file):\n",
    "  # place table across all gpus\n",
    "  plan = []\n",
    "  for gpu_id in range(num_gpus):\n",
    "    distributed_plan = {\n",
    "      'local_embedding_list': [table_id for table_id in range(len(slot_size_array))],\n",
    "      'table_placement_strategy': 'mp',\n",
    "      'sharding_id': gpu_id,\n",
    "      'num_sharding': num_gpus\n",
    "    }\n",
    "    plan.append([distributed_plan])\n",
    "  \n",
    "  # generate global view of table placement\n",
    "  distributed_global_embedding_list = []\n",
    "  for single_gpu_plan in plan:\n",
    "    distributed_global_embedding_list.append(single_gpu_plan[0]['local_embedding_list'])\n",
    "  for single_gpu_plan in plan:\n",
    "      single_gpu_plan[0]['global_embedding_list'] = distributed_global_embedding_list\n",
    "  print('plan is:', plan)\n",
    "  # dump plan file\n",
    "  import json\n",
    "  with open(plan_file, 'w') as f:\n",
    "    json.dump(plan, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f572c884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan is: [[{'local_embedding_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], 'table_placement_strategy': 'mp', 'sharding_id': 0, 'num_sharding': 8, 'global_embedding_list': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]]}], [{'local_embedding_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], 'table_placement_strategy': 'mp', 'sharding_id': 1, 'num_sharding': 8, 'global_embedding_list': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]]}], [{'local_embedding_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], 'table_placement_strategy': 'mp', 'sharding_id': 2, 'num_sharding': 8, 'global_embedding_list': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]]}], [{'local_embedding_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], 'table_placement_strategy': 'mp', 'sharding_id': 3, 'num_sharding': 8, 'global_embedding_list': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]]}], [{'local_embedding_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], 'table_placement_strategy': 'mp', 'sharding_id': 4, 'num_sharding': 8, 'global_embedding_list': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]]}], [{'local_embedding_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], 'table_placement_strategy': 'mp', 'sharding_id': 5, 'num_sharding': 8, 'global_embedding_list': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]]}], [{'local_embedding_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], 'table_placement_strategy': 'mp', 'sharding_id': 6, 'num_sharding': 8, 'global_embedding_list': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]]}], [{'local_embedding_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], 'table_placement_strategy': 'mp', 'sharding_id': 7, 'num_sharding': 8, 'global_embedding_list': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]]}]]\n"
     ]
    }
   ],
   "source": [
    "slot_size_array = [203931, 18598, 14092, 7012, 18977, 4, 6385, 1245, 49, 186213, 71328, 67288, 11, 2168, 7338, 61, 4, 932, 15, 204515, 141526, 199433, 60919, 9137, 71, 34]\n",
    "generate_distributed_plan(slot_size_array, 8, \"./distributed_plan.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d06fbf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 3.7\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][06:00:21.241][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][06:00:21.241][INFO][RK0][main]: Global seed is 2402814931\n",
      "[HCTR][06:00:21.710][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "  GPU 1 ->  node 0\n",
      "  GPU 2 ->  node 0\n",
      "  GPU 3 ->  node 0\n",
      "  GPU 4 ->  node 1\n",
      "  GPU 5 ->  node 1\n",
      "  GPU 6 ->  node 1\n",
      "  GPU 7 ->  node 1\n",
      "[HCTR][06:00:33.192][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][06:00:33.193][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][06:00:33.374][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][06:00:33.387][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][06:00:33.388][INFO][RK0][main]: Device 0: Tesla V100-SXM2-16GB\n",
      "[HCTR][06:00:33.389][INFO][RK0][main]: Device 1: Tesla V100-SXM2-16GB\n",
      "[HCTR][06:00:33.390][INFO][RK0][main]: Device 2: Tesla V100-SXM2-16GB\n",
      "[HCTR][06:00:33.390][INFO][RK0][main]: Device 3: Tesla V100-SXM2-16GB\n",
      "[HCTR][06:00:33.391][INFO][RK0][main]: Device 4: Tesla V100-SXM2-16GB\n",
      "[HCTR][06:00:33.392][INFO][RK0][main]: Device 5: Tesla V100-SXM2-16GB\n",
      "[HCTR][06:00:33.393][INFO][RK0][main]: Device 6: Tesla V100-SXM2-16GB\n",
      "[HCTR][06:00:33.393][INFO][RK0][main]: Device 7: Tesla V100-SXM2-16GB\n",
      "[HCTR][06:00:33.395][INFO][RK0][main]: num of DataReader workers: 8\n",
      "[HCTR][06:00:33.423][DEBUG][RK0][tid #140244531799808]: file_name_ deepfm_data_nvt/train/0.1738817c5c5c47dba75a428d0837cbc3.parquet file_total_rows_ 4586722\n",
      "[HCTR][06:00:33.424][DEBUG][RK0][tid #140244657624832]: file_name_ deepfm_data_nvt/train/5.c5b89db1e82d4842998d560796eab838.parquet file_total_rows_ 4583901\n",
      "[HCTR][06:00:33.424][DEBUG][RK0][tid #140244540192512]: file_name_ deepfm_data_nvt/train/1.c7b6f2423fec47ff97a09ec95f6346f9.parquet file_total_rows_ 4585117\n",
      "[HCTR][06:00:33.425][DEBUG][RK0][tid #140244674410240]: file_name_ deepfm_data_nvt/train/3.4b192542e2ad4cc8b745feb142d1878a.parquet file_total_rows_ 4581022\n",
      "[HCTR][06:00:33.425][DEBUG][RK0][tid #140244397582080]: file_name_ deepfm_data_nvt/train/6.92133f3ee3664684854969202958122f.parquet file_total_rows_ 4581782\n",
      "[HCTR][06:00:33.425][DEBUG][RK0][tid #140244389189376]: file_name_ deepfm_data_nvt/train/7.9345ade3421b40a5803f518c48ae436f.parquet file_total_rows_ 4589169\n",
      "[HCTR][06:00:33.425][DEBUG][RK0][tid #140248860321536]: file_name_ deepfm_data_nvt/train/2.6b134d3f8f0a4f0d9453f1d7c08f74d5.parquet file_total_rows_ 4584304\n",
      "[HCTR][06:00:33.428][INFO][RK0][main]: Vocabulary size: 1221286\n",
      "[HCTR][06:00:33.428][DEBUG][RK0][tid #140244271757056]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][06:00:33.428][DEBUG][RK0][tid #140244263364352]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][06:00:33.428][DEBUG][RK0][tid #140244254971648]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][06:00:33.429][DEBUG][RK0][tid #140244666017536]: file_name_ deepfm_data_nvt/train/4.4f7e95ed8f9b4bcc9b63c5f3278e6905.parquet file_total_rows_ 4580476\n",
      "[HCTR][06:00:33.429][DEBUG][RK0][tid #140239557351168]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][06:00:33.429][DEBUG][RK0][tid #140239574136576]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][06:00:33.429][DEBUG][RK0][tid #140239565743872]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][06:00:33.429][DEBUG][RK0][tid #140239305701120]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][06:00:33.429][DEBUG][RK0][tid #140239297308416]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][06:00:34.086][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][06:01:11.395][INFO][RK0][main]: label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data0,data1,data2,data3,data4,data5,data6,data7,data8,data9,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19,data20,data21,data22,data23,data24,data25\n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Concat                                  emb_vec0                      sparse_embedding1             (None, 26, 128)               \n",
      "                                        emb_vec1                                                                                  \n",
      "                                        emb_vec2                                                                                  \n",
      "                                        emb_vec3                                                                                  \n",
      "                                        emb_vec4                                                                                  \n",
      "                                        emb_vec5                                                                                  \n",
      "                                        emb_vec6                                                                                  \n",
      "                                        emb_vec7                                                                                  \n",
      "                                        emb_vec8                                                                                  \n",
      "                                        emb_vec9                                                                                  \n",
      "                                        emb_vec10                                                                                 \n",
      "                                        emb_vec11                                                                                 \n",
      "                                        emb_vec12                                                                                 \n",
      "                                        emb_vec13                                                                                 \n",
      "                                        emb_vec14                                                                                 \n",
      "                                        emb_vec15                                                                                 \n",
      "                                        emb_vec16                                                                                 \n",
      "                                        emb_vec17                                                                                 \n",
      "                                        emb_vec18                                                                                 \n",
      "                                        emb_vec19                                                                                 \n",
      "                                        emb_vec20                                                                                 \n",
      "                                        emb_vec21                                                                                 \n",
      "                                        emb_vec22                                                                                 \n",
      "                                        emb_vec23                                                                                 \n",
      "                                        emb_vec24                                                                                 \n",
      "                                        emb_vec25                                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dense                         fc1                           (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu1                         fc2                           (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu2                         fc3                           (None, 128)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc3                           relu3                         (None, 128)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Interaction                             relu3                         interaction1                  (None, 480)                   \n",
      "                                        sparse_embedding1                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            interaction1                  fc4                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc4                           relu4                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu4                         fc5                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc5                           relu5                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu5                         fc6                           (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc6                           relu6                         (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu6                         fc7                           (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc7                           relu7                         (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu7                         fc8                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  fc8                           loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][06:01:11.395][INFO][RK0][main]: Use non-epoch mode with number of iterations: 1000\n",
      "[HCTR][06:01:11.395][INFO][RK0][main]: Training batchsize: 65536, evaluation batchsize: 65536\n",
      "[HCTR][06:01:11.395][INFO][RK0][main]: Evaluation interval: 100, snapshot interval: 10000000\n",
      "[HCTR][06:01:11.395][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][06:01:11.395][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][06:01:11.395][INFO][RK0][main]: lr: 0.500000, warmup_steps: 300, end_lr: 0.000000\n",
      "[HCTR][06:01:11.395][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][06:01:11.395][INFO][RK0][main]: Training source file: ./deepfm_data_nvt/train/_file_list.txt\n",
      "[HCTR][06:01:11.395][INFO][RK0][main]: Evaluation source file: ./deepfm_data_nvt/val/_file_list.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][06:01:26.874][INFO][RK0][main]: Iter: 100 Time(100 iters): 15.3271s Loss: 0.143344 lr:0.168333\n",
      "[HCTR][06:01:30.153][DEBUG][RK0][tid #140244271757056]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][06:01:30.211][DEBUG][RK0][tid #140244263364352]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][06:01:30.269][DEBUG][RK0][tid #140244254971648]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][06:01:30.328][DEBUG][RK0][tid #140239574136576]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][06:01:30.387][DEBUG][RK0][tid #140239565743872]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][06:01:30.445][DEBUG][RK0][tid #140239557351168]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][06:01:30.503][DEBUG][RK0][tid #140239305701120]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][06:01:30.562][DEBUG][RK0][tid #140239297308416]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][06:01:30.967][INFO][RK0][main]: Evaluation, AverageLoss: 0.144338\n",
      "[HCTR][06:01:30.967][INFO][RK0][main]: Eval Time for 70 iters: 4.09317s\n",
      "[HCTR][06:01:46.386][INFO][RK0][main]: Iter: 200 Time(100 iters): 19.5117s Loss: 0.141841 lr:0.335\n",
      "[HCTR][06:01:49.772][DEBUG][RK0][tid #140244271757056]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][06:01:49.830][DEBUG][RK0][tid #140244263364352]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][06:01:49.888][DEBUG][RK0][tid #140244254971648]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][06:01:49.946][DEBUG][RK0][tid #140239574136576]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][06:01:50.007][DEBUG][RK0][tid #140239565743872]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][06:01:50.065][DEBUG][RK0][tid #140239557351168]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][06:01:50.123][DEBUG][RK0][tid #140239305701120]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][06:01:50.181][DEBUG][RK0][tid #140239297308416]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][06:01:50.466][INFO][RK0][main]: Evaluation, AverageLoss: 0.141245\n",
      "[HCTR][06:01:50.466][INFO][RK0][main]: Eval Time for 70 iters: 4.07994s\n",
      "[HCTR][06:02:05.887][INFO][RK0][main]: Iter: 300 Time(100 iters): 19.487s Loss: 0.144382 lr:0.5\n",
      "[HCTR][06:02:09.368][DEBUG][RK0][tid #140244271757056]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][06:02:09.425][DEBUG][RK0][tid #140244263364352]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][06:02:09.484][DEBUG][RK0][tid #140244254971648]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][06:02:09.543][DEBUG][RK0][tid #140239574136576]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][06:02:09.601][DEBUG][RK0][tid #140239565743872]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][06:02:09.661][DEBUG][RK0][tid #140239557351168]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][06:02:09.722][DEBUG][RK0][tid #140239305701120]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][06:02:09.782][DEBUG][RK0][tid #140239297308416]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][06:02:09.968][INFO][RK0][main]: Evaluation, AverageLoss: 0.142796\n",
      "[HCTR][06:02:09.968][INFO][RK0][main]: Eval Time for 70 iters: 4.07618s\n",
      "[HCTR][06:02:25.375][INFO][RK0][main]: Iter: 400 Time(100 iters): 19.488s Loss: 0.140091 lr:0.5\n",
      "[HCTR][06:02:28.529][DEBUG][RK0][tid #140244271757056]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][06:02:28.645][DEBUG][RK0][tid #140244254971648]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][06:02:28.703][DEBUG][RK0][tid #140239574136576]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][06:02:28.937][DEBUG][RK0][tid #140239297308416]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][06:02:29.054][DEBUG][RK0][tid #140244263364352]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][06:02:29.230][DEBUG][RK0][tid #140239565743872]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][06:02:29.288][DEBUG][RK0][tid #140239557351168]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][06:02:29.347][DEBUG][RK0][tid #140239305701120]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][06:02:29.454][INFO][RK0][main]: Evaluation, AverageLoss: 0.13898\n",
      "[HCTR][06:02:29.454][INFO][RK0][main]: Eval Time for 70 iters: 4.07863s\n",
      "[HCTR][06:02:44.872][INFO][RK0][main]: Iter: 500 Time(100 iters): 19.4772s Loss: 0.140547 lr:0.5\n",
      "[HCTR][06:02:48.151][DEBUG][RK0][tid #140244271757056]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][06:02:48.209][DEBUG][RK0][tid #140244263364352]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][06:02:48.267][DEBUG][RK0][tid #140244254971648]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][06:02:48.326][DEBUG][RK0][tid #140239574136576]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][06:02:48.383][DEBUG][RK0][tid #140239565743872]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][06:02:48.442][DEBUG][RK0][tid #140239557351168]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][06:02:48.500][DEBUG][RK0][tid #140239305701120]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][06:02:48.558][DEBUG][RK0][tid #140239297308416]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][06:02:48.970][INFO][RK0][main]: Evaluation, AverageLoss: 0.138469\n",
      "[HCTR][06:02:48.970][INFO][RK0][main]: Eval Time for 70 iters: 4.09781s\n",
      "[HCTR][06:02:55.488][DEBUG][RK0][tid #140244531799808]: file_name_ deepfm_data_nvt/train/0.1738817c5c5c47dba75a428d0837cbc3.parquet file_total_rows_ 4586722\n",
      "[HCTR][06:02:55.638][DEBUG][RK0][tid #140244540192512]: file_name_ deepfm_data_nvt/train/1.c7b6f2423fec47ff97a09ec95f6346f9.parquet file_total_rows_ 4585117\n",
      "[HCTR][06:02:55.796][DEBUG][RK0][tid #140248860321536]: file_name_ deepfm_data_nvt/train/2.6b134d3f8f0a4f0d9453f1d7c08f74d5.parquet file_total_rows_ 4584304\n",
      "[HCTR][06:02:55.944][DEBUG][RK0][tid #140244674410240]: file_name_ deepfm_data_nvt/train/3.4b192542e2ad4cc8b745feb142d1878a.parquet file_total_rows_ 4581022\n",
      "[HCTR][06:02:56.101][DEBUG][RK0][tid #140244666017536]: file_name_ deepfm_data_nvt/train/4.4f7e95ed8f9b4bcc9b63c5f3278e6905.parquet file_total_rows_ 4580476\n",
      "[HCTR][06:02:56.258][DEBUG][RK0][tid #140244657624832]: file_name_ deepfm_data_nvt/train/5.c5b89db1e82d4842998d560796eab838.parquet file_total_rows_ 4583901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][06:02:56.411][DEBUG][RK0][tid #140244397582080]: file_name_ deepfm_data_nvt/train/6.92133f3ee3664684854969202958122f.parquet file_total_rows_ 4581782\n",
      "[HCTR][06:02:57.795][DEBUG][RK0][tid #140244389189376]: file_name_ deepfm_data_nvt/train/7.9345ade3421b40a5803f518c48ae436f.parquet file_total_rows_ 4589169\n",
      "[HCTR][06:03:04.377][INFO][RK0][main]: Iter: 600 Time(100 iters): 19.5042s Loss: 0.137026 lr:0.5\n",
      "[HCTR][06:03:07.770][DEBUG][RK0][tid #140244271757056]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][06:03:07.827][DEBUG][RK0][tid #140244263364352]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][06:03:07.885][DEBUG][RK0][tid #140244254971648]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][06:03:07.944][DEBUG][RK0][tid #140239574136576]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][06:03:08.007][DEBUG][RK0][tid #140239565743872]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][06:03:08.063][DEBUG][RK0][tid #140239557351168]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][06:03:08.124][DEBUG][RK0][tid #140239305701120]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][06:03:08.184][DEBUG][RK0][tid #140239297308416]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][06:03:08.480][INFO][RK0][main]: Evaluation, AverageLoss: 0.138509\n",
      "[HCTR][06:03:08.480][INFO][RK0][main]: Eval Time for 70 iters: 4.10372s\n",
      "[HCTR][06:03:23.905][INFO][RK0][main]: Iter: 700 Time(100 iters): 19.5107s Loss: 0.14024 lr:0.5\n",
      "[HCTR][06:03:27.410][DEBUG][RK0][tid #140244271757056]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][06:03:27.468][DEBUG][RK0][tid #140244263364352]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][06:03:27.527][DEBUG][RK0][tid #140244254971648]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][06:03:27.586][DEBUG][RK0][tid #140239574136576]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][06:03:27.644][DEBUG][RK0][tid #140239565743872]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][06:03:27.705][DEBUG][RK0][tid #140239557351168]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][06:03:27.766][DEBUG][RK0][tid #140239305701120]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][06:03:27.825][DEBUG][RK0][tid #140239297308416]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][06:03:28.002][INFO][RK0][main]: Evaluation, AverageLoss: 0.137946\n",
      "[HCTR][06:03:28.002][INFO][RK0][main]: Eval Time for 70 iters: 4.09707s\n",
      "[HCTR][06:03:43.402][INFO][RK0][main]: Iter: 800 Time(100 iters): 19.4966s Loss: 0.141149 lr:0.5\n",
      "[HCTR][06:03:46.564][DEBUG][RK0][tid #140244271757056]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][06:03:46.680][DEBUG][RK0][tid #140244254971648]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][06:03:46.738][DEBUG][RK0][tid #140239574136576]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][06:03:46.972][DEBUG][RK0][tid #140239297308416]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][06:03:47.089][DEBUG][RK0][tid #140244263364352]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][06:03:47.265][DEBUG][RK0][tid #140239565743872]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][06:03:47.323][DEBUG][RK0][tid #140239557351168]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][06:03:47.381][DEBUG][RK0][tid #140239305701120]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][06:03:47.490][INFO][RK0][main]: Evaluation, AverageLoss: 0.137143\n",
      "[HCTR][06:03:47.490][INFO][RK0][main]: Eval Time for 70 iters: 4.0881s\n",
      "[HCTR][06:04:02.895][INFO][RK0][main]: Iter: 900 Time(100 iters): 19.3418s Loss: 0.1369 lr:0.5\n",
      "[HCTR][06:04:06.158][DEBUG][RK0][tid #140244271757056]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][06:04:06.216][DEBUG][RK0][tid #140244263364352]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][06:04:06.274][DEBUG][RK0][tid #140244254971648]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][06:04:06.332][DEBUG][RK0][tid #140239574136576]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][06:04:06.391][DEBUG][RK0][tid #140239565743872]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][06:04:06.448][DEBUG][RK0][tid #140239557351168]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][06:04:06.507][DEBUG][RK0][tid #140239305701120]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][06:04:06.565][DEBUG][RK0][tid #140239297308416]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][06:04:06.976][INFO][RK0][main]: Evaluation, AverageLoss: 0.137767\n",
      "[HCTR][06:04:06.976][INFO][RK0][main]: Eval Time for 70 iters: 4.08132s\n",
      "[HCTR][06:04:22.249][INFO][RK0][main]: Finish 1000 iterations with batchsize: 65536 in 190.85s.\n"
     ]
    }
   ],
   "source": [
    "!python3 dlrm_train.py ./distributed_plan.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87071e6",
   "metadata": {},
   "source": [
    "### Compare performance between different ETPS\n",
    "We can see the iteration time for dataparallel + localized is 103.45s while for distributed is 190.85s, which means different ETPS can greatly affect the performance of embedding. So it's better to put embedding table as data parallel or localized when the table can be fitted into single GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
