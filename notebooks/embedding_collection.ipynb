{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924efc61",
   "metadata": {},
   "source": [
    "# Introduction to the HugeCTR Embedding Collection\n",
    "## About the HugeCTR embedding collection\n",
    "Embedding collection enables users to group multiple embedding lookup operations together, in which vector size / id space / optimizer / table placement strategy of embedding tables can be different so that both flexibility and performance will be enhanced. \n",
    "This notebook includes:\n",
    "1. Introduce the API of the embedding collection.\n",
    "2. Introduce Embedding Table Placement Strategy(ETPS) and how to configure ETPS in embedding collection.\n",
    "3. Use embedding collection in the DLRM model for Criteo dataset as an example to show how to use embedding collection for your model training and evaluation. We provide two different ETPS as reference.\n",
    "\n",
    "## API\n",
    "There are 2 API related with embedding collection:\n",
    "### `hugectr.EmbeddingTableConfig`\n",
    "A placeholder for users to configure the attribute of the embedding table.\n",
    "Parameter:\n",
    "* `id_space`: Integer, the id space this table belongs to. Typically, it is the existing number of EmbeddingTableConfig when you create a new embedding table. \n",
    "* `max_vocabulary_size`: Integer, the vocabulary size of this table. If positive, then the number means how many embedding vectors this table contains. And it will cause overflow if you exceed during training or evaluation.  If you do not know the exact size of the embedding table, you can specify -1 which means the dynamic embedding table will be used and its size can be extended dynamically during training or evaluation.\n",
    "* `ev_size`: Integer, the vector size of embedding this embedding consists of.\n",
    "* `min_key`: Integer, the minimum value of input key.\n",
    "* `max_key`: Integer, the maximum value of input key.\n",
    "* `opt_params`: Optional, hugectr.Optimizer, the optimizer you want to use for this embedding table. If not specified, will use the optimizer specified in `hugectr.Model`.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "# create embedding table\n",
    "num_embedding = 26\n",
    "table_size_array = [203931, 18598, 14092, 7012, 18977, 4, 6385, 1245, 49, 186213, 71328, 67288, 11, 2168, 7338, 61, 4, 932, 15, 204515, 141526, 199433, 60919, 9137, 71, 34]\n",
    "embedding_table_list = []\n",
    "for i in range(num_embedding):\n",
    "     embedding_table_list.append(hugectr.EmbeddingTableConfig(id_space=i, max_vocabulary_size=table_size_array[i], ev_size=128, min_key=0, max_key=table_size_array[i]))\n",
    "```\n",
    "\n",
    "### `hugectr.EmbeddingPlanner`\n",
    "`EmbeddingPlanner` provides `embedding_lookup` for users to specify lookup operations on `hugectr.EmbeddingTableConfig`. \n",
    "\n",
    "#### `embedding_lookup`\n",
    "Parameter:\n",
    "  * `emb_table` : hugectr.EmbeddingTableConfig, the embedding table you want to lookup upon.\n",
    "  * `input`: str, the input tensor name. Should be compatible with the `data_reader_sparse_param_array` in `hugectr.Input` in `hugectr.Model`\n",
    "  * `output`: str, the output tensor name. The shape of output tensor will be (batch_size, 1, embedding vector size).\n",
    "  * `combiner`: str, specify the combiner operation. Currently support `mean`, `sum` and `concat`.\n",
    "\n",
    "#### `create_embedding_collection`\n",
    "After finishing all `embedding_lookup`, users can use `EmbeddingPlanner.create_embedding_collection` to create `hugectr.EmbeddingCollection`, which can be added in `hugectr.Model` for training and evaluation.\n",
    "Parameter:\n",
    "  * `plan_file`: str, a json file which describes the table placement strategy. Will be covered in more detail in section `Plan and Embedding Table Placement Strategy`.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "embedding_planner = hugectr.EmbeddingPlanner()\n",
    "emb_vec_list = []\n",
    "for i in range(num_embedding):\n",
    "     embedding_planner.embedding_lookup(embedding_table_list[i], \"data{}\".format(i), \"emb_vec{}\".format(i), \"sum\")\n",
    "embedding_collection = embedding_planner.create_embedding_collection(\"./plan_7000.json\")\n",
    "```\n",
    "\n",
    "## Plan and Embedding Table Placement Strategy(ETPS)\n",
    "### What is ETPS and why is it important?\n",
    "In the recommendation system, the embedding table is usually so large that a single GPU is not able to hold all embedding tables, where sharding is needed to distribute embedding tables across multiple GPUs. We call such sharding strategy as **Embedding Table Placement Strategy**. It will hugely affect the performance of embedding, since different sharding strategies influence the communication between GPUs, and the optimal placement strategy is highly related with your dataset and lookup operation. So it's very important for users to configure a suitable table placement strategy for their own use case instead of providing a fixed one.   \n",
    "### How to configure ETPS in the embedding collection?\n",
    "We introduce a configurable ETPS interface so that users can adjust their table placement strategy according to their own use case. We use a json file to describe the ETPS in all GPUs, which we call a **plan file**. For example, consider you have 4 table and 5 lookup operations, which may like:\n",
    "```python\n",
    "num_embedding = 5\n",
    "table_size_array = [...]\n",
    "embedding_table_list = []\n",
    "for i in range(num_embedding):\n",
    "     embedding_table_list.append(hugectr.EmbeddingTableConfig(id_space=i, max_vocabulary_size=table_size_array[i], ev_size=128, min_key=0, max_key=table_size_array[i]))\n",
    "\n",
    "embedding_planner = hugectr.EmbeddingPlanner()\n",
    "embedding_planner.embedding_lookup(embedding_table_list[0], \"data0\", \"emb_vec0\", \"sum\") # lookup 0\n",
    "embedding_planner.embedding_lookup(embedding_table_list[1], \"data1\", \"emb_vec1\", \"sum\") # lookup 1\n",
    "embedding_planner.embedding_lookup(embedding_table_list[2], \"data2\", \"emb_vec2\", \"sum\") # lookup 2\n",
    "embedding_planner.embedding_lookup(embedding_table_list[1], \"data3\", \"emb_vec3\", \"sum\") # lookup 3\n",
    "embedding_planner.embedding_lookup(embedding_table_list[3], \"data4\", \"emb_vec4\", \"sum\") # lookup 4\n",
    "```\n",
    "Now you want to configure the ETPS through a plan file. In the plan file, you can group several lookup operations together and do sharding. You can specify which lookup operation / which GPU / which portion of the embedding table in a plan file. The basic principle is one embedding table can only be sharded in a single way. For example, lookup 0 and lookup 3 take place upon the same embedding table. So lookup 0 and lookup 3 should be grouped together and sharded in the same way. \n",
    "If you have 2 GPUs and you want to use data parallel in all 4 embedding tables, you can write plan file like:\n",
    "```json\n",
    "[\n",
    "  [\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              0, 1, 2, 3, 4\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  0, 1, 2, 3, 4\n",
    "              ],\n",
    "              [\n",
    "                  0, 1, 2, 3, 4\n",
    "              ]\n",
    "          ],\n",
    "          \"num_sharding\": 1,\n",
    "          \"sharding_id\": 0,\n",
    "          \"table_placement_strategy\": \"dp\"\n",
    "      }\n",
    "  ],\n",
    "  [\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              0, 1, 2, 3, 4\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  0, 1, 2, 3, 4\n",
    "              ],\n",
    "              [\n",
    "                  0, 1, 2, 3, 4\n",
    "              ]\n",
    "          ],\n",
    "          \"num_sharding\": 1,\n",
    "          \"sharding_id\": 0,\n",
    "          \"table_placement_strategy\": \"dp\"\n",
    "      }\n",
    "  ]\n",
    "]\n",
    "```\n",
    "The plan file consists of a list which describes the table placement strategy in each gpu orderly. In each gpu, we use a list to describe multiple groups of sharded lookup operations. Each group of sharded lookup operation is a dictionary which includes:\n",
    "* `local_embedding_list`: a list of integers, which lookup operations current gpu contains.\n",
    "* `global_embedding_list`: a list of lists of integers, the current group lookup operations in all gpus.\n",
    "* `num_sharding`: an integer, how many shards you want to shard the current group lookup operations.\n",
    "* `sharding_id`: an integer, the index of the current group lookup operations.\n",
    "* `table_placement_strategy`: str, can be `mp` or `dp`. `mp` means model parallel and `dp` means data parallel.\n",
    "\n",
    "You are allowed to apply more complex ways for ETPS. Let's say we want to shard lookup 0, 1, 2, 3 across GPUs while lookup 4 to be data parallel. We can use:\n",
    "```json\n",
    "[\n",
    "  [\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              0,\n",
    "              2\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  0,\n",
    "                  2\n",
    "              ],\n",
    "              [\n",
    "                  1,\n",
    "                  3\n",
    "              ]\n",
    "          ],\n",
    "          \"table_placement_strategy\": \"mp\"\n",
    "      },\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              4\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  4\n",
    "              ],\n",
    "              [\n",
    "                  4\n",
    "              ]\n",
    "          ],\n",
    "          \"table_placement_strategy\": \"dp\"\n",
    "      }\n",
    "  ],\n",
    "  [\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              1,\n",
    "              3\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  0,\n",
    "                  2\n",
    "              ],\n",
    "              [\n",
    "                  1,\n",
    "                  3\n",
    "              ]\n",
    "          ],\n",
    "          \"table_placement_strategy\": \"mp\"\n",
    "      },\n",
    "      {\n",
    "          \"local_embedding_list\": [\n",
    "              4\n",
    "          ],\n",
    "          \"global_embedding_list\": [\n",
    "              [\n",
    "                  4\n",
    "              ],\n",
    "              [\n",
    "                  4\n",
    "              ]\n",
    "          ],\n",
    "          \"table_placement_strategy\": \"dp\"\n",
    "      }\n",
    "  ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a8a29",
   "metadata": {},
   "source": [
    "## DLRM Model\n",
    "### Parepare Data\n",
    "You can follow the instruction under [samples/deepfm/README.md#Preprocess the Dataset Through NVTabular](../samples/deepfm/README.md#Preprocess_the_Dataset_Through_NVTabular) to prepare data.\n",
    "### Prepare Train Script\n",
    "We will use single DGX-1 to run DLRM in this notebook. The GPU info in DGX-1 is as follows. It consists of 8 V100-SXM2 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf29e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 23 00:14:56 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    42W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    45W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    44W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    42W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    44W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    42W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    44W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    41W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0996e7",
   "metadata": {},
   "source": [
    "We build our train script through embedding collection API. And we will use command argument to pass plan file and test different ETPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1ddb6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlrm_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlrm_train.py\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/workdir/build/lib')\n",
    "import hugectr\n",
    "\n",
    "plan_file = sys.argv[1]\n",
    "table_size_array = [203931, 18598, 14092, 7012, 18977, 4, 6385, 1245, 49, 186213, 71328, 67288, 11, 2168, 7338, 61, 4, 932, 15, 204515, 141526, 199433, 60919, 9137, 71, 34]\n",
    "\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 70,\n",
    "                              batchsize_eval = 65536,\n",
    "                              batchsize = 65536,\n",
    "                              lr = 0.5,\n",
    "                              warmup_steps = 300,\n",
    "                              vvgpu = [[0,1,2,3,4,5,6,7]],\n",
    "                              repeat_dataset = True,\n",
    "                              i64_input_key = True,\n",
    "                              metrics_spec = {hugectr.MetricsType.AverageLoss:0.0},\n",
    "                              use_embedding_collection = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                  source = [\"./deepfm_data_nvt/train/_file_list.txt\"],\n",
    "                                  eval_source = \"./deepfm_data_nvt/val/_file_list.txt\",\n",
    "                                  check_type=hugectr.Check_t.Non,\n",
    "                                  slot_size_array = table_size_array)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.SGD,\n",
    "                                    update_type = hugectr.Update_t.Local,\n",
    "                                    atomic_update = True)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "num_embedding = 26\n",
    "\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data{}\".format(i), 1, False, 1) for i in range(num_embedding)]))\n",
    "\n",
    "# create embedding table\n",
    "embedding_table_list = []\n",
    "for i in range(num_embedding):\n",
    "    embedding_table_list.append(hugectr.EmbeddingTableConfig(id_space=i, max_vocabulary_size=table_size_array[i], ev_size=128, min_key=0, max_key=table_size_array[i]))\n",
    "# create embedding planner and embedding collection\n",
    "embedding_planner = hugectr.EmbeddingPlanner()\n",
    "emb_vec_list = []\n",
    "for i in range(num_embedding):\n",
    "    embedding_planner.embedding_lookup(emb_table=embedding_table_list[i], \n",
    "                                       bottom_name=f\"data{i}\", \n",
    "                                       top_name=f\"emb_vec{i}\", \n",
    "                                       combiner=\"sum\")\n",
    "embedding_collection = embedding_planner.create_embedding_collection(plan_file)\n",
    "\n",
    "model.add(embedding_collection)\n",
    "# need concat\n",
    "model.add(hugectr.DenseLayer(layer_type=hugectr.Layer_t.Concat,\n",
    "                              bottom_names = [\"emb_vec{}\".format(i) for i in range(num_embedding)],\n",
    "                              top_names = [\"sparse_embedding1\"],\n",
    "                              axis = 1))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dense\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))                           \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=256))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))                            \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=128))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc3\"],\n",
    "                            top_names = [\"relu3\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Interaction, # interaction only support 3-D input\n",
    "                            bottom_names = [\"relu3\",\"sparse_embedding1\"],\n",
    "                            top_names = [\"interaction1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"interaction1\"],\n",
    "                            top_names = [\"fc4\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc4\"],\n",
    "                            top_names = [\"relu4\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu4\"],\n",
    "                            top_names = [\"fc5\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc5\"],\n",
    "                            top_names = [\"relu5\"]))                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu5\"],\n",
    "                            top_names = [\"fc6\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc6\"],\n",
    "                            top_names = [\"relu6\"]))                               \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu6\"],\n",
    "                            top_names = [\"fc7\"],\n",
    "                            num_output=256))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc7\"],\n",
    "                            top_names = [\"relu7\"]))                                                                              \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu7\"],\n",
    "                            top_names = [\"fc8\"],\n",
    "                            num_output=1))                                                                                           \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc8\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 1000, display = 100, eval_interval = 100, snapshot = 10000000, snapshot_prefix = \"dlrm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d6f22",
   "metadata": {},
   "source": [
    "### ETPS: Data parallel + Localized\n",
    "We want to put small size table as data parallel while for other tables, each table will be on single GPU and different GPU will hold different table(The same way we we use in `hugectr.LocalizedHashEmbedding`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f53e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plan(plan):\n",
    "    for id,single_gpu_plan in enumerate(plan):\n",
    "        print('single_gpu_plan index = {}'.format(id))\n",
    "        for plan_attr in single_gpu_plan:\n",
    "            for key in plan_attr:\n",
    "                if (key != 'global_embedding_list'):\n",
    "                    print('\\t{}:{}'.format(key,plan_attr[key]))\n",
    "                else:\n",
    "                    prefix_len = len(key)\n",
    "                    left_space_fill = ' '*prefix_len\n",
    "                    print('\\t{}:{}'.format(key,plan_attr[key][0]))\n",
    "                    for index in range(1,len(plan_attr[key])):\n",
    "                        print('\\t{}:{}'.format(left_space_fill,plan_attr[key][index]))\n",
    "def generate_plan(table_size_array, num_gpus, plan_file):\n",
    "    \n",
    "    mp_table = [i for i in range(len(table_size_array)) if table_size_array[i] > 6000]\n",
    "    dp_table = [i for i in range(len(table_size_array)) if table_size_array[i] <= 6000]\n",
    "\n",
    "    # place table across all gpus\n",
    "    plan = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        single_gpu_plan = []\n",
    "        mp_plan = {\n",
    "          'local_embedding_list': [table_id for i, table_id in enumerate(mp_table) if i % num_gpus == gpu_id],\n",
    "          'table_placement_strategy': 'mp'\n",
    "        }\n",
    "        dp_plan = {\n",
    "          'local_embedding_list': dp_table,\n",
    "          'table_placement_strategy': 'dp'\n",
    "        }\n",
    "        single_gpu_plan.append(mp_plan)\n",
    "        single_gpu_plan.append(dp_plan)\n",
    "        plan.append(single_gpu_plan)\n",
    "\n",
    "    # generate global view of table placement\n",
    "    mp_global_embedding_list = []\n",
    "    dp_global_embedding_list = []\n",
    "    for single_gpu_plan in plan:\n",
    "        mp_global_embedding_list.append(single_gpu_plan[0]['local_embedding_list'])\n",
    "        dp_global_embedding_list.append(single_gpu_plan[1]['local_embedding_list'])\n",
    "    for single_gpu_plan in plan:\n",
    "        single_gpu_plan[0]['global_embedding_list'] = mp_global_embedding_list\n",
    "        single_gpu_plan[1]['global_embedding_list'] = dp_global_embedding_list\n",
    "    print_plan(plan)\n",
    "    # dump plan file\n",
    "    import json\n",
    "    with open(plan_file, 'w') as f:\n",
    "        json.dump(plan, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bec0981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_gpu_plan index = 0\n",
      "\tlocal_embedding_list:[0, 11]\n",
      "\ttable_placement_strategy:mp\n",
      "\tglobal_embedding_list:[0, 11]\n",
      "\t                     :[1, 14]\n",
      "\t                     :[2, 19]\n",
      "\t                     :[3, 20]\n",
      "\t                     :[4, 21]\n",
      "\t                     :[6, 22]\n",
      "\t                     :[9, 23]\n",
      "\t                     :[10]\n",
      "\tlocal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\ttable_placement_strategy:dp\n",
      "\tglobal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "single_gpu_plan index = 1\n",
      "\tlocal_embedding_list:[1, 14]\n",
      "\ttable_placement_strategy:mp\n",
      "\tglobal_embedding_list:[0, 11]\n",
      "\t                     :[1, 14]\n",
      "\t                     :[2, 19]\n",
      "\t                     :[3, 20]\n",
      "\t                     :[4, 21]\n",
      "\t                     :[6, 22]\n",
      "\t                     :[9, 23]\n",
      "\t                     :[10]\n",
      "\tlocal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\ttable_placement_strategy:dp\n",
      "\tglobal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "single_gpu_plan index = 2\n",
      "\tlocal_embedding_list:[2, 19]\n",
      "\ttable_placement_strategy:mp\n",
      "\tglobal_embedding_list:[0, 11]\n",
      "\t                     :[1, 14]\n",
      "\t                     :[2, 19]\n",
      "\t                     :[3, 20]\n",
      "\t                     :[4, 21]\n",
      "\t                     :[6, 22]\n",
      "\t                     :[9, 23]\n",
      "\t                     :[10]\n",
      "\tlocal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\ttable_placement_strategy:dp\n",
      "\tglobal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "single_gpu_plan index = 3\n",
      "\tlocal_embedding_list:[3, 20]\n",
      "\ttable_placement_strategy:mp\n",
      "\tglobal_embedding_list:[0, 11]\n",
      "\t                     :[1, 14]\n",
      "\t                     :[2, 19]\n",
      "\t                     :[3, 20]\n",
      "\t                     :[4, 21]\n",
      "\t                     :[6, 22]\n",
      "\t                     :[9, 23]\n",
      "\t                     :[10]\n",
      "\tlocal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\ttable_placement_strategy:dp\n",
      "\tglobal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "single_gpu_plan index = 4\n",
      "\tlocal_embedding_list:[4, 21]\n",
      "\ttable_placement_strategy:mp\n",
      "\tglobal_embedding_list:[0, 11]\n",
      "\t                     :[1, 14]\n",
      "\t                     :[2, 19]\n",
      "\t                     :[3, 20]\n",
      "\t                     :[4, 21]\n",
      "\t                     :[6, 22]\n",
      "\t                     :[9, 23]\n",
      "\t                     :[10]\n",
      "\tlocal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\ttable_placement_strategy:dp\n",
      "\tglobal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "single_gpu_plan index = 5\n",
      "\tlocal_embedding_list:[6, 22]\n",
      "\ttable_placement_strategy:mp\n",
      "\tglobal_embedding_list:[0, 11]\n",
      "\t                     :[1, 14]\n",
      "\t                     :[2, 19]\n",
      "\t                     :[3, 20]\n",
      "\t                     :[4, 21]\n",
      "\t                     :[6, 22]\n",
      "\t                     :[9, 23]\n",
      "\t                     :[10]\n",
      "\tlocal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\ttable_placement_strategy:dp\n",
      "\tglobal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "single_gpu_plan index = 6\n",
      "\tlocal_embedding_list:[9, 23]\n",
      "\ttable_placement_strategy:mp\n",
      "\tglobal_embedding_list:[0, 11]\n",
      "\t                     :[1, 14]\n",
      "\t                     :[2, 19]\n",
      "\t                     :[3, 20]\n",
      "\t                     :[4, 21]\n",
      "\t                     :[6, 22]\n",
      "\t                     :[9, 23]\n",
      "\t                     :[10]\n",
      "\tlocal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\ttable_placement_strategy:dp\n",
      "\tglobal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "single_gpu_plan index = 7\n",
      "\tlocal_embedding_list:[10]\n",
      "\ttable_placement_strategy:mp\n",
      "\tglobal_embedding_list:[0, 11]\n",
      "\t                     :[1, 14]\n",
      "\t                     :[2, 19]\n",
      "\t                     :[3, 20]\n",
      "\t                     :[4, 21]\n",
      "\t                     :[6, 22]\n",
      "\t                     :[9, 23]\n",
      "\t                     :[10]\n",
      "\tlocal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\ttable_placement_strategy:dp\n",
      "\tglobal_embedding_list:[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n",
      "\t                     :[5, 7, 8, 12, 13, 15, 16, 17, 18, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "table_size_array = [203931, 18598, 14092, 7012, 18977, 4, 6385, 1245, 49, 186213, 71328, 67288, 11, 2168, 7338, 61, 4, 932, 15, 204515, 141526, 199433, 60919, 9137, 71, 34]\n",
    "generate_plan(table_size_array, 8, \"./dp_and_localized_plan.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24015e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 3.7\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][01:41:54.133][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][01:41:54.133][INFO][RK0][main]: Global seed is 1382527389\n",
      "[HCTR][01:41:54.593][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "  GPU 1 ->  node 0\n",
      "  GPU 2 ->  node 0\n",
      "  GPU 3 ->  node 0\n",
      "  GPU 4 ->  node 1\n",
      "  GPU 5 ->  node 1\n",
      "  GPU 6 ->  node 1\n",
      "  GPU 7 ->  node 1\n",
      "[HCTR][01:42:05.911][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][01:42:05.912][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][01:42:06.092][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][01:42:06.104][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][01:42:06.105][INFO][RK0][main]: Device 0: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:42:06.106][INFO][RK0][main]: Device 1: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:42:06.106][INFO][RK0][main]: Device 2: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:42:06.107][INFO][RK0][main]: Device 3: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:42:06.108][INFO][RK0][main]: Device 4: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:42:06.109][INFO][RK0][main]: Device 5: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:42:06.109][INFO][RK0][main]: Device 6: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:42:06.110][INFO][RK0][main]: Device 7: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:42:06.112][INFO][RK0][main]: num of DataReader workers: 8\n",
      "[HCTR][01:42:06.140][DEBUG][RK0][tid #140426254210816]: file_name_ deepfm_data_nvt/train/4.4f7e95ed8f9b4bcc9b63c5f3278e6905.parquet file_total_rows_ 4580476\n",
      "[HCTR][01:42:06.140][DEBUG][RK0][tid #140426128385792]: file_name_ deepfm_data_nvt/train/1.c7b6f2423fec47ff97a09ec95f6346f9.parquet file_total_rows_ 4585117\n",
      "[HCTR][01:42:06.140][DEBUG][RK0][tid #140430373013248]: file_name_ deepfm_data_nvt/train/2.6b134d3f8f0a4f0d9453f1d7c08f74d5.parquet file_total_rows_ 4584304\n",
      "[HCTR][01:42:06.142][DEBUG][RK0][tid #140426136778496]: file_name_ deepfm_data_nvt/train/5.c5b89db1e82d4842998d560796eab838.parquet file_total_rows_ 4583901\n",
      "[HCTR][01:42:06.142][DEBUG][RK0][tid #140426262603520]: file_name_ deepfm_data_nvt/train/3.4b192542e2ad4cc8b745feb142d1878a.parquet file_total_rows_ 4581022\n",
      "[HCTR][01:42:06.142][DEBUG][RK0][tid #140425583122176]: file_name_ deepfm_data_nvt/train/6.92133f3ee3664684854969202958122f.parquet file_total_rows_ 4581782\n",
      "[HCTR][01:42:06.142][DEBUG][RK0][tid #140426119993088]: file_name_ deepfm_data_nvt/train/0.1738817c5c5c47dba75a428d0837cbc3.parquet file_total_rows_ 4586722\n",
      "[HCTR][01:42:06.144][DEBUG][RK0][tid #140421372049152]: file_name_ deepfm_data_nvt/train/7.9345ade3421b40a5803f518c48ae436f.parquet file_total_rows_ 4589169\n",
      "[HCTR][01:42:06.145][INFO][RK0][main]: Vocabulary size: 1221286\n",
      "[HCTR][01:42:06.146][DEBUG][RK0][tid #140421363656448]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:42:06.146][DEBUG][RK0][tid #140421355263744]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:42:06.146][DEBUG][RK0][tid #140421237831424]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:42:06.146][DEBUG][RK0][tid #140421229438720]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:42:06.146][DEBUG][RK0][tid #140421221046016]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:42:06.146][DEBUG][RK0][tid #140421103613696]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:42:06.146][DEBUG][RK0][tid #140421095220992]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:42:06.147][DEBUG][RK0][tid #140421086828288]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:42:06.637][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][01:42:43.926][INFO][RK0][main]: label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data0,data1,data2,data3,data4,data5,data6,data7,data8,data9,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19,data20,data21,data22,data23,data24,data25\n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "EmbeddingCollection                     data0                         emb_vec0                      (None, 1, 128)                \n",
      "                                        data1                         emb_vec1                      (None, 1, 128)                \n",
      "                                        data2                         emb_vec2                      (None, 1, 128)                \n",
      "                                        data3                         emb_vec3                      (None, 1, 128)                \n",
      "                                        data4                         emb_vec4                      (None, 1, 128)                \n",
      "                                        data5                         emb_vec5                      (None, 1, 128)                \n",
      "                                        data6                         emb_vec6                      (None, 1, 128)                \n",
      "                                        data7                         emb_vec7                      (None, 1, 128)                \n",
      "                                        data8                         emb_vec8                      (None, 1, 128)                \n",
      "                                        data9                         emb_vec9                      (None, 1, 128)                \n",
      "                                        data10                        emb_vec10                     (None, 1, 128)                \n",
      "                                        data11                        emb_vec11                     (None, 1, 128)                \n",
      "                                        data12                        emb_vec12                     (None, 1, 128)                \n",
      "                                        data13                        emb_vec13                     (None, 1, 128)                \n",
      "                                        data14                        emb_vec14                     (None, 1, 128)                \n",
      "                                        data15                        emb_vec15                     (None, 1, 128)                \n",
      "                                        data16                        emb_vec16                     (None, 1, 128)                \n",
      "                                        data17                        emb_vec17                     (None, 1, 128)                \n",
      "                                        data18                        emb_vec18                     (None, 1, 128)                \n",
      "                                        data19                        emb_vec19                     (None, 1, 128)                \n",
      "                                        data20                        emb_vec20                     (None, 1, 128)                \n",
      "                                        data21                        emb_vec21                     (None, 1, 128)                \n",
      "                                        data22                        emb_vec22                     (None, 1, 128)                \n",
      "                                        data23                        emb_vec23                     (None, 1, 128)                \n",
      "                                        data24                        emb_vec24                     (None, 1, 128)                \n",
      "                                        data25                        emb_vec25                     (None, 1, 128)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  emb_vec0                      sparse_embedding1             (None, 26, 128)               \n",
      "                                        emb_vec1                                                                                  \n",
      "                                        emb_vec2                                                                                  \n",
      "                                        emb_vec3                                                                                  \n",
      "                                        emb_vec4                                                                                  \n",
      "                                        emb_vec5                                                                                  \n",
      "                                        emb_vec6                                                                                  \n",
      "                                        emb_vec7                                                                                  \n",
      "                                        emb_vec8                                                                                  \n",
      "                                        emb_vec9                                                                                  \n",
      "                                        emb_vec10                                                                                 \n",
      "                                        emb_vec11                                                                                 \n",
      "                                        emb_vec12                                                                                 \n",
      "                                        emb_vec13                                                                                 \n",
      "                                        emb_vec14                                                                                 \n",
      "                                        emb_vec15                                                                                 \n",
      "                                        emb_vec16                                                                                 \n",
      "                                        emb_vec17                                                                                 \n",
      "                                        emb_vec18                                                                                 \n",
      "                                        emb_vec19                                                                                 \n",
      "                                        emb_vec20                                                                                 \n",
      "                                        emb_vec21                                                                                 \n",
      "                                        emb_vec22                                                                                 \n",
      "                                        emb_vec23                                                                                 \n",
      "                                        emb_vec24                                                                                 \n",
      "                                        emb_vec25                                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dense                         fc1                           (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu1                         fc2                           (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu2                         fc3                           (None, 128)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc3                           relu3                         (None, 128)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Interaction                             relu3                         interaction1                  (None, 480)                   \n",
      "                                        sparse_embedding1                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            interaction1                  fc4                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc4                           relu4                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu4                         fc5                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc5                           relu5                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu5                         fc6                           (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc6                           relu6                         (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu6                         fc7                           (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc7                           relu7                         (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu7                         fc8                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  fc8                           loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][01:42:43.926][INFO][RK0][main]: Use non-epoch mode with number of iterations: 1000\n",
      "[HCTR][01:42:43.926][INFO][RK0][main]: Training batchsize: 65536, evaluation batchsize: 65536\n",
      "[HCTR][01:42:43.926][INFO][RK0][main]: Evaluation interval: 100, snapshot interval: 10000000\n",
      "[HCTR][01:42:43.926][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][01:42:43.926][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][01:42:43.926][INFO][RK0][main]: lr: 0.500000, warmup_steps: 300, end_lr: 0.000000\n",
      "[HCTR][01:42:43.926][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][01:42:43.926][INFO][RK0][main]: Training source file: ./deepfm_data_nvt/train/_file_list.txt\n",
      "[HCTR][01:42:43.926][INFO][RK0][main]: Evaluation source file: ./deepfm_data_nvt/val/_file_list.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][01:42:52.116][INFO][RK0][main]: Iter: 100 Time(100 iters): 8.11962s Loss: 0.14083 lr:0.168333\n",
      "[HCTR][01:42:54.073][DEBUG][RK0][tid #140421363656448]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:42:54.111][DEBUG][RK0][tid #140421355263744]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:42:54.149][DEBUG][RK0][tid #140421237831424]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:42:54.186][DEBUG][RK0][tid #140421229438720]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:42:54.229][DEBUG][RK0][tid #140421221046016]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:42:54.268][DEBUG][RK0][tid #140421103613696]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:42:54.301][DEBUG][RK0][tid #140421095220992]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:42:54.340][DEBUG][RK0][tid #140421086828288]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:42:54.577][INFO][RK0][main]: Evaluation, AverageLoss: 0.14194\n",
      "[HCTR][01:42:54.577][INFO][RK0][main]: Eval Time for 70 iters: 2.46099s\n",
      "[HCTR][01:43:02.681][INFO][RK0][main]: Iter: 200 Time(100 iters): 10.4967s Loss: 0.142118 lr:0.335\n",
      "[HCTR][01:43:04.739][DEBUG][RK0][tid #140421363656448]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:43:04.777][DEBUG][RK0][tid #140421355263744]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:43:04.815][DEBUG][RK0][tid #140421237831424]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:43:04.857][DEBUG][RK0][tid #140421229438720]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:43:04.901][DEBUG][RK0][tid #140421221046016]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:43:04.942][DEBUG][RK0][tid #140421103613696]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:43:04.975][DEBUG][RK0][tid #140421095220992]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:43:05.020][DEBUG][RK0][tid #140421086828288]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:43:05.197][INFO][RK0][main]: Evaluation, AverageLoss: 0.141608\n",
      "[HCTR][01:43:05.197][INFO][RK0][main]: Eval Time for 70 iters: 2.516s\n",
      "[HCTR][01:43:13.322][INFO][RK0][main]: Iter: 300 Time(100 iters): 10.5711s Loss: 0.143727 lr:0.5\n",
      "[HCTR][01:43:15.451][DEBUG][RK0][tid #140421363656448]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:43:15.489][DEBUG][RK0][tid #140421355263744]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:43:15.532][DEBUG][RK0][tid #140421237831424]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:43:15.570][DEBUG][RK0][tid #140421229438720]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:43:15.608][DEBUG][RK0][tid #140421221046016]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:43:15.650][DEBUG][RK0][tid #140421103613696]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:43:15.682][DEBUG][RK0][tid #140421095220992]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:43:15.724][DEBUG][RK0][tid #140421086828288]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:43:15.833][INFO][RK0][main]: Evaluation, AverageLoss: 0.140967\n",
      "[HCTR][01:43:15.833][INFO][RK0][main]: Eval Time for 70 iters: 2.50662s\n",
      "[HCTR][01:43:23.954][INFO][RK0][main]: Iter: 400 Time(100 iters): 10.5672s Loss: 0.141495 lr:0.5\n",
      "[HCTR][01:43:25.902][DEBUG][RK0][tid #140421363656448]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:43:25.974][DEBUG][RK0][tid #140421237831424]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:43:26.011][DEBUG][RK0][tid #140421229438720]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:43:26.156][DEBUG][RK0][tid #140421086828288]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:43:26.231][DEBUG][RK0][tid #140421355263744]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:43:26.336][DEBUG][RK0][tid #140421221046016]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:43:26.373][DEBUG][RK0][tid #140421103613696]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:43:26.411][DEBUG][RK0][tid #140421095220992]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:43:26.489][INFO][RK0][main]: Evaluation, AverageLoss: 0.140099\n",
      "[HCTR][01:43:26.489][INFO][RK0][main]: Eval Time for 70 iters: 2.53481s\n",
      "[HCTR][01:43:34.607][INFO][RK0][main]: Iter: 500 Time(100 iters): 10.6508s Loss: 0.142044 lr:0.5\n",
      "[HCTR][01:43:36.591][DEBUG][RK0][tid #140421363656448]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:43:36.628][DEBUG][RK0][tid #140421355263744]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:43:36.671][DEBUG][RK0][tid #140421237831424]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:43:36.708][DEBUG][RK0][tid #140421229438720]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:43:36.746][DEBUG][RK0][tid #140421221046016]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:43:36.783][DEBUG][RK0][tid #140421103613696]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:43:36.826][DEBUG][RK0][tid #140421095220992]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:43:36.864][DEBUG][RK0][tid #140421086828288]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:43:37.111][INFO][RK0][main]: Evaluation, AverageLoss: 0.140368\n",
      "[HCTR][01:43:37.111][INFO][RK0][main]: Eval Time for 70 iters: 2.50361s\n",
      "[HCTR][01:43:40.545][DEBUG][RK0][tid #140426119993088]: file_name_ deepfm_data_nvt/train/0.1738817c5c5c47dba75a428d0837cbc3.parquet file_total_rows_ 4586722\n",
      "[HCTR][01:43:40.625][DEBUG][RK0][tid #140426128385792]: file_name_ deepfm_data_nvt/train/1.c7b6f2423fec47ff97a09ec95f6346f9.parquet file_total_rows_ 4585117\n",
      "[HCTR][01:43:40.705][DEBUG][RK0][tid #140430373013248]: file_name_ deepfm_data_nvt/train/2.6b134d3f8f0a4f0d9453f1d7c08f74d5.parquet file_total_rows_ 4584304\n",
      "[HCTR][01:43:40.786][DEBUG][RK0][tid #140426262603520]: file_name_ deepfm_data_nvt/train/3.4b192542e2ad4cc8b745feb142d1878a.parquet file_total_rows_ 4581022\n",
      "[HCTR][01:43:40.866][DEBUG][RK0][tid #140426254210816]: file_name_ deepfm_data_nvt/train/4.4f7e95ed8f9b4bcc9b63c5f3278e6905.parquet file_total_rows_ 4580476\n",
      "[HCTR][01:43:40.960][DEBUG][RK0][tid #140426136778496]: file_name_ deepfm_data_nvt/train/5.c5b89db1e82d4842998d560796eab838.parquet file_total_rows_ 4583901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][01:43:41.042][DEBUG][RK0][tid #140425583122176]: file_name_ deepfm_data_nvt/train/6.92133f3ee3664684854969202958122f.parquet file_total_rows_ 4581782\n",
      "[HCTR][01:43:41.759][DEBUG][RK0][tid #140421372049152]: file_name_ deepfm_data_nvt/train/7.9345ade3421b40a5803f518c48ae436f.parquet file_total_rows_ 4589169\n",
      "[HCTR][01:43:45.226][INFO][RK0][main]: Iter: 600 Time(100 iters): 10.5546s Loss: 0.138196 lr:0.5\n",
      "[HCTR][01:43:47.271][DEBUG][RK0][tid #140421363656448]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:43:47.309][DEBUG][RK0][tid #140421355263744]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:43:47.347][DEBUG][RK0][tid #140421237831424]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:43:47.390][DEBUG][RK0][tid #140421229438720]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:43:47.427][DEBUG][RK0][tid #140421221046016]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:43:47.470][DEBUG][RK0][tid #140421103613696]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:43:47.513][DEBUG][RK0][tid #140421095220992]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:43:47.557][DEBUG][RK0][tid #140421086828288]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:43:47.736][INFO][RK0][main]: Evaluation, AverageLoss: 0.14019\n",
      "[HCTR][01:43:47.736][INFO][RK0][main]: Eval Time for 70 iters: 2.50955s\n",
      "[HCTR][01:43:55.870][INFO][RK0][main]: Iter: 700 Time(100 iters): 10.642s Loss: 0.142398 lr:0.5\n",
      "[HCTR][01:43:57.989][DEBUG][RK0][tid #140421363656448]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:43:58.027][DEBUG][RK0][tid #140421355263744]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:43:58.065][DEBUG][RK0][tid #140421237831424]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:43:58.108][DEBUG][RK0][tid #140421229438720]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:43:58.146][DEBUG][RK0][tid #140421221046016]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:43:58.185][DEBUG][RK0][tid #140421103613696]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:43:58.222][DEBUG][RK0][tid #140421095220992]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:43:58.256][DEBUG][RK0][tid #140421086828288]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:43:58.359][INFO][RK0][main]: Evaluation, AverageLoss: 0.140128\n",
      "[HCTR][01:43:58.359][INFO][RK0][main]: Eval Time for 70 iters: 2.48947s\n",
      "[HCTR][01:44:06.520][INFO][RK0][main]: Iter: 800 Time(100 iters): 10.5812s Loss: 0.143292 lr:0.5\n",
      "[HCTR][01:44:08.461][DEBUG][RK0][tid #140421363656448]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:44:08.534][DEBUG][RK0][tid #140421237831424]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:44:08.571][DEBUG][RK0][tid #140421229438720]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:44:08.713][DEBUG][RK0][tid #140421086828288]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:44:08.786][DEBUG][RK0][tid #140421355263744]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:44:08.894][DEBUG][RK0][tid #140421221046016]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:44:08.931][DEBUG][RK0][tid #140421103613696]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:44:08.970][DEBUG][RK0][tid #140421095220992]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:44:09.048][INFO][RK0][main]: Evaluation, AverageLoss: 0.13925\n",
      "[HCTR][01:44:09.048][INFO][RK0][main]: Eval Time for 70 iters: 2.52788s\n",
      "[HCTR][01:44:17.148][INFO][RK0][main]: Iter: 900 Time(100 iters): 10.5585s Loss: 0.139011 lr:0.5\n",
      "[HCTR][01:44:19.107][DEBUG][RK0][tid #140421363656448]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:44:19.146][DEBUG][RK0][tid #140421355263744]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:44:19.183][DEBUG][RK0][tid #140421237831424]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:44:19.225][DEBUG][RK0][tid #140421229438720]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:44:19.268][DEBUG][RK0][tid #140421221046016]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:44:19.309][DEBUG][RK0][tid #140421103613696]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:44:19.343][DEBUG][RK0][tid #140421095220992]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:44:19.383][DEBUG][RK0][tid #140421086828288]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:44:19.623][INFO][RK0][main]: Evaluation, AverageLoss: 0.139763\n",
      "[HCTR][01:44:19.623][INFO][RK0][main]: Eval Time for 70 iters: 2.47443s\n",
      "[HCTR][01:44:27.601][INFO][RK0][main]: Finish 1000 iterations with batchsize: 65536 in 103.67s.\n"
     ]
    }
   ],
   "source": [
    "!python3 dlrm_train.py ./dp_and_localized_plan.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ddef90",
   "metadata": {},
   "source": [
    "### ETPS: Distributed \n",
    "We want to distributed all tables across all gpus(the same way we use in  `hugectr.DistributedHashEmbedding`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a64116e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distributed_plan(table_size_array, num_gpus, plan_file):\n",
    "    # place table across all gpus\n",
    "    plan = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        distributed_plan = {\n",
    "          'local_embedding_list': [table_id for table_id in range(len(table_size_array))],\n",
    "          'table_placement_strategy': 'mp',\n",
    "          'sharding_id': gpu_id,\n",
    "          'num_sharding': num_gpus\n",
    "        }\n",
    "        plan.append([distributed_plan])\n",
    "\n",
    "    # generate global view of table placement\n",
    "    distributed_global_embedding_list = []\n",
    "    for single_gpu_plan in plan:\n",
    "        distributed_global_embedding_list.append(single_gpu_plan[0]['local_embedding_list'])\n",
    "    for single_gpu_plan in plan:\n",
    "        single_gpu_plan[0]['global_embedding_list'] = distributed_global_embedding_list\n",
    "    print_plan(plan)\n",
    "    # dump plan file\n",
    "    import json\n",
    "    with open(plan_file, 'w') as f:\n",
    "        json.dump(plan, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f572c884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_gpu_plan index = 0\n",
      "\tlocal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\ttable_placement_strategy:mp\n",
      "\tsharding_id:0\n",
      "\tnum_sharding:8\n",
      "\tglobal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "single_gpu_plan index = 1\n",
      "\tlocal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\ttable_placement_strategy:mp\n",
      "\tsharding_id:1\n",
      "\tnum_sharding:8\n",
      "\tglobal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "single_gpu_plan index = 2\n",
      "\tlocal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\ttable_placement_strategy:mp\n",
      "\tsharding_id:2\n",
      "\tnum_sharding:8\n",
      "\tglobal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "single_gpu_plan index = 3\n",
      "\tlocal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\ttable_placement_strategy:mp\n",
      "\tsharding_id:3\n",
      "\tnum_sharding:8\n",
      "\tglobal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "single_gpu_plan index = 4\n",
      "\tlocal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\ttable_placement_strategy:mp\n",
      "\tsharding_id:4\n",
      "\tnum_sharding:8\n",
      "\tglobal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "single_gpu_plan index = 5\n",
      "\tlocal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\ttable_placement_strategy:mp\n",
      "\tsharding_id:5\n",
      "\tnum_sharding:8\n",
      "\tglobal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "single_gpu_plan index = 6\n",
      "\tlocal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\ttable_placement_strategy:mp\n",
      "\tsharding_id:6\n",
      "\tnum_sharding:8\n",
      "\tglobal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "single_gpu_plan index = 7\n",
      "\tlocal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\ttable_placement_strategy:mp\n",
      "\tsharding_id:7\n",
      "\tnum_sharding:8\n",
      "\tglobal_embedding_list:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\t                     :[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "table_size_array = [203931, 18598, 14092, 7012, 18977, 4, 6385, 1245, 49, 186213, 71328, 67288, 11, 2168, 7338, 61, 4, 932, 15, 204515, 141526, 199433, 60919, 9137, 71, 34]\n",
    "generate_distributed_plan(table_size_array, 8, \"./distributed_plan.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d06fbf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 3.7\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][01:52:30.368][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][01:52:30.368][INFO][RK0][main]: Global seed is 3880200297\n",
      "[HCTR][01:52:30.827][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "  GPU 1 ->  node 0\n",
      "  GPU 2 ->  node 0\n",
      "  GPU 3 ->  node 0\n",
      "  GPU 4 ->  node 1\n",
      "  GPU 5 ->  node 1\n",
      "  GPU 6 ->  node 1\n",
      "  GPU 7 ->  node 1\n",
      "[HCTR][01:52:42.749][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][01:52:42.751][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][01:52:42.935][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][01:52:42.947][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][01:52:42.948][INFO][RK0][main]: Device 0: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:52:42.949][INFO][RK0][main]: Device 1: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:52:42.950][INFO][RK0][main]: Device 2: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:52:42.951][INFO][RK0][main]: Device 3: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:52:42.951][INFO][RK0][main]: Device 4: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:52:42.952][INFO][RK0][main]: Device 5: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:52:42.953][INFO][RK0][main]: Device 6: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:52:42.954][INFO][RK0][main]: Device 7: Tesla V100-SXM2-16GB\n",
      "[HCTR][01:52:42.955][INFO][RK0][main]: num of DataReader workers: 8\n",
      "[HCTR][01:52:42.983][DEBUG][RK0][tid #139892067653376]: file_name_ deepfm_data_nvt/train/0.1738817c5c5c47dba75a428d0837cbc3.parquet file_total_rows_ 4586722\n",
      "[HCTR][01:52:42.983][DEBUG][RK0][tid #139892197672704]: file_name_ deepfm_data_nvt/train/4.4f7e95ed8f9b4bcc9b63c5f3278e6905.parquet file_total_rows_ 4580476\n",
      "[HCTR][01:52:42.983][DEBUG][RK0][tid #139892076046080]: file_name_ deepfm_data_nvt/train/1.c7b6f2423fec47ff97a09ec95f6346f9.parquet file_total_rows_ 4585117\n",
      "[HCTR][01:52:42.985][DEBUG][RK0][tid #139896429729536]: file_name_ deepfm_data_nvt/train/2.6b134d3f8f0a4f0d9453f1d7c08f74d5.parquet file_total_rows_ 4584304\n",
      "[HCTR][01:52:42.985][DEBUG][RK0][tid #139892084438784]: file_name_ deepfm_data_nvt/train/5.c5b89db1e82d4842998d560796eab838.parquet file_total_rows_ 4583901\n",
      "[HCTR][01:52:42.985][DEBUG][RK0][tid #139891816003328]: file_name_ deepfm_data_nvt/train/7.9345ade3421b40a5803f518c48ae436f.parquet file_total_rows_ 4589169\n",
      "[HCTR][01:52:42.985][DEBUG][RK0][tid #139891933435648]: file_name_ deepfm_data_nvt/train/6.92133f3ee3664684854969202958122f.parquet file_total_rows_ 4581782\n",
      "[HCTR][01:52:42.987][DEBUG][RK0][tid #139892206065408]: file_name_ deepfm_data_nvt/train/3.4b192542e2ad4cc8b745feb142d1878a.parquet file_total_rows_ 4581022\n",
      "[HCTR][01:52:43.026][INFO][RK0][main]: Vocabulary size: 1221286\n",
      "[HCTR][01:52:43.027][DEBUG][RK0][tid #139891665000192]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:52:43.027][DEBUG][RK0][tid #139891144914688]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:52:43.027][DEBUG][RK0][tid #139891136521984]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:52:43.027][DEBUG][RK0][tid #139891128129280]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:52:43.028][DEBUG][RK0][tid #139891807610624]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:52:43.030][DEBUG][RK0][tid #139891799217920]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:52:43.030][DEBUG][RK0][tid #139891681785600]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:52:43.031][DEBUG][RK0][tid #139891673392896]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:52:43.620][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][01:53:20.900][INFO][RK0][main]: label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data0,data1,data2,data3,data4,data5,data6,data7,data8,data9,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19,data20,data21,data22,data23,data24,data25\n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "EmbeddingCollection                     data0                         emb_vec0                      (None, 1, 128)                \n",
      "                                        data1                         emb_vec1                      (None, 1, 128)                \n",
      "                                        data2                         emb_vec2                      (None, 1, 128)                \n",
      "                                        data3                         emb_vec3                      (None, 1, 128)                \n",
      "                                        data4                         emb_vec4                      (None, 1, 128)                \n",
      "                                        data5                         emb_vec5                      (None, 1, 128)                \n",
      "                                        data6                         emb_vec6                      (None, 1, 128)                \n",
      "                                        data7                         emb_vec7                      (None, 1, 128)                \n",
      "                                        data8                         emb_vec8                      (None, 1, 128)                \n",
      "                                        data9                         emb_vec9                      (None, 1, 128)                \n",
      "                                        data10                        emb_vec10                     (None, 1, 128)                \n",
      "                                        data11                        emb_vec11                     (None, 1, 128)                \n",
      "                                        data12                        emb_vec12                     (None, 1, 128)                \n",
      "                                        data13                        emb_vec13                     (None, 1, 128)                \n",
      "                                        data14                        emb_vec14                     (None, 1, 128)                \n",
      "                                        data15                        emb_vec15                     (None, 1, 128)                \n",
      "                                        data16                        emb_vec16                     (None, 1, 128)                \n",
      "                                        data17                        emb_vec17                     (None, 1, 128)                \n",
      "                                        data18                        emb_vec18                     (None, 1, 128)                \n",
      "                                        data19                        emb_vec19                     (None, 1, 128)                \n",
      "                                        data20                        emb_vec20                     (None, 1, 128)                \n",
      "                                        data21                        emb_vec21                     (None, 1, 128)                \n",
      "                                        data22                        emb_vec22                     (None, 1, 128)                \n",
      "                                        data23                        emb_vec23                     (None, 1, 128)                \n",
      "                                        data24                        emb_vec24                     (None, 1, 128)                \n",
      "                                        data25                        emb_vec25                     (None, 1, 128)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  emb_vec0                      sparse_embedding1             (None, 26, 128)               \n",
      "                                        emb_vec1                                                                                  \n",
      "                                        emb_vec2                                                                                  \n",
      "                                        emb_vec3                                                                                  \n",
      "                                        emb_vec4                                                                                  \n",
      "                                        emb_vec5                                                                                  \n",
      "                                        emb_vec6                                                                                  \n",
      "                                        emb_vec7                                                                                  \n",
      "                                        emb_vec8                                                                                  \n",
      "                                        emb_vec9                                                                                  \n",
      "                                        emb_vec10                                                                                 \n",
      "                                        emb_vec11                                                                                 \n",
      "                                        emb_vec12                                                                                 \n",
      "                                        emb_vec13                                                                                 \n",
      "                                        emb_vec14                                                                                 \n",
      "                                        emb_vec15                                                                                 \n",
      "                                        emb_vec16                                                                                 \n",
      "                                        emb_vec17                                                                                 \n",
      "                                        emb_vec18                                                                                 \n",
      "                                        emb_vec19                                                                                 \n",
      "                                        emb_vec20                                                                                 \n",
      "                                        emb_vec21                                                                                 \n",
      "                                        emb_vec22                                                                                 \n",
      "                                        emb_vec23                                                                                 \n",
      "                                        emb_vec24                                                                                 \n",
      "                                        emb_vec25                                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dense                         fc1                           (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu1                         fc2                           (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu2                         fc3                           (None, 128)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc3                           relu3                         (None, 128)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Interaction                             relu3                         interaction1                  (None, 480)                   \n",
      "                                        sparse_embedding1                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            interaction1                  fc4                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc4                           relu4                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu4                         fc5                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc5                           relu5                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu5                         fc6                           (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc6                           relu6                         (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu6                         fc7                           (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc7                           relu7                         (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu7                         fc8                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  fc8                           loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][01:53:20.900][INFO][RK0][main]: Use non-epoch mode with number of iterations: 1000\n",
      "[HCTR][01:53:20.900][INFO][RK0][main]: Training batchsize: 65536, evaluation batchsize: 65536\n",
      "[HCTR][01:53:20.900][INFO][RK0][main]: Evaluation interval: 100, snapshot interval: 10000000\n",
      "[HCTR][01:53:20.900][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][01:53:20.900][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][01:53:20.900][INFO][RK0][main]: lr: 0.500000, warmup_steps: 300, end_lr: 0.000000\n",
      "[HCTR][01:53:20.900][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][01:53:20.900][INFO][RK0][main]: Training source file: ./deepfm_data_nvt/train/_file_list.txt\n",
      "[HCTR][01:53:20.900][INFO][RK0][main]: Evaluation source file: ./deepfm_data_nvt/val/_file_list.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][01:53:36.384][INFO][RK0][main]: Iter: 100 Time(100 iters): 15.3325s Loss: 0.143876 lr:0.168333\n",
      "[HCTR][01:53:39.665][DEBUG][RK0][tid #139891807610624]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:53:39.723][DEBUG][RK0][tid #139891799217920]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:53:39.781][DEBUG][RK0][tid #139891681785600]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:53:39.839][DEBUG][RK0][tid #139891673392896]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:53:39.899][DEBUG][RK0][tid #139891665000192]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:53:39.957][DEBUG][RK0][tid #139891144914688]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:53:40.015][DEBUG][RK0][tid #139891136521984]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:53:40.072][DEBUG][RK0][tid #139891128129280]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:53:40.474][INFO][RK0][main]: Evaluation, AverageLoss: 0.144542\n",
      "[HCTR][01:53:40.474][INFO][RK0][main]: Eval Time for 70 iters: 4.08951s\n",
      "[HCTR][01:53:55.902][INFO][RK0][main]: Iter: 200 Time(100 iters): 19.5175s Loss: 0.142737 lr:0.335\n",
      "[HCTR][01:53:59.310][DEBUG][RK0][tid #139891807610624]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:53:59.368][DEBUG][RK0][tid #139891799217920]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:53:59.426][DEBUG][RK0][tid #139891681785600]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:53:59.485][DEBUG][RK0][tid #139891673392896]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:53:59.543][DEBUG][RK0][tid #139891665000192]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:53:59.601][DEBUG][RK0][tid #139891144914688]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:53:59.662][DEBUG][RK0][tid #139891136521984]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:53:59.720][DEBUG][RK0][tid #139891128129280]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:54:00.068][INFO][RK0][main]: Evaluation, AverageLoss: 0.14205\n",
      "[HCTR][01:54:00.068][INFO][RK0][main]: Eval Time for 70 iters: 4.16626s\n",
      "[HCTR][01:54:15.452][INFO][RK0][main]: Iter: 300 Time(100 iters): 19.5502s Loss: 0.143411 lr:0.5\n",
      "[HCTR][01:54:18.958][DEBUG][RK0][tid #139891807610624]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:54:19.016][DEBUG][RK0][tid #139891799217920]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:54:19.074][DEBUG][RK0][tid #139891681785600]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:54:19.133][DEBUG][RK0][tid #139891673392896]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:54:19.192][DEBUG][RK0][tid #139891665000192]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:54:19.250][DEBUG][RK0][tid #139891144914688]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:54:19.309][DEBUG][RK0][tid #139891136521984]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:54:19.367][DEBUG][RK0][tid #139891128129280]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:54:19.533][INFO][RK0][main]: Evaluation, AverageLoss: 0.1409\n",
      "[HCTR][01:54:19.533][INFO][RK0][main]: Eval Time for 70 iters: 4.07662s\n",
      "[HCTR][01:54:34.957][INFO][RK0][main]: Iter: 400 Time(100 iters): 19.5044s Loss: 0.140436 lr:0.5\n",
      "[HCTR][01:54:38.116][DEBUG][RK0][tid #139891807610624]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:54:38.232][DEBUG][RK0][tid #139891681785600]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:54:38.291][DEBUG][RK0][tid #139891673392896]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:54:38.527][DEBUG][RK0][tid #139891128129280]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:54:38.643][DEBUG][RK0][tid #139891799217920]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:54:38.818][DEBUG][RK0][tid #139891665000192]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:54:38.876][DEBUG][RK0][tid #139891144914688]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:54:38.934][DEBUG][RK0][tid #139891136521984]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:54:39.045][INFO][RK0][main]: Evaluation, AverageLoss: 0.139204\n",
      "[HCTR][01:54:39.045][INFO][RK0][main]: Eval Time for 70 iters: 4.08741s\n",
      "[HCTR][01:54:54.477][INFO][RK0][main]: Iter: 500 Time(100 iters): 19.5201s Loss: 0.14102 lr:0.5\n",
      "[HCTR][01:54:57.792][DEBUG][RK0][tid #139891807610624]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:54:57.849][DEBUG][RK0][tid #139891799217920]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:54:57.907][DEBUG][RK0][tid #139891681785600]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:54:57.967][DEBUG][RK0][tid #139891673392896]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:54:58.026][DEBUG][RK0][tid #139891665000192]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:54:58.084][DEBUG][RK0][tid #139891144914688]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:54:58.145][DEBUG][RK0][tid #139891136521984]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:54:58.203][DEBUG][RK0][tid #139891128129280]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:54:58.619][INFO][RK0][main]: Evaluation, AverageLoss: 0.138585\n",
      "[HCTR][01:54:58.619][INFO][RK0][main]: Eval Time for 70 iters: 4.13754s\n",
      "[HCTR][01:55:05.143][DEBUG][RK0][tid #139892067653376]: file_name_ deepfm_data_nvt/train/0.1738817c5c5c47dba75a428d0837cbc3.parquet file_total_rows_ 4586722\n",
      "[HCTR][01:55:05.290][DEBUG][RK0][tid #139892076046080]: file_name_ deepfm_data_nvt/train/1.c7b6f2423fec47ff97a09ec95f6346f9.parquet file_total_rows_ 4585117\n",
      "[HCTR][01:55:05.446][DEBUG][RK0][tid #139896429729536]: file_name_ deepfm_data_nvt/train/2.6b134d3f8f0a4f0d9453f1d7c08f74d5.parquet file_total_rows_ 4584304\n",
      "[HCTR][01:55:05.595][DEBUG][RK0][tid #139892206065408]: file_name_ deepfm_data_nvt/train/3.4b192542e2ad4cc8b745feb142d1878a.parquet file_total_rows_ 4581022\n",
      "[HCTR][01:55:05.753][DEBUG][RK0][tid #139892197672704]: file_name_ deepfm_data_nvt/train/4.4f7e95ed8f9b4bcc9b63c5f3278e6905.parquet file_total_rows_ 4580476\n",
      "[HCTR][01:55:05.907][DEBUG][RK0][tid #139892084438784]: file_name_ deepfm_data_nvt/train/5.c5b89db1e82d4842998d560796eab838.parquet file_total_rows_ 4583901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][01:55:06.057][DEBUG][RK0][tid #139891933435648]: file_name_ deepfm_data_nvt/train/6.92133f3ee3664684854969202958122f.parquet file_total_rows_ 4581782\n",
      "[HCTR][01:55:07.441][DEBUG][RK0][tid #139891816003328]: file_name_ deepfm_data_nvt/train/7.9345ade3421b40a5803f518c48ae436f.parquet file_total_rows_ 4589169\n",
      "[HCTR][01:55:14.044][INFO][RK0][main]: Iter: 600 Time(100 iters): 19.5667s Loss: 0.135635 lr:0.5\n",
      "[HCTR][01:55:17.437][DEBUG][RK0][tid #139891807610624]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:55:17.495][DEBUG][RK0][tid #139891799217920]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:55:17.553][DEBUG][RK0][tid #139891681785600]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:55:17.612][DEBUG][RK0][tid #139891673392896]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:55:17.670][DEBUG][RK0][tid #139891665000192]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:55:17.728][DEBUG][RK0][tid #139891144914688]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:55:17.787][DEBUG][RK0][tid #139891136521984]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:55:17.845][DEBUG][RK0][tid #139891128129280]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:55:18.133][INFO][RK0][main]: Evaluation, AverageLoss: 0.137923\n",
      "[HCTR][01:55:18.133][INFO][RK0][main]: Eval Time for 70 iters: 4.08857s\n",
      "[HCTR][01:55:33.562][INFO][RK0][main]: Iter: 700 Time(100 iters): 19.5034s Loss: 0.140273 lr:0.5\n",
      "[HCTR][01:55:37.080][DEBUG][RK0][tid #139891807610624]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:55:37.139][DEBUG][RK0][tid #139891799217920]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:55:37.197][DEBUG][RK0][tid #139891681785600]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:55:37.256][DEBUG][RK0][tid #139891673392896]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:55:37.314][DEBUG][RK0][tid #139891665000192]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:55:37.373][DEBUG][RK0][tid #139891144914688]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:55:37.431][DEBUG][RK0][tid #139891136521984]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:55:37.490][DEBUG][RK0][tid #139891128129280]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:55:37.657][INFO][RK0][main]: Evaluation, AverageLoss: 0.137693\n",
      "[HCTR][01:55:37.657][INFO][RK0][main]: Eval Time for 70 iters: 4.09027s\n",
      "[HCTR][01:55:53.087][INFO][RK0][main]: Iter: 800 Time(100 iters): 19.5246s Loss: 0.141184 lr:0.5\n",
      "[HCTR][01:55:56.251][DEBUG][RK0][tid #139891807610624]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:55:56.368][DEBUG][RK0][tid #139891681785600]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:55:56.426][DEBUG][RK0][tid #139891673392896]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:55:56.662][DEBUG][RK0][tid #139891128129280]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:55:56.779][DEBUG][RK0][tid #139891799217920]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:55:56.955][DEBUG][RK0][tid #139891665000192]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:55:57.013][DEBUG][RK0][tid #139891144914688]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:55:57.072][DEBUG][RK0][tid #139891136521984]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:55:57.190][INFO][RK0][main]: Evaluation, AverageLoss: 0.137289\n",
      "[HCTR][01:55:57.190][INFO][RK0][main]: Eval Time for 70 iters: 4.10246s\n",
      "[HCTR][01:56:12.599][INFO][RK0][main]: Iter: 900 Time(100 iters): 19.3601s Loss: 0.136896 lr:0.5\n",
      "[HCTR][01:56:15.894][DEBUG][RK0][tid #139891807610624]: file_name_ deepfm_data_nvt/val/0.35ab81b16b4a409ba42a1baf89dcba52.parquet file_total_rows_ 571942\n",
      "[HCTR][01:56:15.952][DEBUG][RK0][tid #139891799217920]: file_name_ deepfm_data_nvt/val/1.01854d707a564342aef3af44b814de1c.parquet file_total_rows_ 573919\n",
      "[HCTR][01:56:16.011][DEBUG][RK0][tid #139891681785600]: file_name_ deepfm_data_nvt/val/2.7d7593c16af64625973ed246f68af624.parquet file_total_rows_ 572137\n",
      "[HCTR][01:56:16.070][DEBUG][RK0][tid #139891673392896]: file_name_ deepfm_data_nvt/val/3.eec657484d40418cbf2648541592d09e.parquet file_total_rows_ 572545\n",
      "[HCTR][01:56:16.128][DEBUG][RK0][tid #139891665000192]: file_name_ deepfm_data_nvt/val/4.e60c2f9421d84490bbc4de5f15ec5a0f.parquet file_total_rows_ 573664\n",
      "[HCTR][01:56:16.187][DEBUG][RK0][tid #139891144914688]: file_name_ deepfm_data_nvt/val/5.883be83fecd74c1fbac00321911f2787.parquet file_total_rows_ 573448\n",
      "[HCTR][01:56:16.246][DEBUG][RK0][tid #139891136521984]: file_name_ deepfm_data_nvt/val/6.0f6ed30e74dc49668d1e1011e819e9e3.parquet file_total_rows_ 573727\n",
      "[HCTR][01:56:16.303][DEBUG][RK0][tid #139891128129280]: file_name_ deepfm_data_nvt/val/7.9e48c14d9bde498a8ef5d840d636d276.parquet file_total_rows_ 572680\n",
      "[HCTR][01:56:16.709][INFO][RK0][main]: Evaluation, AverageLoss: 0.137649\n",
      "[HCTR][01:56:16.709][INFO][RK0][main]: Eval Time for 70 iters: 4.11049s\n",
      "[HCTR][01:56:31.912][INFO][RK0][main]: Finish 1000 iterations with batchsize: 65536 in 191.01s.\n"
     ]
    }
   ],
   "source": [
    "!python3 dlrm_train.py ./distributed_plan.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87071e6",
   "metadata": {},
   "source": [
    "### Compare performance between different ETPS\n",
    "We can see the iteration time for dataparallel + localized is 103.45s while for distributed is 190.85s, which means different ETPS can greatly affect the performance of embedding. So it's better to put embedding table as data parallel or localized when the table can be fitted into single GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
