{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_movie-lens-example/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR demo on Movie lens data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "HugeCTR is a recommender-specific framework that is capable of distributed training across multiple GPUs and nodes for click-through-rate (CTR) estimation.\n",
    "HugeCTR is a component of NVIDIA Merlin ([documentation](https://nvidia-merlin.github.io/Merlin/main/README.html) | [GitHub](https://github.com/NVIDIA-Merlin/Merlin)).\n",
    "Merlin which is a framework that accelerates the entire pipeline from data ingestion and training to deploying GPU-accelerated recommender systems.\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "* Training a deep-learning recommender model (DLRM) on the MovieLens 20M [dataset](https://grouplens.org/datasets/movielens/20m/).\n",
    "* Walk through data preprocessing, training a DLRM model with HugeCTR, and then using the movie embedding to answer item similarity queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Docker containers\n",
    "\n",
    "Start the notebook inside a running 22.07 or later NGC Docker container: `nvcr.io/nvidia/merlin/merlin-hugectr:22.07`.\n",
    "The HugeCTR Python interface is installed to the path `/usr/local/hugectr/lib/` and the path is added to the environment variable `PYTHONPATH`.\n",
    "You can use the HugeCTR Python interface within the Docker container without any additional configuration.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "This notebook requires a Pascal, Volta, Turing, Ampere or newer GPUs, such as P100, V100, T4 or A100.\n",
    "You can view the GPU information with the `nvidia-smi` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 15 07:05:22 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    41W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    43W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    43W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    41W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    42W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    42W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    42W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    42W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download and preprocessing\n",
    "\n",
    "We first install a few extra utilities for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and installing 'tqdm' package.\n",
      "Downloading and installing 'unzip' command\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [663 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]                \n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2087 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1461 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [888 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]       \n",
      "Get:9 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.5 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]     \n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1176 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [30.2 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2539 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1569 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [27.5 kB]\n",
      "Fetched 24.0 MB in 3s (7097 kB/s)                            \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  unzip\n",
      "The following NEW packages will be installed:\n",
      "  unzip zip\n",
      "0 upgraded, 2 newly installed, 0 to remove and 51 not upgraded.\n",
      "Need to get 336 kB of archives.\n",
      "After this operation, 1231 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 unzip amd64 6.0-25ubuntu1 [169 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 zip amd64 3.0-11build1 [167 kB]\n",
      "Fetched 336 kB in 1s (314 kB/s)\n",
      "Selecting previously unselected package unzip.\n",
      "(Reading database ... 43708 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-25ubuntu1_amd64.deb ...\n",
      "Unpacking unzip (6.0-25ubuntu1) ...\n",
      "Selecting previously unselected package zip.\n",
      "Preparing to unpack .../zip_3.0-11build1_amd64.deb ...\n",
      "Unpacking zip (3.0-11build1) ...\n",
      "Setting up unzip (6.0-25ubuntu1) ...\n",
      "Setting up zip (3.0-11build1) ...\n",
      "Processing triggers for mime-support (3.64ubuntu1) ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading and installing 'tqdm' package.\")\n",
    "!pip3 -q install torch tqdm\n",
    "\n",
    "print(\"Downloading and installing 'unzip' command\")\n",
    "!apt-get update\n",
    "!apt-get install -y zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download and unzip the MovieLens 20M [dataset](https://grouplens.org/datasets/movielens/20m/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting 'Movie Lens 20M' dataset.\n",
      "Archive:  data/ml-20m.zip\n",
      "   creating: data/ml-20m/\n",
      "  inflating: data/ml-20m/genome-scores.csv  \n",
      "  inflating: data/ml-20m/genome-tags.csv  \n",
      "  inflating: data/ml-20m/links.csv   \n",
      "  inflating: data/ml-20m/movies.csv  \n",
      "  inflating: data/ml-20m/ratings.csv  \n",
      "  inflating: data/ml-20m/README.txt  \n",
      "  inflating: data/ml-20m/tags.csv    \n",
      "ml-20m\tml-20m.zip\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading and extracting 'Movie Lens 20M' dataset.\")\n",
    "#!wget -nc http://files.grouplens.org/datasets/movielens/ml-20m.zip -P data -q --show-progress\n",
    "!unzip -n data/ml-20m.zip -d data\n",
    "!ls ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovieLens data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "MIN_RATINGS = 20\n",
    "USER_COLUMN = 'userId'\n",
    "ITEM_COLUMN = 'movieId'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read the data into a Pandas dataframe and encode `userID` and `itemID` with integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out users with less than 20 ratings\n",
      "Mapping original user and item IDs to new sequential IDs\n",
      "Number of users: 138493\n",
      "Number of items: 26744\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/ml-20m/ratings.csv')\n",
    "print(\"Filtering out users with less than {} ratings\".format(MIN_RATINGS))\n",
    "grouped = df.groupby(USER_COLUMN)\n",
    "df = grouped.filter(lambda x: len(x) >= MIN_RATINGS)\n",
    "\n",
    "print(\"Mapping original user and item IDs to new sequential IDs\")\n",
    "df[USER_COLUMN], unique_users = pd.factorize(df[USER_COLUMN])\n",
    "df[ITEM_COLUMN], unique_items = pd.factorize(df[ITEM_COLUMN])\n",
    "\n",
    "nb_users = len(unique_users)\n",
    "nb_items = len(unique_items)\n",
    "\n",
    "print(\"Number of users: %d\\nNumber of items: %d\"%(len(unique_users), len(unique_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the data into a train and test set.\n",
    "The last movie each user has recently rated is used for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4182421</th>\n",
       "      <td>28506</td>\n",
       "      <td>3258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18950979</th>\n",
       "      <td>131159</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18950936</th>\n",
       "      <td>131159</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18950930</th>\n",
       "      <td>131159</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12341178</th>\n",
       "      <td>85251</td>\n",
       "      <td>1867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId\n",
       "4182421    28506     3258\n",
       "18950979  131159       23\n",
       "18950936  131159        3\n",
       "18950930  131159      630\n",
       "12341178   85251     1867"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to sort before popping to get the last item\n",
    "df.sort_values(by='timestamp', inplace=True)\n",
    "    \n",
    "# clean up data\n",
    "del df['rating'], df['timestamp']\n",
    "df = df.drop_duplicates() # assuming it keeps order\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HugeCTR expect user ID and item ID to be different, so we will add nb_users to the movieId to prevent key range overlapping\n",
    "df['movieId'] = df['movieId'] + nb_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have filtered and sorted by time data, we can split test data out\n",
    "grouped_sorted = df.groupby(USER_COLUMN, group_keys=False)\n",
    "test_data = grouped_sorted.tail(1).sort_values(by=USER_COLUMN)\n",
    "\n",
    "# need to pop for each group\n",
    "train_data = grouped_sorted.apply(lambda x: x.iloc[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>138513</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>138512</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>138579</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>138554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>138516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  target\n",
       "20       0   138513       1\n",
       "19       0   138512       1\n",
       "86       0   138579       1\n",
       "61       0   138554       1\n",
       "23       0   138516       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['target']=1\n",
    "test_data['target']=1\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the MovieLens data contains only positive examples, first we define a utility function to generate negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TestNegSampler:\n",
    "    def __init__(self, train_ratings, nb_users, nb_items, nb_neg):\n",
    "        self.nb_neg = nb_neg\n",
    "        self.nb_users = nb_users \n",
    "        self.nb_items = nb_items \n",
    "\n",
    "        # compute unique ids for quickly created hash set and fast lookup\n",
    "        ids = (train_ratings[:, 0] * self.nb_items) + train_ratings[:, 1]\n",
    "        self.set = set(ids)\n",
    "\n",
    "    def generate(self, batch_size=128*1024):\n",
    "        users = torch.arange(0, self.nb_users).reshape([1, -1]).repeat([self.nb_neg, 1]).transpose(0, 1).reshape(-1)\n",
    "\n",
    "        items = [-1] * len(users)\n",
    "\n",
    "        random_items = torch.LongTensor(batch_size).random_(0, self.nb_items).tolist()\n",
    "        print('Generating validation negatives...')\n",
    "        for idx, u in enumerate(tqdm.tqdm(users.tolist())):\n",
    "            if not random_items:\n",
    "                random_items = torch.LongTensor(batch_size).random_(0, self.nb_items).tolist()\n",
    "            j = random_items.pop()\n",
    "            while u * self.nb_items + j in self.set:\n",
    "                if not random_items:\n",
    "                    random_items = torch.LongTensor(batch_size).random_(0, self.nb_items).tolist()\n",
    "                j = random_items.pop()\n",
    "\n",
    "            items[idx] = j\n",
    "        items = torch.LongTensor(items)\n",
    "        return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate the negative samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating validation negatives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 69246500/69246500 [00:57<00:00, 1197676.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating validation negatives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13849300/13849300 [00:11<00:00, 1181648.22it/s]\n"
     ]
    }
   ],
   "source": [
    "sampler = _TestNegSampler(df.values, nb_users, nb_items, 500)  # using 500 negative samples\n",
    "train_negs = sampler.generate()\n",
    "train_negs = train_negs.reshape(-1, 500)\n",
    "\n",
    "sampler = _TestNegSampler(df.values, nb_users, nb_items, 100)  # using 100 negative samples\n",
    "test_negs = sampler.generate()\n",
    "test_negs = test_negs.reshape(-1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 138493/138493 [06:23<00:00, 360.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 138493/138493 [01:16<00:00, 1804.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generating negative samples for training\n",
    "train_data_neg = np.zeros((train_negs.shape[0]*train_negs.shape[1],3), dtype=int)\n",
    "idx = 0\n",
    "for i in tqdm.tqdm(range(train_negs.shape[0])):\n",
    "    for j in range(train_negs.shape[1]):\n",
    "        train_data_neg[idx, 0] = i # user ID\n",
    "        train_data_neg[idx, 1] = train_negs[i, j] # negative item ID\n",
    "        idx += 1\n",
    "    \n",
    "# generating negative samples for testing\n",
    "test_data_neg = np.zeros((test_negs.shape[0]*test_negs.shape[1],3), dtype=int)\n",
    "idx = 0\n",
    "for i in tqdm.tqdm(range(test_negs.shape[0])):\n",
    "    for j in range(test_negs.shape[1]):\n",
    "        test_data_neg[idx, 0] = i\n",
    "        test_data_neg[idx, 1] = test_negs[i, j]\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_np= np.concatenate([train_data_neg, train_data.values])\n",
    "np.random.shuffle(train_data_np)\n",
    "\n",
    "test_data_np= np.concatenate([test_data_neg, test_data.values])\n",
    "np.random.shuffle(test_data_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write HugeCTR data files\n",
    "\n",
    "After pre-processing, we write the data to disk using HugeCTR the [Norm](https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#norm) dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctypes import c_longlong as ll\n",
    "from ctypes import c_uint\n",
    "from ctypes import c_float\n",
    "from ctypes import c_int\n",
    "\n",
    "def write_hugeCTR_data(huge_ctr_data, filename='huge_ctr_data.dat'):\n",
    "    print(\"Writing %d samples\"%huge_ctr_data.shape[0])\n",
    "    with open(filename, 'wb') as f:\n",
    "        #write header\n",
    "        f.write(ll(0)) # 0: no error check; 1: check_num\n",
    "        f.write(ll(huge_ctr_data.shape[0])) # the number of samples in this data file\n",
    "        f.write(ll(1)) # dimension of label\n",
    "        f.write(ll(1)) # dimension of dense feature\n",
    "        f.write(ll(2)) # long long slot_num\n",
    "        for _ in range(3): f.write(ll(0)) # reserved for future use\n",
    "\n",
    "        for i in tqdm.tqdm(range(huge_ctr_data.shape[0])):\n",
    "            f.write(c_float(huge_ctr_data[i,2])) # float label[label_dim];\n",
    "            f.write(c_float(0)) # dummy dense feature\n",
    "            f.write(c_int(1)) # slot 1 nnz: user ID\n",
    "            f.write(c_uint(huge_ctr_data[i,0]))\n",
    "            f.write(c_int(1)) # slot 2 nnz: item ID\n",
    "            f.write(c_uint(huge_ctr_data[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filelist(filelist_name, num_files, filename_prefix):\n",
    "    with open(filelist_name, 'wt') as f:\n",
    "        f.write('{0}\\n'.format(num_files));\n",
    "        for i in range(num_files):\n",
    "            f.write('{0}_{1}.dat\\n'.format(filename_prefix, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8910827/8910827 [00:28<00:00, 313062.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8910827/8910827 [00:28<00:00, 314545.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8910827/8910827 [00:28<00:00, 313687.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8910827/8910827 [00:28<00:00, 316105.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8910827/8910827 [00:28<00:00, 313179.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8910827/8910827 [00:28<00:00, 314053.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8910827/8910827 [00:28<00:00, 312377.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8910827/8910827 [00:28<00:00, 313288.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8910827/8910827 [00:28<00:00, 313456.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8910827/8910827 [00:28<00:00, 312600.20it/s]\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./data/hugeCTR\n",
    "!mkdir ./data/hugeCTR\n",
    "\n",
    "for i, data_arr in enumerate(np.array_split(train_data_np,10)):\n",
    "    write_hugeCTR_data(data_arr, filename='./data/hugeCTR/train_huge_ctr_data_%d.dat'%i)\n",
    "\n",
    "generate_filelist('./data/hugeCTR/train_filelist.txt', 10, './data/hugeCTR/train_huge_ctr_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398780 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1398780/1398780 [00:04<00:00, 314708.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398780 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1398780/1398780 [00:04<00:00, 313743.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398780 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1398780/1398780 [00:04<00:00, 316072.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1398779/1398779 [00:04<00:00, 315541.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1398779/1398779 [00:04<00:00, 315705.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1398779/1398779 [00:04<00:00, 315520.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1398779/1398779 [00:04<00:00, 313371.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1398779/1398779 [00:04<00:00, 314972.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1398779/1398779 [00:04<00:00, 314166.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1398779/1398779 [00:04<00:00, 315031.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, data_arr in enumerate(np.array_split(test_data_np,10)):\n",
    "    write_hugeCTR_data(data_arr, filename='./data/hugeCTR/test_huge_ctr_data_%d.dat'%i)\n",
    "    \n",
    "generate_filelist('./data/hugeCTR/test_filelist.txt', 10, './data/hugeCTR/test_huge_ctr_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HugeCTR DLRM training\n",
    "\n",
    "In this section, we will train a DLRM network on the augmented movie lens data. First, we write the training Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hugectr_dlrm_movielens.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hugectr_dlrm_movielens.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 1000,\n",
    "                              batchsize_eval = 65536,\n",
    "                              batchsize = 65536,\n",
    "                              lr = 0.1,\n",
    "                              warmup_steps = 1000,\n",
    "                              decay_start = 10000,\n",
    "                              decay_steps = 40000,\n",
    "                              decay_power = 2.0,\n",
    "                              end_lr = 1e-5,\n",
    "                              vvgpu = [[0, 1]],\n",
    "                              repeat_dataset = True,\n",
    "                              use_mixed_precision = True,\n",
    "                              scaler = 1024)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                                  source = [\"./data/hugeCTR/train_filelist.txt\"],\n",
    "                                  eval_source = \"./data/hugeCTR/test_filelist.txt\",\n",
    "                                  num_workers = 2,\n",
    "                                  check_type = hugectr.Check_t.Non)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.SGD,\n",
    "                                    update_type = hugectr.Update_t.Local,\n",
    "                                    atomic_update = True)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 1, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data1\", 1, True, 2)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 150,\n",
    "                            embedding_vec_size = 64,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"dense\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=64))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=128))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=64))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Interaction,\n",
    "                            bottom_names = [\"fc3\",\"sparse_embedding1\"],\n",
    "                            top_names = [\"interaction1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"interaction1\"],\n",
    "                            top_names = [\"fc4\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"fc4\"],\n",
    "                            top_names = [\"fc5\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"fc5\"],\n",
    "                            top_names = [\"fc6\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"fc6\"],\n",
    "                            top_names = [\"fc7\"],\n",
    "                            num_output=256))                                                  \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"fc7\"],\n",
    "                            top_names = [\"fc8\"],\n",
    "                            num_output=1))                                                                                           \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc8\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 50000, display = 1000, eval_interval = 3000, snapshot = 49000, snapshot_prefix = \"./hugeCTR_saved_model_DLRM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./hugeCTR_saved_model_DLRM/\n",
    "!mkdir ./hugeCTR_saved_model_DLRM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 3.8\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][08:52:43.612][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][08:52:43.612][INFO][RK0][main]: Global seed is 3027443801\n",
      "[HCTR][08:52:43.615][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "  GPU 1 ->  node 0\n",
      "[HCTR][08:52:46.789][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][08:52:46.796][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][08:52:46.798][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][08:52:46.799][INFO][RK0][main]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HCTR][08:52:46.800][INFO][RK0][main]: Device 1: Tesla V100-SXM2-32GB\n",
      "[HCTR][08:52:46.800][INFO][RK0][main]: num of DataReader workers for train: 2\n",
      "[HCTR][08:52:46.800][INFO][RK0][main]: num of DataReader workers for eval: 2\n",
      "[HCTR][08:52:46.809][INFO][RK0][main]: max_vocabulary_size_per_gpu_=614400\n",
      "[HCTR][08:52:46.812][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][08:53:14.439][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][08:53:14.439][INFO][RK0][tid #140704957323008]: gpu1 start to init embedding\n",
      "[HCTR][08:53:14.440][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][08:53:14.440][INFO][RK0][tid #140704957323008]: gpu1 init embedding done\n",
      "[HCTR][08:53:14.450][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][08:53:14.488][INFO][RK0][main]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][08:53:14.489][INFO][RK0][main]: label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 1)                               \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "LocalizedSlotSparseEmbeddingHash        data1                         sparse_embedding1             (None, 2, 64)                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "FusedInnerProduct                       dense                         fc1                           (None, 64)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "FusedInnerProduct                       fc1                           fc2                           (None, 128)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "FusedInnerProduct                       fc2                           fc3                           (None, 64)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Interaction                             fc3                           interaction1                  (None, 68)                    \n",
      "                                        sparse_embedding1                                                                         \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "FusedInnerProduct                       interaction1                  fc4                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "FusedInnerProduct                       fc4                           fc5                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "FusedInnerProduct                       fc5                           fc6                           (None, 512)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "FusedInnerProduct                       fc6                           fc7                           (None, 256)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            fc7                           fc8                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  fc8                           loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][08:53:14.489][INFO][RK0][main]: Use non-epoch mode with number of iterations: 50000\n",
      "[HCTR][08:53:14.489][INFO][RK0][main]: Training batchsize: 65536, evaluation batchsize: 65536\n",
      "[HCTR][08:53:14.489][INFO][RK0][main]: Evaluation interval: 3000, snapshot interval: 49000\n",
      "[HCTR][08:53:14.489][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][08:53:14.489][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][08:53:14.489][INFO][RK0][main]: Use mixed precision: True, scaler: 1024.000000, use cuda graph: True\n",
      "[HCTR][08:53:14.489][INFO][RK0][main]: lr: 0.100000, warmup_steps: 1000, end_lr: 0.000010\n",
      "[HCTR][08:53:14.489][INFO][RK0][main]: decay_start: 10000, decay_steps: 40000, decay_power: 2.000000\n",
      "[HCTR][08:53:14.489][INFO][RK0][main]: Training source file: ./data/hugeCTR/train_filelist.txt\n",
      "[HCTR][08:53:14.489][INFO][RK0][main]: Evaluation source file: ./data/hugeCTR/test_filelist.txt\n",
      "[HCTR][08:53:23.393][INFO][RK0][main]: Iter: 1000 Time(1000 iters): 8.8962s Loss: 0.528513 lr:0.1\n",
      "[HCTR][08:53:32.267][INFO][RK0][main]: Iter: 2000 Time(1000 iters): 8.86544s Loss: 0.528953 lr:0.1\n",
      "[HCTR][08:53:41.173][INFO][RK0][main]: Iter: 3000 Time(1000 iters): 8.89732s Loss: 0.52741 lr:0.1\n",
      "[HCTR][08:53:46.649][INFO][RK0][main]: Evaluation, AUC: 0.615216\n",
      "[HCTR][08:53:46.649][INFO][RK0][main]: Eval Time for 1000 iters: 5.47524s\n",
      "[HCTR][08:53:55.561][INFO][RK0][main]: Iter: 4000 Time(1000 iters): 14.38s Loss: 0.173515 lr:0.1\n",
      "[HCTR][08:54:04.467][INFO][RK0][main]: Iter: 5000 Time(1000 iters): 8.89775s Loss: 0.0643398 lr:0.1\n",
      "[HCTR][08:54:13.378][INFO][RK0][main]: Iter: 6000 Time(1000 iters): 8.90232s Loss: 0.055425 lr:0.1\n",
      "[HCTR][08:54:18.613][INFO][RK0][main]: Evaluation, AUC: 0.983938\n",
      "[HCTR][08:54:18.613][INFO][RK0][main]: Eval Time for 1000 iters: 5.23426s\n",
      "[HCTR][08:54:27.527][INFO][RK0][main]: Iter: 7000 Time(1000 iters): 14.14s Loss: 0.0444573 lr:0.1\n",
      "[HCTR][08:54:36.392][INFO][RK0][main]: Iter: 8000 Time(1000 iters): 8.85758s Loss: 0.354917 lr:0.1\n",
      "[HCTR][08:54:45.263][INFO][RK0][main]: Iter: 9000 Time(1000 iters): 8.86224s Loss: 0.0637668 lr:0.1\n",
      "[HCTR][08:54:50.450][INFO][RK0][main]: Evaluation, AUC: 0.966228\n",
      "[HCTR][08:54:50.450][INFO][RK0][main]: Eval Time for 1000 iters: 5.18632s\n",
      "[HCTR][08:54:59.305][INFO][RK0][main]: Iter: 10000 Time(1000 iters): 14.0335s Loss: 0.0474014 lr:0.099995\n",
      "[HCTR][08:55:08.171][INFO][RK0][main]: Iter: 11000 Time(1000 iters): 8.8579s Loss: 0.0336978 lr:0.0950576\n",
      "[HCTR][08:55:16.985][INFO][RK0][main]: Iter: 12000 Time(1000 iters): 8.80581s Loss: 0.0208526 lr:0.0902453\n",
      "[HCTR][08:55:22.100][INFO][RK0][main]: Evaluation, AUC: 0.990911\n",
      "[HCTR][08:55:22.100][INFO][RK0][main]: Eval Time for 1000 iters: 5.11441s\n",
      "[HCTR][08:55:30.936][INFO][RK0][main]: Iter: 13000 Time(1000 iters): 13.9421s Loss: 0.0173013 lr:0.0855579\n",
      "[HCTR][08:55:39.769][INFO][RK0][main]: Iter: 14000 Time(1000 iters): 8.82507s Loss: 0.0128202 lr:0.0809955\n",
      "[HCTR][08:55:48.619][INFO][RK0][main]: Iter: 15000 Time(1000 iters): 8.84112s Loss: 0.0100981 lr:0.0765581\n",
      "[HCTR][08:55:53.942][INFO][RK0][main]: Evaluation, AUC: 0.996372\n",
      "[HCTR][08:55:53.942][INFO][RK0][main]: Eval Time for 1000 iters: 5.32278s\n",
      "[HCTR][08:56:02.785][INFO][RK0][main]: Iter: 16000 Time(1000 iters): 14.1583s Loss: 0.00852386 lr:0.0722457\n",
      "[HCTR][08:56:11.624][INFO][RK0][main]: Iter: 17000 Time(1000 iters): 8.82997s Loss: 0.00812518 lr:0.0680584\n",
      "[HCTR][08:56:20.473][INFO][RK0][main]: Iter: 18000 Time(1000 iters): 8.84099s Loss: 0.00878625 lr:0.063996\n",
      "[HCTR][08:56:25.671][INFO][RK0][main]: Evaluation, AUC: 0.997613\n",
      "[HCTR][08:56:25.671][INFO][RK0][main]: Eval Time for 1000 iters: 5.19794s\n",
      "[HCTR][08:56:34.533][INFO][RK0][main]: Iter: 19000 Time(1000 iters): 14.0519s Loss: 0.00652799 lr:0.0600586\n",
      "[HCTR][08:56:43.383][INFO][RK0][main]: Iter: 20000 Time(1000 iters): 8.84127s Loss: 0.00636787 lr:0.0562463\n",
      "[HCTR][08:56:52.245][INFO][RK0][main]: Iter: 21000 Time(1000 iters): 8.85349s Loss: 0.00630231 lr:0.0525589\n",
      "[HCTR][08:56:57.272][INFO][RK0][main]: Evaluation, AUC: 0.998177\n",
      "[HCTR][08:56:57.272][INFO][RK0][main]: Eval Time for 1000 iters: 5.02735s\n",
      "[HCTR][08:57:06.114][INFO][RK0][main]: Iter: 22000 Time(1000 iters): 13.8611s Loss: 0.00599465 lr:0.0489965\n",
      "[HCTR][08:57:14.971][INFO][RK0][main]: Iter: 23000 Time(1000 iters): 8.84889s Loss: 0.00456903 lr:0.0455591\n",
      "[HCTR][08:57:23.829][INFO][RK0][main]: Iter: 24000 Time(1000 iters): 8.84915s Loss: 0.0048366 lr:0.0422468\n",
      "[HCTR][08:57:28.904][INFO][RK0][main]: Evaluation, AUC: 0.998516\n",
      "[HCTR][08:57:28.904][INFO][RK0][main]: Eval Time for 1000 iters: 5.07521s\n",
      "[HCTR][08:57:37.757][INFO][RK0][main]: Iter: 25000 Time(1000 iters): 13.9202s Loss: 0.00472847 lr:0.0390594\n",
      "[HCTR][08:57:46.597][INFO][RK0][main]: Iter: 26000 Time(1000 iters): 8.8316s Loss: 0.00477947 lr:0.035997\n",
      "[HCTR][08:57:55.448][INFO][RK0][main]: Iter: 27000 Time(1000 iters): 8.84248s Loss: 0.00496196 lr:0.0330596\n",
      "[HCTR][08:58:00.628][INFO][RK0][main]: Evaluation, AUC: 0.998732\n",
      "[HCTR][08:58:00.628][INFO][RK0][main]: Eval Time for 1000 iters: 5.17941s\n",
      "[HCTR][08:58:09.475][INFO][RK0][main]: Iter: 28000 Time(1000 iters): 14.0191s Loss: 0.00393799 lr:0.0302472\n",
      "[HCTR][08:58:18.304][INFO][RK0][main]: Iter: 29000 Time(1000 iters): 8.82012s Loss: 0.00410887 lr:0.0275599\n",
      "[HCTR][08:58:27.122][INFO][RK0][main]: Iter: 30000 Time(1000 iters): 8.80965s Loss: 0.00343625 lr:0.0249975\n",
      "[HCTR][08:58:32.205][INFO][RK0][main]: Evaluation, AUC: 0.998878\n",
      "[HCTR][08:58:32.205][INFO][RK0][main]: Eval Time for 1000 iters: 5.08249s\n",
      "[HCTR][08:58:41.057][INFO][RK0][main]: Iter: 31000 Time(1000 iters): 13.9267s Loss: 0.00338647 lr:0.0225601\n",
      "[HCTR][08:58:49.898][INFO][RK0][main]: Iter: 32000 Time(1000 iters): 8.83291s Loss: 0.00431207 lr:0.0202478\n",
      "[HCTR][08:58:58.759][INFO][RK0][main]: Iter: 33000 Time(1000 iters): 8.85196s Loss: 0.00314963 lr:0.0180604\n",
      "[HCTR][08:59:04.056][INFO][RK0][main]: Evaluation, AUC: 0.998967\n",
      "[HCTR][08:59:04.056][INFO][RK0][main]: Eval Time for 1000 iters: 5.29728s\n",
      "[HCTR][08:59:12.903][INFO][RK0][main]: Iter: 34000 Time(1000 iters): 14.1363s Loss: 0.00491561 lr:0.015998\n",
      "[HCTR][08:59:21.769][INFO][RK0][main]: Iter: 35000 Time(1000 iters): 8.85741s Loss: 0.00385364 lr:0.0140606\n",
      "[HCTR][08:59:30.614][INFO][RK0][main]: Iter: 36000 Time(1000 iters): 8.8366s Loss: 0.00431366 lr:0.0122482\n",
      "[HCTR][08:59:35.777][INFO][RK0][main]: Evaluation, AUC: 0.999021\n",
      "[HCTR][08:59:35.777][INFO][RK0][main]: Eval Time for 1000 iters: 5.16256s\n",
      "[HCTR][08:59:44.585][INFO][RK0][main]: Iter: 37000 Time(1000 iters): 13.9628s Loss: 0.00293767 lr:0.0105609\n",
      "[HCTR][08:59:53.412][INFO][RK0][main]: Iter: 38000 Time(1000 iters): 8.81858s Loss: 0.00274502 lr:0.0089985\n",
      "[HCTR][09:00:02.255][INFO][RK0][main]: Iter: 39000 Time(1000 iters): 8.83457s Loss: 0.00254011 lr:0.00756112\n",
      "[HCTR][09:00:07.380][INFO][RK0][main]: Evaluation, AUC: 0.999059\n",
      "[HCTR][09:00:07.380][INFO][RK0][main]: Eval Time for 1000 iters: 5.1243s\n",
      "[HCTR][09:00:16.245][INFO][RK0][main]: Iter: 40000 Time(1000 iters): 13.982s Loss: 0.00315883 lr:0.00624875\n",
      "[HCTR][09:00:25.106][INFO][RK0][main]: Iter: 41000 Time(1000 iters): 8.85296s Loss: 0.0038635 lr:0.00506138\n",
      "[HCTR][09:00:33.969][INFO][RK0][main]: Iter: 42000 Time(1000 iters): 8.85403s Loss: 0.0034295 lr:0.003999\n",
      "[HCTR][09:00:39.221][INFO][RK0][main]: Evaluation, AUC: 0.999073\n",
      "[HCTR][09:00:39.221][INFO][RK0][main]: Eval Time for 1000 iters: 5.2517s\n",
      "[HCTR][09:00:48.067][INFO][RK0][main]: Iter: 43000 Time(1000 iters): 14.0899s Loss: 0.00349809 lr:0.00306162\n",
      "[HCTR][09:00:56.913][INFO][RK0][main]: Iter: 44000 Time(1000 iters): 8.83807s Loss: 0.0017837 lr:0.00224925\n",
      "[HCTR][09:01:05.775][INFO][RK0][main]: Iter: 45000 Time(1000 iters): 8.85327s Loss: 0.00304943 lr:0.00156188\n",
      "[HCTR][09:01:10.893][INFO][RK0][main]: Evaluation, AUC: 0.999083\n",
      "[HCTR][09:01:10.893][INFO][RK0][main]: Eval Time for 1000 iters: 5.11746s\n",
      "[HCTR][09:01:19.722][INFO][RK0][main]: Iter: 46000 Time(1000 iters): 13.9386s Loss: 0.00260634 lr:0.0009995\n",
      "[HCTR][09:01:28.590][INFO][RK0][main]: Iter: 47000 Time(1000 iters): 8.85932s Loss: 0.00273577 lr:0.000562125\n",
      "[HCTR][09:01:37.437][INFO][RK0][main]: Iter: 48000 Time(1000 iters): 8.8387s Loss: 0.00348975 lr:0.00024975\n",
      "[HCTR][09:01:42.659][INFO][RK0][main]: Evaluation, AUC: 0.999091\n",
      "[HCTR][09:01:42.659][INFO][RK0][main]: Eval Time for 1000 iters: 5.22141s\n",
      "[HCTR][09:01:51.535][INFO][RK0][main]: Iter: 49000 Time(1000 iters): 14.0898s Loss: 0.00397105 lr:6.23751e-05\n",
      "[HCTR][09:01:51.576][INFO][RK0][main]: Rank0: Dump hash table from GPU0\n",
      "[HCTR][09:01:51.576][INFO][RK0][main]: Rank0: Dump hash table from GPU1\n",
      "[HCTR][09:01:51.583][INFO][RK0][main]: Rank0: Write hash table <key,value> pairs to file\n",
      "[HCTR][09:01:51.662][INFO][RK0][main]: Done\n",
      "[HCTR][09:01:51.671][INFO][RK0][main]: Dumping sparse weights to files, successful\n",
      "[HCTR][09:01:51.671][INFO][RK0][main]: Dumping sparse optimzer states to files, successful\n",
      "[HCTR][09:01:51.680][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][09:01:51.681][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n",
      "[HCTR][09:02:00.584][INFO][RK0][main]: Finish 50000 iterations with batchsize: 65536 in 526.10s.\n"
     ]
    }
   ],
   "source": [
    "!python3 hugectr_dlrm_movielens.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer item similarity with DLRM embedding\n",
    "\n",
    "In this section, we demonstrate how the output of HugeCTR training can be used to carry out simple inference tasks. Specifically, we will show that the movie embeddings can be used for simple item-to-item similarity queries. Such a simple inference can be used as an efficient candidate generator to generate a small set of candidates prior to deep learning model re-ranking. \n",
    "\n",
    "First, we read the embedding tables and extract the movie embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct \n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "key_type = 'I64'\n",
    "key_type_map = {\"I32\": [\"I\", 4], \"I64\": [\"q\", 8]}\n",
    "\n",
    "embedding_vec_size = 64\n",
    "\n",
    "HUGE_CTR_VERSION = 2.21 # set HugeCTR version here, 2.2 for v2.2, 2.21 for v2.21\n",
    "\n",
    "if HUGE_CTR_VERSION <= 2.2:\n",
    "    each_key_size = key_type_map[key_type][1] + key_type_map[key_type][1] + 4 * embedding_vec_size\n",
    "else:\n",
    "    each_key_size = key_type_map[key_type][1] + 8 + 4 * embedding_vec_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_table = {}\n",
    "        \n",
    "with open(\"./hugeCTR_saved_model_DLRM/0_sparse_49000.model\" + \"/key\", 'rb') as key_file, \\\n",
    "     open(\"./hugeCTR_saved_model_DLRM/0_sparse_49000.model\" + \"/emb_vector\", 'rb') as vec_file:\n",
    "    try:\n",
    "        while True:\n",
    "            key_buffer = key_file.read(key_type_map[key_type][1])\n",
    "            vec_buffer = vec_file.read(4 * embedding_vec_size)\n",
    "            if len(key_buffer) == 0 or len(vec_buffer) == 0:\n",
    "                break\n",
    "            key = struct.unpack(key_type_map[key_type][0], key_buffer)[0]\n",
    "            values = struct.unpack(str(embedding_vec_size) + \"f\", vec_buffer)\n",
    "\n",
    "            embedding_table[key] = values\n",
    "\n",
    "    except BaseException as error:\n",
    "        print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping between the MovieId and the keys in the embedding table\n",
    "def mid_to_key(mid):\n",
    "    return mid + nb_users\n",
    "\n",
    "def key_to_mid(key):\n",
    "    return key - nb_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_key = max(embedding_table.keys())\n",
    "item_embedding = np.zeros((max_key + 1, embedding_vec_size), dtype='float')\n",
    "for i in embedding_table.keys():\n",
    "    item_embedding[i] = embedding_table[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer nearest neighbor queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def find_similar_movies(nn_movie_id, item_embedding, k=10, metric=\"euclidean\"):\n",
    "    #find the top K similar items according to one of the distance metric: cosine or euclidean\n",
    "    sim = 1-cdist(item_embedding, item_embedding[nn_movie_id].reshape(1, -1), metric=metric)\n",
    "   \n",
    "    return sim.squeeze().argsort()[-k:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "movies = pd.read_csv(\"./data/ml-20m/movies.csv\", index_col=\"movieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype='int64', name='movieId')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165237, 64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy\n",
      "Similar movies: \n",
      "339 While You Were Sleeping (1995) Comedy|Romance\n",
      "2549 Wing Commander (1999) Action|Sci-Fi\n",
      "=================================\n",
      "\n",
      "Query:  Jumanji (1995) Adventure|Children|Fantasy\n",
      "Similar movies: \n",
      "511 Program, The (1993) Action|Drama\n",
      "1897 High Art (1998) Drama|Romance\n",
      "314 Secret of Roan Inish, The (1994) Children|Drama|Fantasy|Mystery\n",
      "28 Persuasion (1995) Drama|Romance\n",
      "194 Smoke (1995) Comedy|Drama\n",
      "80 White Balloon, The (Badkonake sefid) (1995) Children|Drama\n",
      "10 GoldenEye (1995) Action|Adventure|Thriller\n",
      "1084 Bonnie and Clyde (1967) Crime|Drama\n",
      "649 Cold Fever (Á köldum klaka) (1995) Comedy|Drama\n",
      "=================================\n",
      "\n",
      "Query:  Grumpier Old Men (1995) Comedy|Romance\n",
      "Similar movies: \n",
      "626 Thin Line Between Love and Hate, A (1996) Comedy\n",
      "952 Around the World in 80 Days (1956) Adventure|Comedy\n",
      "1119 Drunks (1995) Drama\n",
      "353 Crow, The (1994) Action|Crime|Fantasy|Thriller\n",
      "791 Last Klezmer: Leopold Kozlowski, His Life and Music, The (1994) Documentary\n",
      "1115 Sleepover (1995) Drama\n",
      "237 Forget Paris (1995) Comedy|Romance\n",
      "389 Colonel Chabert, Le (1994) Drama|Romance|War\n",
      "=================================\n",
      "\n",
      "Query:  Waiting to Exhale (1995) Comedy|Drama|Romance\n",
      "Similar movies: \n",
      "406 Federal Hill (1994) Drama\n",
      "827 Convent, The (O Convento) (1995) Drama\n",
      "266 Legends of the Fall (1994) Drama|Romance|War|Western\n",
      "261 Little Women (1994) Drama\n",
      "264 Enfer, L' (1994) Drama\n",
      "511 Program, The (1993) Action|Drama\n",
      "2506 Other Sister, The (1999) Comedy|Drama|Romance\n",
      "1061 Sleepers (1996) Thriller\n",
      "206 Unzipped (1995) Documentary\n",
      "=================================\n",
      "\n",
      "Query:  Father of the Bride Part II (1995) Comedy\n",
      "Similar movies: \n",
      "2965 Omega Code, The (1999) Action\n",
      "1050 Looking for Richard (1996) Documentary|Drama\n",
      "=================================\n",
      "\n",
      "Query:  Heat (1995) Action|Crime|Thriller\n",
      "Similar movies: \n",
      "5370 Big Bad Mama II (1987) Action|Comedy\n",
      "1528 Intimate Relations (1996) Comedy\n",
      "1679 Chairman of the Board (1998) Comedy\n",
      "603 Bye Bye, Love (1995) Comedy\n",
      "2786 Haunted Honeymoon (1986) Comedy\n",
      "=================================\n",
      "\n",
      "Query:  Sabrina (1995) Comedy|Romance\n",
      "Similar movies: \n",
      "260 Star Wars: Episode IV - A New Hope (1977) Action|Adventure|Sci-Fi\n",
      "603 Bye Bye, Love (1995) Comedy\n",
      "726 Last Dance (1996) Drama\n",
      "47 Seven (a.k.a. Se7en) (1995) Mystery|Thriller\n",
      "2162 NeverEnding Story II: The Next Chapter, The (1990) Adventure|Children|Fantasy\n",
      "82 Antonia's Line (Antonia) (1995) Comedy|Drama\n",
      "=================================\n",
      "\n",
      "Query:  Tom and Huck (1995) Adventure|Children\n",
      "Similar movies: \n",
      "368 Maverick (1994) Adventure|Comedy|Western\n",
      "3579 I Dreamed of Africa (2000) Drama\n",
      "477 What's Love Got to Do with It? (1993) Drama|Musical\n",
      "423 Blown Away (1994) Action|Thriller\n",
      "339 While You Were Sleeping (1995) Comedy|Romance\n",
      "1693 Amistad (1997) Drama|Mystery\n",
      "35 Carrington (1995) Drama|Romance\n",
      "400 Homage (1995) Drama\n",
      "=================================\n",
      "\n",
      "Query:  Sudden Death (1995) Action\n",
      "Similar movies: \n",
      "742 Thinner (1996) Horror|Thriller\n",
      "481 Kalifornia (1993) Drama|Thriller\n",
      "715 Horseman on the Roof, The (Hussard sur le toit, Le) (1995) Drama|Romance\n",
      "237 Forget Paris (1995) Comedy|Romance\n",
      "640 Diabolique (1996) Drama|Thriller\n",
      "574 Spanking the Monkey (1994) Comedy|Drama\n",
      "32 Twelve Monkeys (a.k.a. 12 Monkeys) (1995) Mystery|Sci-Fi|Thriller\n",
      "8 Tom and Huck (1995) Adventure|Children\n",
      "=================================\n",
      "\n",
      "Query:  GoldenEye (1995) Action|Adventure|Thriller\n",
      "Similar movies: \n",
      "257 Just Cause (1995) Mystery|Thriller\n",
      "1913 Picnic at Hanging Rock (1975) Drama|Mystery\n",
      "1224 Henry V (1989) Action|Drama|Romance|War\n",
      "1542 Brassed Off (1996) Comedy|Drama|Romance\n",
      "243 Gordy (1995) Children|Comedy|Fantasy\n",
      "2335 Waterboy, The (1998) Comedy\n",
      "1218 Killer, The (Die xue shuang xiong) (1989) Action|Crime|Drama|Thriller\n",
      "477 What's Love Got to Do with It? (1993) Drama|Musical\n",
      "1894 Six Days Seven Nights (1998) Adventure|Comedy|Romance\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for movie_ID in movies.index[:10]:\n",
    "    try:\n",
    "        print(\"Query: \", movies.loc[movie_ID][\"title\"], movies.loc[movie_ID][\"genres\"])\n",
    "\n",
    "        print(\"Similar movies: \")\n",
    "        similar_movies = find_similar_movies(mid_to_key(movie_ID), item_embedding)\n",
    "\n",
    "        for i in similar_movies[1:]:\n",
    "            try:\n",
    "                print(key_to_mid(i), movies.loc[key_to_mid(i)][\"title\"], movies.loc[key_to_mid(i)][\"genres\"])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        print(\"=================================\\n\")\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
