<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Benchmark the DLRM Model with HPS &mdash; Merlin HugeCTR  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />

  
    <link rel="canonical" href="https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_dlrm_benchmark.html" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Profiling HPS" href="profiling_hps.html" />
    <link rel="prev" title="HPS Plugin for Torch" href="hps_torch_api/lookup_layer.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Since the HugeCTR <code>v23.09</code>, the offline inference has been deprecated.
      Since the HugeCTR <code>v24.06</code>, the HPS has been deprecated.
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">HUGECTR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Hierarchical Parameter Server</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="hps_database_backend.html">HPS Database Backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_tf_user_guide.html">HPS Plugin for TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_trt_user_guide.html">HPS Plugin for TensorRT</a></li>
<li class="toctree-l2"><a class="reference internal" href="hps_torch_user_guide.html">HPS Plugin for Torch</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Benchmark HPS-integrated DLRM</a></li>
<li class="toctree-l2"><a class="reference internal" href="profiling_hps.html">Profiling HPS</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../sparse_operation_kit.html">Sparse Operation Kit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Hierarchical Parameter Server</a></li>
      <li class="breadcrumb-item active">Benchmark the DLRM Model with HPS</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="benchmark-the-dlrm-model-with-hps">
<h1>Benchmark the DLRM Model with HPS<a class="headerlink" href="#benchmark-the-dlrm-model-with-hps" title="Permalink to this heading"></a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#benchmark-setup" id="id1">Benchmark Setup</a></p></li>
<li><p><a class="reference internal" href="#results" id="id2">Results</a></p></li>
<li><p><a class="reference internal" href="#resources" id="id3">Resources</a></p></li>
</ul>
</nav>
<section id="benchmark-setup">
<h2>Benchmark Setup<a class="headerlink" href="#benchmark-setup" title="Permalink to this heading"></a></h2>
<p>We create the DLRM model with native TensorFlow and its counterpart with HPS Plugin for TensorFlow using the <a class="reference download internal" download="" href="../_downloads/dbb8b96b5782fa5bb7367e4c0cc75722/create_tf_models.py"><span class="xref download myst">create_tf_models.py</span></a> script. The DLRM model with native TF is in the SavedModel format and the size is about 16GB, which is almost the size of embedding weights because the size of dense layer weights is small. The DLRM model with the plugin leverages HPS to store the embedding table and perform embedding lookup. The JSON configuration file and the embedding table file required by HPS are also generated by the script.</p>
<p>Furthermore, we build the TensorRT engines for the DLRM model using the <a class="reference download internal" download="" href="../_downloads/e858fb50f7c9701eb1852ee88cd4ab31/create_trt_engines.py"><span class="xref download myst">create_trt_engines.py</span></a> script, in both fp32 and fp16 modes. The script configures the engines with the HPS Plugin for TensorRT. The workflow can be summarized as three steps: convert TF SavedModel to ONNX, perform ONNX graph surgery to insert HPS plugin layer and build the TensorRT engines with HPS Plugin for TensorRT.</p>
<p>We compare three deployment methods on the Triton Inference Server:</p>
<ul class="simple">
<li><p><strong>DLRM with Native TensorFlow</strong>: The experimental option <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/VariablePolicy">VariablePolicy.SAVE_VARIABLE_DEVICES</a> is used to enable the CPU and GPU hybrid deployment of the DLRM SavedModel, i.e., the embedding table is on CPU while the MLP layers are on GPU. This deployment method is common for native TF models with large embedding tables and can be regarded as the baseline of this benchmark. The deployment is on the Triton backend for TensorFlow.</p></li>
<li><p><strong>DLRM with HPS Plugin for TensorFlow</strong>: In this DLRM SavedModel, <code class="docutils literal notranslate"><span class="pre">tf.nn.embedding_lookup</span></code> is replaced by <code class="docutils literal notranslate"><span class="pre">hps.LookupLayer</span></code> to perform embedding lookup and the MLP layers are kept unchanged. The deployment is on the Triton backend for TensorFlow.</p></li>
<li><p><strong>DLRM with HPS Plugin for TensorRT</strong>: The HPS plugin layer is integrated into the built TensorRT engines, and the MLP layers are accelerated by TensorRT. The TensorRT engines are built with minimum batch size 1, optimum 1024 and maximum 131072. Both fp32 and fp16 modes are investigated. The deployment is on the Triton backend for TensorRT.</p></li>
</ul>
<p>The benchmark is conducted on the <code class="docutils literal notranslate"><span class="pre">A100-SXM4-80GB</span></code> GPU with one Triton model instance on it. The GPU embedding cache of HPS is turned on and the cache percentage is configured as <code class="docutils literal notranslate"><span class="pre">0.2</span></code>. For details about how to deploy TF models with HPS Plugin for TensorFlow and TRT engines with HPS Plugin for TensorRT on Triton, please refer to <a class="reference internal" href="#../hps_tf/notebooks/hps_tensorflow_triton_deployment_demo.ipynb"><span class="xref myst">hps_tensorflow_triton_deployment_demo.ipynb</span></a> and <a class="reference internal" href="#../hps_trt/notebooks/demo_for_tf_trained_model.ipynb"><span class="xref myst">demo_for_tf_trained_model.ipynb</span></a>.</p>
<p>After launching the Triton Inference Server, we send the same batch of inference data repeatedly using Triton Performance Analyzer. In this case, the embedding lookup is served by the GPU embedding cache of HPS and the best-case performance of HPS can be studied. The command and the sample data to measure the latency for a batch with one sample follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>perf_analyzer<span class="w"> </span>-m<span class="w"> </span><span class="si">${</span><span class="nv">MODEL_NAME</span><span class="si">}</span><span class="w"> </span>-u<span class="w"> </span>localhost:8000<span class="w"> </span>--input-data<span class="w"> </span><span class="m">1</span>.json<span class="w"> </span>--shape<span class="w"> </span>categorical_features:1,26<span class="w"> </span>--shape<span class="w"> </span>numerical_features:1,13
</pre></div>
</div>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="nt">&quot;data&quot;</span><span class="p">:[</span>
<span class="p">{</span>
<span class="nt">&quot;categorical_features&quot;</span><span class="p">:[</span><span class="mi">276633</span><span class="p">,</span><span class="mi">7912898</span><span class="p">,</span><span class="mi">7946796</span><span class="p">,</span><span class="mi">7963854</span><span class="p">,</span><span class="mi">7971191</span><span class="p">,</span><span class="mi">7991237</span><span class="p">,</span><span class="mi">7991368</span><span class="p">,</span><span class="mi">7998351</span><span class="p">,</span><span class="mi">7999728</span><span class="p">,</span><span class="mi">8014930</span><span class="p">,</span><span class="mi">13554004</span><span class="p">,</span><span class="mi">14136456</span><span class="p">,</span><span class="mi">14382203</span><span class="p">,</span><span class="mi">14382219</span><span class="p">,</span><span class="mi">14384425</span><span class="p">,</span><span class="mi">14395091</span><span class="p">,</span><span class="mi">14395194</span><span class="p">,</span><span class="mi">14395215</span><span class="p">,</span><span class="mi">14396165</span><span class="p">,</span><span class="mi">14671338</span><span class="p">,</span><span class="mi">22562171</span><span class="p">,</span><span class="mi">25307802</span><span class="p">,</span><span class="mi">32394527</span><span class="p">,</span><span class="mi">32697105</span><span class="p">,</span><span class="mi">32709007</span><span class="p">,</span><span class="mi">32709104</span><span class="p">],</span>
<span class="nt">&quot;numerical_features&quot;</span><span class="p">:[</span><span class="mf">3.76171875</span><span class="p">,</span><span class="mf">3.806640625</span><span class="p">,</span><span class="mf">1.609375</span><span class="p">,</span><span class="mf">4.04296875</span><span class="p">,</span><span class="mf">1.7919921875</span><span class="p">,</span><span class="mf">1.0986328125</span><span class="p">,</span><span class="mf">1.0986328125</span><span class="p">,</span><span class="mf">1.609375</span><span class="p">,</span><span class="mf">2.9453125</span><span class="p">,</span><span class="mf">1.0986328125</span><span class="p">,</span><span class="mf">1.38671875</span><span class="p">,</span><span class="mf">8.3984375</span><span class="p">,</span><span class="mf">1.9462890625</span><span class="p">]</span>
<span class="p">}</span>
<span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>We take the forward latency at the server side as our benchmark metric, which is reported by the performance analyzer via the <code class="docutils literal notranslate"><span class="pre">compute</span> <span class="pre">infer</span></code> field:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>Server:
<span class="w">    </span>Inference<span class="w"> </span>count:<span class="w"> </span><span class="m">28589</span>
<span class="w">    </span>Execution<span class="w"> </span>count:<span class="w"> </span><span class="m">28589</span>
<span class="w">    </span>Successful<span class="w"> </span>request<span class="w"> </span>count:<span class="w"> </span><span class="m">28589</span>
<span class="w">    </span>Avg<span class="w"> </span>request<span class="w"> </span>latency:<span class="w"> </span><span class="m">562</span><span class="w"> </span>usec<span class="w"> </span><span class="o">(</span>overhead<span class="w"> </span><span class="m">9</span><span class="w"> </span>usec<span class="w"> </span>+<span class="w"> </span>queue<span class="w"> </span><span class="m">9</span><span class="w"> </span>usec<span class="w"> </span>+<span class="w"> </span>compute<span class="w"> </span>input<span class="w"> </span><span class="m">59</span><span class="w"> </span>usec<span class="w"> </span>+<span class="w"> </span>compute<span class="w"> </span>infer<span class="w"> </span><span class="m">431</span><span class="w"> </span>usec<span class="w"> </span>+<span class="w"> </span>compute<span class="w"> </span>output<span class="w"> </span><span class="m">53</span><span class="w"> </span>usec<span class="o">)</span>
</pre></div>
</div>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this heading"></a></h2>
<p>The benchmark is conducted with the Merlin TensorFlow container <code class="docutils literal notranslate"><span class="pre">nvcr.io/nvidia/merlin/merlin-tensorflow:23.02</span></code> on a machine with <code class="docutils literal notranslate"><span class="pre">A100-SXM4-80GB</span> <span class="pre">+</span> <span class="pre">2</span> <span class="pre">x</span> <span class="pre">AMD</span> <span class="pre">EPYC</span> <span class="pre">7742</span> <span class="pre">64-Core</span> <span class="pre">Processor</span></code>. The software versions are listed below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>TensorFlow<span class="w"> </span>version:<span class="w"> </span><span class="m">2</span>.10.0
Triton<span class="w"> </span>version:<span class="w"> </span><span class="m">22</span>.11
TensorRT<span class="w"> </span>version:<span class="w"> </span><span class="m">8</span>.5.1-1+cuda11.8
</pre></div>
</div>
<p>The per-batch forward latency, in microseconds, measured at the server side is shown in the following table and Figure 1. The Y-axis is logarithmic. The FP16 TRT engine with HPS achieves the best performance on almost all batch sizes, and has about 10x speedup to the Native TF baseline on large batch sizes.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Batch size</p></th>
<th class="head"><p>Native TF</p></th>
<th class="head"><p>TF with HPS</p></th>
<th class="head"><p>FP32 TRT with HPS</p></th>
<th class="head"><p>FP16 TRT with HPS</p></th>
<th class="head"><p>Speedup - FP16 TRT with HPS to Native TF</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>32</p></td>
<td><p>551</p></td>
<td><p>612</p></td>
<td><p>380</p></td>
<td><p>389</p></td>
<td><p>1.42</p></td>
</tr>
<tr class="row-odd"><td><p>64</p></td>
<td><p>608</p></td>
<td><p>667</p></td>
<td><p>381</p></td>
<td><p>346</p></td>
<td><p>1.76</p></td>
</tr>
<tr class="row-even"><td><p>256</p></td>
<td><p>832</p></td>
<td><p>639</p></td>
<td><p>438</p></td>
<td><p>428</p></td>
<td><p>1.94</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>1911</p></td>
<td><p>849</p></td>
<td><p>604</p></td>
<td><p>534</p></td>
<td><p>3.58</p></td>
</tr>
<tr class="row-even"><td><p>2048</p></td>
<td><p>4580</p></td>
<td><p>1059</p></td>
<td><p>927</p></td>
<td><p>766</p></td>
<td><p>5.98</p></td>
</tr>
<tr class="row-odd"><td><p>4096</p></td>
<td><p>9872</p></td>
<td><p>1459</p></td>
<td><p>1446</p></td>
<td><p>1114</p></td>
<td><p>8.86</p></td>
</tr>
<tr class="row-even"><td><p>8192</p></td>
<td><p>19643</p></td>
<td><p>2490</p></td>
<td><p>2432</p></td>
<td><p>1767</p></td>
<td><p>11.12</p></td>
</tr>
<tr class="row-odd"><td><p>16384</p></td>
<td><p>35292</p></td>
<td><p>4131</p></td>
<td><p>4355</p></td>
<td><p>3053</p></td>
<td><p>11.56</p></td>
</tr>
<tr class="row-even"><td><p>32768</p></td>
<td><p>54090</p></td>
<td><p>7795</p></td>
<td><p>6816</p></td>
<td><p>5247</p></td>
<td><p>10.31</p></td>
</tr>
<tr class="row-odd"><td><p>65536</p></td>
<td><p>107742</p></td>
<td><p>15036</p></td>
<td><p>13012</p></td>
<td><p>10022</p></td>
<td><p>10.75</p></td>
</tr>
<tr class="row-even"><td><p>131072</p></td>
<td><p>213990</p></td>
<td><p>29374</p></td>
<td><p>25440</p></td>
<td><p>19340</p></td>
<td><p>11.06</p></td>
</tr>
</tbody>
</table>
<a class="reference internal image-reference" href="../_images/hps_dlrm_latency.png"><img alt="The DLRM inference latency for different deployment methods" src="../_images/hps_dlrm_latency.png" style="width: 720px;" /></a>
<div style="text-align:center;">Figure 1. The DLRM inference latency.</div>
<p><br></br></p>
</section>
<section id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/triton-inference-server/tensorflow_backend">Triton TensorFlow Backend</a></p></li>
<li><p><a class="reference external" href="https://github.com/triton-inference-server/tensorrt_backend">Triton TensorRT Backend</a></p></li>
<li><p><a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/user_guide/perf_analyzer.md">Triton Performance Analyzer</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hps_torch_api/lookup_layer.html" class="btn btn-neutral float-left" title="HPS Plugin for Torch" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="profiling_hps.html" class="btn btn-neutral float-right" title="Profiling HPS" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v24.06.00
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../v23.08.00/hierarchical_parameter_server/hps_dlrm_benchmark.html">v23.08.00</a></dd>
      <dd><a href="../../v23.09.00/hierarchical_parameter_server/hps_dlrm_benchmark.html">v23.09.00</a></dd>
      <dd><a href="../../v23.12.00/hierarchical_parameter_server/hps_dlrm_benchmark.html">v23.12.00</a></dd>
      <dd><a href="../../v24.04.00/hierarchical_parameter_server/hps_dlrm_benchmark.html">v24.04.00</a></dd>
      <dd><a href="hps_dlrm_benchmark.html">v24.06.00</a></dd>
      <dd><a href="../../v25.03.00/index.html">v25.03.00</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../main/index.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>