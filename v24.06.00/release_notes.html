<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Release Notes &mdash; Merlin HugeCTR  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />

  
    <link rel="canonical" href="https://nvidia-merlin.github.io/HugeCTR/main/release_notes.html" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Contributing to HugeCTR" href="hugectr_contributor_guide.html" />
    <link rel="prev" title="Questions and Answers" href="QAList.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Since the HugeCTR <code>v23.09</code>, the offline inference has been deprecated.
      Since the HugeCTR <code>v24.06</code>, the HPS has been deprecated.
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">HUGECTR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="hierarchical_parameter_server/index.html">Hierarchical Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_operation_kit.html">Sparse Operation Kit</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Release Notes</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="release-notes">
<h1>Release Notes<a class="headerlink" href="#release-notes" title="Permalink to this heading"></a></h1>
<section id="what-s-new-in-version-23-12">
<h2>What’s New in Version 23.12<a class="headerlink" href="#what-s-new-in-version-23-12" title="Permalink to this heading"></a></h2>
<ul>
<li><p><strong>Lock-free Inference Cache in HPS</strong></p>
<ul class="simple">
<li><p>We have added a new lock-free GPU embedding cache for the hierarhical parameter server, which can further improve the performance of embedding table lookup in inference. It also doesn’t lead to data inconsistency even if concurrent model updates or missing key insertions are in use. That is because we ensure the cache consistency through the asynchronous stream synchronization mechanism. To enable lock-free GPU embedding cache, a user only needs to set <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_database_backend.html#configuration">“embedding_cache_type”</a> to <code class="docutils literal notranslate"><span class="pre">dynamic</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;use_hctr_cache_implementation&quot;</span></code> to <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Official SOK Release</strong></p>
<ul class="simple">
<li><p>The SOK is not an <code class="docutils literal notranslate"><span class="pre">experiment</span></code> package anymore but is now officially supported by HugeCTR. Do <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">sparse_operation_kit</span> <span class="pre">as</span> <span class="pre">sok</span></code> instead of <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">sparse_operation_kit</span> <span class="pre">import</span> <span class="pre">experiment</span> <span class="pre">as</span> <span class="pre">sok</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sok.DynamicVariable</span></code> supports Merlin-HKV as its backend</p></li>
<li><p>The parallel dump and load functions are added</p></li>
</ul>
</li>
<li><p><strong>Code Cleaning and Deprecation</strong></p>
<ul class="simple">
<li><p>Deprecated the <code class="docutils literal notranslate"><span class="pre">Model::export_predictions</span></code> function. Use the  <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#check-out-tensor-method">Model::check_out_tensor</a> function instead.</p></li>
<li><p>We have deprecated the <code class="docutils literal notranslate"><span class="pre">Norm</span></code> and legacy <code class="docutils literal notranslate"><span class="pre">Raw</span></code> DataReaders. Use <code class="docutils literal notranslate"><span class="pre">hugectr.DataReaderType_t.RawAsync</span></code> or <code class="docutils literal notranslate"><span class="pre">hugectr.DataReaderType_t.Parquet</span></code> as their alternatives.</p></li>
</ul>
</li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>Improved the performance of the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HierarchicalKV">HKV</a> lookup via the SOK</p></li>
<li><p>Fix an illegal memory access issue from the SOK backward pass, occurring in a corner case</p></li>
<li><p>Resolved the mean combiner returning zeroes, when the pooling factor is zero, which can make the SOK lookup return NaN.</p></li>
<li><p>Fixed some dependency related build issues</p></li>
<li><p>Optimized the performance of the dynamic embedding table (DET) in the SOK.</p></li>
<li><p>Fixed the crash when a user specifies negative keys in using the DET via the SOK.</p></li>
<li><p>Resolved the occasional correctness issue which becomes visible during the backward propagation phase of the SOK, in handling thousands of embedding tables.</p></li>
<li><p>Removed the runtime errors happening in the Tensorflow &gt;= 2.13.</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>If we set <code class="docutils literal notranslate"><span class="pre">max_eval_batches</span></code> and <code class="docutils literal notranslate"><span class="pre">batchsize_eval</span></code> to some large values such as 5000 and 12000 respectively, the training process leads to the illegal memory access error. <a class="reference external" href="https://github.com/NVIDIA/cccl/issues/293">The issue</a> is from the CUB, and is fixed in its latest version. However, because it is only included in CUDA 12.3, which is not used by our NGC container yet, until we update our NGC container to rely upon that version of CUDA, please rebuild HugeCTR with the newest CUB as a workaround. Otherwise, please try to avoid such large <code class="docutils literal notranslate"><span class="pre">max_eval_batches</span></code> and <code class="docutils literal notranslate"><span class="pre">batchsize_eval</span></code>.</p></li>
<li><p>HugeCTR can lead to a runtime error if client code calls RMM’s <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> or  <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> because HugeCTR’s Parquet Data Reader also calls <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code>, and it becomes visible to other libraries in the same process. Refer to [this issue] (<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/356">https://github.com/NVIDIA-Merlin/HugeCTR/issues/356</a>) . As a workaround, a user can set an environment variable <code class="docutils literal notranslate"><span class="pre">HCTR_RMM_SETTABLE</span></code> to 0 to disable HugeCTR to set a custom RMM device resource, if they know <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code>  is called outside HugeCTR. But be cautious, as it could affect the performance of parquet reading.</p></li>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">this NCCL known issue</a> and this GitHub issue](<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">https://github.com/NVIDIA-Merlin/HugeCTR/issues/243</a>).</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
<li><p>Dumping Adam optimizer states to AWS S3 is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-23-11">
<h2>What’s New in Version 23.11<a class="headerlink" href="#what-s-new-in-version-23-11" title="Permalink to this heading"></a></h2>
<ul>
<li><p><strong>Code Cleaning and Deprecation</strong></p>
<ul class="simple">
<li><p>The offline inference has been deprecated from our documentation, notebook suite, and code. Please check out the HPS plugin for <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_tf_user_guide.html">TensorFlow</a> and <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_trt_user_guide.html">TensorRT</a>. The multi-GPU inference is not illustrated in <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/main/hps_trt/notebooks/demo_for_tf_trained_model.ipynb">this HPS TRT notebook</a>.</p></li>
<li><p>We are working on deprecating the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.html">Embedding Training Cache (ETC)</a>. If you trying using that feature, it still works but omits a deprecation warning message. In a near-futre release, they will be removed from the API and code level. Please refer to the NVIDIA <a class="reference external" href="https://github.com/NVIDIA-Merlin/HierarchicalKV">HierarchicalKV</a> as an alternative.</p></li>
<li><p>In this release, we have also cleaned up our C++ code and CMakeLists.txt to improve their maintainability and fix minor but potential issues. There will be more code cleanup in several future releases.</p></li>
</ul>
</li>
<li><p><strong>General Updates</strong>:</p>
<ul class="simple">
<li><p>Enabled the support of the static CUDA runtime. Now you can experimentally enable the feature by specifying <code class="docutils literal notranslate"><span class="pre">-DUSE_CUDART_STATIC=ON</span></code> in configuring the code with cmake, while the dynamic CUDA runtime is still used by default.</p></li>
<li><p>Added HPS as a custom extension for TorchScript. A user can leverage the HPS embedding lookup during the inference of scripted torch module.</p></li>
</ul>
</li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>Resolved a couple of performance regressions when the SOK is used together with HKV, related to unique operation and unified memory</p></li>
<li><p>Reduced the unnessary memory consumption of intermediate buffers in loading and dumping the SOK embedding</p></li>
<li><p>Fixed the Interaction Layer to support large <code class="docutils literal notranslate"><span class="pre">num_slots</span></code></p></li>
<li><p>Resolved the occasional runtime error in using multiple H800 GPUs</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>If we set <code class="docutils literal notranslate"><span class="pre">max_eval_batches</span></code> and <code class="docutils literal notranslate"><span class="pre">batchsize_eval</span></code> to some large values such as 5000 and 12000 respectively, the training process leads to the illegal memory access error. <a class="reference external" href="https://github.com/NVIDIA/cccl/issues/293">The issue</a> is from the CUB, and is fixed in its latest version. However, because it is only included in CUDA 12.3, which is not used by our NGC container yet, until we update our NGC container to rely upon that version of CUDA, please rebuild HugeCTR with the newest CUB as a workaround. Otherwise, please try to avoid such large <code class="docutils literal notranslate"><span class="pre">max_eval_batches</span></code> and <code class="docutils literal notranslate"><span class="pre">batchsize_eval</span></code>.</p></li>
<li><p>HugeCTR can lead to a runtime error if client code calls RMM’s <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> or  <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> because HugeCTR’s Parquet Data Reader also calls <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code>, and it becomes visible to other libraries in the same process. Refer to [this issue] (<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/356">https://github.com/NVIDIA-Merlin/HugeCTR/issues/356</a>) . As a workaround, a user can set an environment variable <code class="docutils literal notranslate"><span class="pre">HCTR_RMM_SETTABLE</span></code> to 0 to disable HugeCTR to set a custom RMM device resource, if they know <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code>  is called outside HugeCTR. But be cautious, as it could affect the performance of parquet reading.</p></li>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">this NCCL known issue</a> and this GitHub issue](<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">https://github.com/NVIDIA-Merlin/HugeCTR/issues/243</a>).</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
<li><p>Dumping Adam optimizer states to AWS S3 is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-23-08">
<h2>What’s New in Version 23.08<a class="headerlink" href="#what-s-new-in-version-23-08" title="Permalink to this heading"></a></h2>
<ul>
<li><p><strong>Hierarchical Parameter Server</strong>:</p>
<ul class="simple">
<li><p>Support static EC fp8 quantization: We already support quantization for fp8 in the static cache. HPS will perform fp8 quantization on the embedding vector when reading the embedding table by enable fp8_quant configuration, and perform fp32 dequantization on the embedding vector corresponding to the queried embedding key in the static embedding cache, so as to ensure the accuracy of dense part prediction.</p></li>
<li><p>Large model deployment demo based on HPS TensorRT-plugin: This demo shows how to use the HPS TRT-plugin to build a complete TRT engine for deploying a 147GB embedding table based on a 1TB Criteo dataset. We also provide static embedding implementation for fully offloading embedding tables to host page-locke memory for benchmarks on x86 and Grace Hopper Superchip.</p></li>
<li><p>Issues Fixed</p>
<ul>
<li><p>Resolve Kafka update ingestion error. There was an error that prevented handing over online parameter updates coming from Kafka message queues to Redis database backends.</p></li>
<li><p>Fixed HPS Triton backend re-initializing the embedding cache issue due to undefined null when getting the embedded cache on the corresponding device.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>HugeCTR Training &amp; SOK</strong>:</p>
<ul class="simple">
<li><p>Dense Embedding Support in Embedding Collection: We add the dense embedding in embedding collection. To use the dense embedding, a user just needs to specify the <code class="docutils literal notranslate"><span class="pre">_concat_</span></code> as the combiner. For more information, please refer to <a class="reference internal" href="#test/embedding_collection_test/dgx_a100_one_hot.py"><span class="xref myst">dense_embedding.py</span></a>.</p></li>
<li><p>Refinement of sequence mask layer and attention softmax layer to support cross-attention.</p></li>
<li><p>We introduce a more generalized reshape layer which allows user to reshape source tensor to destination tensor without dimension restriction. Please refer <a class="reference internal" href="#docs/source/api/hugectr_layer_book.md#reshape-layer"><span class="xref myst">Reshape Layer API</span></a> for more detailed information</p></li>
<li><p>Issues Fixed</p>
<ul>
<li><p>Fix error when using Localized Variable in Sparse Operation Kit</p></li>
<li><p>Fix bug in Sparse Operation Kit backward computing.</p></li>
<li><p>Fix some  SOK performance bugs by replacing the calls to <code class="docutils literal notranslate"><span class="pre">DeviceSegmentedSort</span></code> with <code class="docutils literal notranslate"><span class="pre">DeviceSegmentedRadixSort</span></code></p></li>
<li><p>Fix a bug from the SOK’s Python API side, which led to the duplicate calls to the model’s forward function and thus degraded the performance.</p></li>
<li><p>Reduce the CPU launch overhead</p>
<ul>
<li><p>Remove dynamic vector allocation in DataDistributor</p></li>
<li><p>Remove the use of the checkout value tensor from the DataReader. The data reader generates a nested std::vector on-the-fly and returns the vector to the embedding collection, which incur lots of host overhead. We have made it a class member so that the overhead can be eliminated.</p></li>
</ul>
</li>
<li><p>Align with the latest parquet update.
We have fixed a bug due to the parquet_reader_options::set_num_rows() update of cudf 23.06: <a class="reference external" href="https://github.com/rapidsai/cudf/pull/13063">PR</a> .</p></li>
<li><p>Fix core23 assertion of debug mode
We have fixed an assertion bug while the new core library is enabled if HugeCTR is built in debug mode.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>General Updates</strong>:</p>
<ul class="simple">
<li><p>Cleaned up logging code. Added compile-time format-string validation. Fixed issue where HCTR_PRINT did not interpret format strings properly.</p></li>
<li><p>Enabled the experimental enablement of the static CUDA runtime. Use <code class="docutils literal notranslate"><span class="pre">-DUSE_CUDART_STATIC=ON</span></code> in cmak’ing</p></li>
<li><p>Modified the data preprocessing documentation to clarify the correct commands to use in different situations. Fixed the error of the description of arguments</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>HugeCTR can lead to a runtime error if client code calls RMM’s <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> or  <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> because HugeCTR’s Parquet Data Reader also calls <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code>, and it becomes visible to other libraries in the same process. Refer to [this issue] (<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/356">https://github.com/NVIDIA-Merlin/HugeCTR/issues/356</a>) . As a workaround, a user can set an environment variable <code class="docutils literal notranslate"><span class="pre">HCTR_RMM_SETTABLE</span></code> to 0 to disable HugeCTR to set a custom RMM device resource, if they know <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code>  is called outside HugeCTR. But be cautious, as it could affect the performance of parquet reading.</p></li>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">this NCCL known issue</a> and this GitHub issue](<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">https://github.com/NVIDIA-Merlin/HugeCTR/issues/243</a>).</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
<li><p>Dumping Adam optimizer states to AWS S3 is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-23-06">
<h2>What’s New in Version 23.06<a class="headerlink" href="#what-s-new-in-version-23-06" title="Permalink to this heading"></a></h2>
<p>In this release, we have fixed issues and enhanced the code.</p>
<ul>
<li><p><strong>3G Embedding Updates</strong>:</p>
<ul class="simple">
<li><p>Refactored the <code class="docutils literal notranslate"><span class="pre">DataDistributor</span></code> related code</p></li>
<li><p>New SOK <code class="docutils literal notranslate"><span class="pre">load()</span></code> and <code class="docutils literal notranslate"><span class="pre">dump()</span></code> APIs are usable in TensorFlow 2. To use the API, specify <code class="docutils literal notranslate"><span class="pre">sok_vars</span></code> in addition to <code class="docutils literal notranslate"><span class="pre">path</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sok_vars</span></code> is a list of <code class="docutils literal notranslate"><span class="pre">sok.variable</span></code> and/or <code class="docutils literal notranslate"><span class="pre">sok.dynamic_variable</span></code>.</p></li>
<li><p>If you want to store optimizer states such as <code class="docutils literal notranslate"><span class="pre">m</span></code> and <code class="docutils literal notranslate"><span class="pre">v</span></code> of <code class="docutils literal notranslate"><span class="pre">Adam</span></code>,  the <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> must be specified as well.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> must be a <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Optimizer</span></code> or <code class="docutils literal notranslate"><span class="pre">sok.OptimizerWrapper</span></code> while their underlying type must be <code class="docutils literal notranslate"><span class="pre">SGD</span></code>, <code class="docutils literal notranslate"><span class="pre">Adamax</span></code>, <code class="docutils literal notranslate"><span class="pre">Adadelta</span></code>, <code class="docutils literal notranslate"><span class="pre">Adagrad</span></code>, or <code class="docutils literal notranslate"><span class="pre">Ftrl</span></code>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sparse_operation_kit</span> <span class="k">as</span> <span class="nn">sok</span>

<span class="n">sok</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">sok_vars</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">sok</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">sok_vars</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>These APIs are independent from the number of GPUs in use and the sharding strategy. For instance, a distributed embedding table trained and dumped with 8 GPUs can be loaded to train on a 4-GPU machine.</p>
</li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>Fixed the segmentation fault and wrong initialization when the embedding table fusion is enabled in using the HPS UVM implementation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cudaDeviceSynchronize()</span></code> is removed when building the HugeCTR in the debug mode, so you can enable the CUDA Graph even in the debug mode.</p></li>
<li><p>Modified some Notebooks to use the most recent version of NGC container</p></li>
<li><p>Fixed the <code class="docutils literal notranslate"><span class="pre">EmbeddingTableCollection</span></code> utest to run correctly with multiple GPUs</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>HugeCTR can lead to a runtime error if client code calls RMM’s <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> or  <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> because HugeCTR’s Parquet Data Reader also calls <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code>, and it becomes visible to other libraries in the same process. Refer to [this issue] (<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/356">https://github.com/NVIDIA-Merlin/HugeCTR/issues/356</a>) . As a workaround, set an environment variable <code class="docutils literal notranslate"><span class="pre">HCTR_RMM_SETTABLE</span></code> to 0 to disable HugeCTR to set a custom RMM device resource, if they know <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code>  is called outside HugeCTR. But be cautious, as it could affect the performance of parquet reading.</p></li>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">this NCCL known issue</a> and <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">this GitHub issue</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka,make sure that a sufficient number of Kafka brokers are running, operating properly, and reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers. Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
<li><p>Dumping Adam optimizer states to AWS S3 is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-23-04">
<h2>What’s New in Version 23.04<a class="headerlink" href="#what-s-new-in-version-23-04" title="Permalink to this heading"></a></h2>
<ul>
<li><p><strong>Hierarchical Parameter Server Enhancements</strong>:</p>
<ul class="simple">
<li><p>HPS Table Fusion: From this release, you can fuse tables of the same embedding vector size in HPS. We support this feature in the HPS plugin for TensorFlow and the Triton backend for HPS. To turn on table fusion, set <code class="docutils literal notranslate"><span class="pre">fuse_embedding_table</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code> in the HPS JSON file. This feature requires that the key values in different tables do not overlap and the embedding lookup layers are not dependent on each other in the model graph. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_database_backend.html#configuration">HPS configuration</a> and <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hps_tf/notebooks/hps_table_fusion_demo.html">HPS table fusion demo notebook</a>. This feature can reduce the embedding lookup latency significantly when there are multiple tables and GPU embedding cache is employed. About 3x speedup is achieved on V100 for the fused case demonstrated in the notebook compared to the unfused one.</p></li>
<li><p>UVM Support: We have upgraded the static embedding solution. For embedding tables whose size exceeds the device memory, we will save high-frequency embeddings in the HBM as an embedding cache and offload the remaining embeddings to the UVM. Compared with the dynamic cache solution that offloads the remaining embeddings to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_database_backend.html#volatile-database-configuration">Volatile DB</a>, the UVM solution has higher CPU lookup throughput. We will support online updating of the UVM solution in a future release. Users can switch between different embedding cache solutions through the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_database_backend.html#inference-parameters">embedding_cache_type</a> configuration parameter.</p></li>
<li><p>Triton Perf Analayzer’s Request Generator: We have added an <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/main/tools/inference_test_scripts/request_generator">inference request generator</a> to generate the JSON request format required by Triton Perf Analyzer. By using this request generator together with the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/main/tools/inference_test_scripts/model_generator">model generator</a>, you can use the Triton Perf Analyzer to profile the HPS performance and do stress testing. For API documentation and demo usage, please refer to <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/main/tools/inference_test_scripts/README.md">README</a></p></li>
</ul>
</li>
<li><p><strong>General Updates</strong>:</p>
<ul class="simple">
<li><p>DenseLayerComputeConfig: MLP and CrossLayer support asynchronous weight gradient computations with data gradient backpropagation when training. We have added a new member <code class="docutils literal notranslate"><span class="pre">hugectr</span> <span class="pre">DenseLayerComputeConfig</span></code> to <code class="docutils literal notranslate"><span class="pre">hugectr.DenseLayer</span></code> for configuring the computing behavior. The knob for enabling asynchronous weight gradient computations has been moved from <code class="docutils literal notranslate"><span class="pre">hugectr.CreateSolver</span></code> to <code class="docutils literal notranslate"><span class="pre">hugectr.DenseLayerComputeConfig.async_wgrad</span></code>. The knob for controlling the fusion mode of weight gradients and bias gradients has been moved from <code class="docutils literal notranslate"><span class="pre">hugectr.DenseLayerSwitchs</span></code> to <code class="docutils literal notranslate"><span class="pre">hugectr.DenseLayerComputeConfig.fuse_wb</span></code>.</p></li>
<li><p>Hopper Architecture Support: Users can build HugeCTR from scratch with the compute capability 9.0 (<code class="docutils literal notranslate"><span class="pre">DSM=90</span></code>), so that it can run on Hopper architectures. Note that our NGC container does not support the compute capability yet. Users who are unfamiliar with how to build HugeCTR can refer to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_contributor_guide.html#build-hugectr-training-container-from-source">HugeCTR Contribution Guide</a>.</p></li>
<li><p>RoCE Support for Hybrid Embedding: With the parameter <code class="docutils literal notranslate"><span class="pre">CommunicationType.IB_NVLink_Hier</span></code> in <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#hybridembeddingparam">HybridEmbeddingParams</a>, the RoCE is supported. We have also added 2 environment variables <code class="docutils literal notranslate"><span class="pre">HUGECTR_ROCE_GID</span></code> and <code class="docutils literal notranslate"><span class="pre">HUGECTR_ROCE_TC</span></code> so that a user can control the RoCE NIC’s GID and traffic class.
<a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#hybridembeddingparam-class">https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#hybridembeddingparam-class</a></p></li>
</ul>
</li>
<li><p><strong>Documentation Updates</strong>:</p>
<ul class="simple">
<li><p>Data Reader: We have enhanced our Raw data reader to read multi-hot input data, connecting with an embedding collection seamlessly. The raw dataset format is strengthened as well. Refer to our <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#raw">online documentation</a> for more details. We have refined the description for Norm datasest as well.</p></li>
<li><p>Embedding Collection: We have added the knob <code class="docutils literal notranslate"><span class="pre">is_exclusive_keys</span></code> to enable potential acceleration if a user has already preprocessed the input of embedding collection to make the resulting tables exclusive with one another. We have also added the nob <code class="docutils literal notranslate"><span class="pre">comm_strategy</span></code> in embedding collection for user to configure optimized communication strategy in multi-node training</p></li>
<li><p>HPS Plugin: We have fixed the unit of measurement for DLRM inference benchmark results that leverage the HPS plugin. We have updated the user guide for the HPS plugin for TensorFlow and the HPS plugin for TensorRT</p></li>
<li><p>Embedding Cache: We have updated the usage of three types of embedding cache. We have updated the descriptions of the three types of embedding cache as well.</p></li>
</ul>
</li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>We added a slots emptiness check to prevent <code class="docutils literal notranslate"><span class="pre">SparseParam</span></code> from being misused.</p></li>
<li><p>We revised MPI lifetime service to become MPI init service with slightly greater scope and clearer interface. In this effort, we also fixed a rare bug that could lead access violations during the MPI shutdown procedure.</p></li>
<li><p>We fixed a segment fault that occurs when a GPU has no embedding wgrad to update.</p></li>
<li><p>SOK build &amp; runtime error related to TF version: We made the SOK Experiment](<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/main/sparse_operation_kit/experiment">https://github.com/NVIDIA-Merlin/HugeCTR/tree/main/sparse_operation_kit/experiment</a>) compatible with the Tensorflow &gt;= v2.11.0. The legacy SOK doesn’t support that and newer versions of Tensorflow.</p></li>
<li><p>HPS requires CPU memory to be at least 2.5x larger than the model size during its initialization. From this release, we parse the model embedding files through chunks and reduce the required memory to 1.3x model size.</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>HugeCTR can lead to a runtime error if client code calls RMM’s <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> or  <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> because HugeCTR’s Parquet Data Reader also calls <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code>, and it becomes visible to other libraries in the same process. Refer to [this issue] (<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/356">https://github.com/NVIDIA-Merlin/HugeCTR/issues/356</a>) . As a workaround, a user can set an environment variable <code class="docutils literal notranslate"><span class="pre">HCTR_RMM_SETTABLE</span></code> to 0 to disable HugeCTR to set a custom RMM device resource, if they know <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code>  is called outside HugeCTR. But be cautious, as it could affect the performance of parquet reading.</p></li>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">this NCCL known issue</a> and this GitHub issue](<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">https://github.com/NVIDIA-Merlin/HugeCTR/issues/243</a>).</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
<li><p>Dumping Adam optimizer states to AWS S3 is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-23-02">
<h2>What’s New in Version 23.02<a class="headerlink" href="#what-s-new-in-version-23-02" title="Permalink to this heading"></a></h2>
<ul>
<li><p><strong>HPS Enhancements</strong>:</p>
<ul class="simple">
<li><p>Enabled <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/hps_tf_user_guide.html">the HPS Tensorflow plugin</a>.</p></li>
<li><p>Enabled the max_norm clipping for the HPS Tensorflow plugin.</p></li>
<li><p>Optimized the performance of HPS HashMap fetch.</p></li>
<li><p>Enabled <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/main/HugeCTR/src/inference_benchmark/hps_profiler.md">the HPS Profiler</a>.</p></li>
</ul>
</li>
<li><p><strong>Google Cloud Storage (GCS) Support</strong>:</p>
<ul class="simple">
<li><p>Added the support of Google Cloud Storage(GCS) for both training and inference. For more details, check out the GCS section in <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/main/notebooks/training_and_inference_with_remote_filesystem.ipynb">the <strong>training with remote filesystem</strong> notebook</a>.</p></li>
</ul>
</li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>Fixed a bug in HPS static table, which leads to a wrong results when the batch size is larger than 256.</p></li>
<li><p>Fixed a preprocessing issue in the <code class="docutils literal notranslate"><span class="pre">wdl_prediction</span></code> notebook.</p></li>
<li><p>Corrected how devices are set and managed in HPS and InferenceModel.</p></li>
<li><p>Fixed the debug build error.</p></li>
<li><p>Fixed the build error related with the CUDA 12.0.</p></li>
<li><p>Fixed reported issues with respect to Multi-Process HashMap in notebook and a couple of minor issues on the side.</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also the NCCL <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">known issue</a> and the GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">issue</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and are reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
<li><p>Dumping Adam optimizer states to AWS S3 is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-4-3">
<h2>What’s New in Version 4.3<a class="headerlink" href="#what-s-new-in-version-4-3" title="Permalink to this heading"></a></h2>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>In January 2023, the HugeCTR team plans to deprecate semantic versioning, such as <code class="docutils literal notranslate"><span class="pre">v4.3</span></code>.
Afterward, the library will use calendar versioning only, such as <code class="docutils literal notranslate"><span class="pre">v23.01</span></code>.</p>
</div>
<ul>
<li><p><strong>Support for BERT and Variants</strong>:
This release includes support for BERT in HugeCTR.
The documentation includes updates to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.3/api/hugectr_layer_book.html#multiheadattention-layer">MultiHeadAttention</a> layer and adds documentation for the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.3/api/hugectr_layer_book.html#sequencemask-layer">SequenceMask</a> layer.
For more information, refer to the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v4.3/samples/bst">samples/bst</a> directory of the repository in GitHub.</p></li>
<li><p><strong>HPS Plugin for TensorFlow integration with TensorFlow-TensorRT (TF-TRT)</strong>:
This release includes plugin support for integration with TensorFlow-TensorRT.
For sample code, refer to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.3/hps_tf/notebooks/hps_tensorflow_triton_deployment_demo.html">Deploy SavedModel using HPS with Triton TensorFlow Backend</a> notebook.</p></li>
<li><p><strong>Deep &amp; Cross Network Layer version 2 Support</strong>:
This release includes support for Deep &amp; Cross Network version 2.
For conceptual information, refer to <a class="reference external" href="https://arxiv.org/abs/2008.13535">https://arxiv.org/abs/2008.13535</a>.
The documentation for the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.3/api/hugectr_layer_book.html#multicross-layer">MultiCross Layer</a> is updated.</p></li>
<li><p><strong>Enhancements to Hierarchical Parameter Server</strong>:</p>
<ul class="simple">
<li><p>RedisClusterBackend now supports TLS/SSL communication.
For sample code, refer to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.3/notebooks/hps_demo.html">Hierarchical Parameter Server Demo</a> notebook.
The notebook is updated with step-by-step instructions to show you how to setup HPS to use Redis with (and without) encryption.
The <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.3/hugectr_parameter_server.html#volatile-database-parameters">Volatile Database Parameters</a> documentation for HPS is updated with the <code class="docutils literal notranslate"><span class="pre">enable_tls</span></code>, <code class="docutils literal notranslate"><span class="pre">tls_ca_certificate</span></code>, <code class="docutils literal notranslate"><span class="pre">tls_client_certificate</span></code>, <code class="docutils literal notranslate"><span class="pre">tls_client_key</span></code>, and <code class="docutils literal notranslate"><span class="pre">tls_server_name_identification</span></code> parameters.</p></li>
<li><p>MultiProcessHashMapBackend includes a bug fix that prevented configuring the shared memory size when using JSON file-based configuration.</p></li>
<li><p>On-device input keys are supported now so that an extra host-to-device copy is removed to improve performance.</p></li>
<li><p>A dependency on the XX-Hash library is removed.
The library is no longer used by HugeCTR.</p></li>
<li><p>Added the static table support to the embedding cache.
The static table is suitable when the embedding table can be placed entirely in GPU memory.
In this case, the static table is more than three times faster than the embedding cache lookup.
The static table does not support embedding updates.</p></li>
</ul>
</li>
<li><p><strong>Support for New Optimizers</strong>:</p>
<ul class="simple">
<li><p>Added support for SGD, Momentum SGD, Nesterov Momentum, AdaGrad, RMS-Prop, Adam and FTRL optimizers for dynamic embedding table (DET).
For sample code, refer to the <code class="docutils literal notranslate"><span class="pre">test_embedding_table_optimizer.cpp</span></code> file in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v4.3/test/utest/embedding_collection">test/utest/embedding_collection/</a> directory of the repository on GitHub.</p></li>
<li><p>Added support for the FTRL optimizer for dense networks.</p></li>
</ul>
</li>
<li><p><strong>Data Reading from S3 for Offline Inference</strong>:
In addition to reading during training, HugeCTR now supports reading data from remote file systems such as HDFS and S3 during offline inference by using the DataSourceParams API.
The <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.3/notebooks/training_and_inference_with_remote_filesystem.html">HugeCTR Training and Inference with Remote File System Example</a> is updated to demonstrate the new functionality.</p></li>
<li><p><strong>Documentation Enhancements</strong>:</p>
<ul class="simple">
<li><p>The set up <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.3/notebooks/index.html">instructions for running the example notebooks</a> are revised for clarity.</p></li>
<li><p>The example notebooks are also updated to show using a data preprocessing script that simplifies the user experience.</p></li>
<li><p>Documentation for the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.3/api/hugectr_layer_book.html#mlp-layer">MLP Layer</a> is new.</p></li>
<li><p>Several 2022 talks and blogs are added to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.3/hugectr_talks_blogs.html">HugeCTR Talks and Blogs</a> page.</p></li>
</ul>
</li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>The original CUDA device with NUMA bind before a call to some HugeCTR APIs is recovered correctly now.
This issue sometimes lead to a problem when you mixed calls to HugeCTR and other CUDA enabled libraries.</p></li>
<li><p>Fixed the occasional CUDA kernel launch failure of embedding when installed HugeCTR with macro DEBUG.</p></li>
<li><p>Fixed an SOK build error that was related to TensorFlow v2.1.0 and higher.
The issue was that the C++ API and C++ standard were updated to use C++17.</p></li>
<li><p>Fixed a CUDA 12 related compilation error.</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>HugeCTR can lead to a runtime error if client code calls the RMM <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> method or  <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> method.
The error is due to the Parquet data reader in HugeCTR also calling <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code>.
As a result, the device becomes visible to other libraries in the same process.
Refer to GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/356">issue #356</a> for more information.
As a workaround, you can set environment variable <code class="docutils literal notranslate"><span class="pre">HCTR_RMM_SETTABLE</span></code> to <code class="docutils literal notranslate"><span class="pre">0</span></code> to prevent HugeCTR from setting a custom RMM device resource, if you know that <code class="docutils literal notranslate"><span class="pre">rmm::mr::set_current_device_resource()</span></code> is called by client code other than HugeCTR.
But be cautious because the setting can reduce the performance of Parquet reading.</p></li>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also the NCCL <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">known issue</a> and the GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">issue #243</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and are reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
<li><p>Dumping Adam optimizer states to AWS S3 is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-4-2">
<h2>What’s New in Version 4.2<a class="headerlink" href="#what-s-new-in-version-4-2" title="Permalink to this heading"></a></h2>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>In January 2023, the HugeCTR team plans to deprecate semantic versioning, such as <code class="docutils literal notranslate"><span class="pre">v4.2</span></code>.
Afterward, the library will use calendar versioning only, such as <code class="docutils literal notranslate"><span class="pre">v23.01</span></code>.</p>
</div>
<ul>
<li><p><strong>Change to HPS with Redis or Kafka</strong>:
This release includes a change to Hierarchical Parameter Server and affects deployments that use <code class="docutils literal notranslate"><span class="pre">RedisClusterBackend</span></code> or model parameter streaming with Kafka.
A third-party library that was used for HPS partition selection algorithm is replaced to improve performance.
The new algorithm can produce different partition assignments for volatile databases.
As a result, volatile database backends that retain data between application startup, such as the <code class="docutils literal notranslate"><span class="pre">RedisClusterBackend</span></code>, must be reinitialized.
Model streaming with Kafka is equally affected.
To avoid issues with updates, reset all respective queue offsets to the <code class="docutils literal notranslate"><span class="pre">end_offset</span></code> before you reinitialize the <code class="docutils literal notranslate"><span class="pre">RedisClusterBackend</span></code>.</p></li>
<li><p><strong>Enhancements to the Sparse Operation Kit in DeepRec</strong>:
This release includes updates to the Sparse Operation Kit to improve the performance of the embedding variable lookup operation in DeepRec.
The API for the <code class="docutils literal notranslate"><span class="pre">lookup_sparse()</span></code> function is changed to remove the <code class="docutils literal notranslate"><span class="pre">hotness</span></code> argument.
The <code class="docutils literal notranslate"><span class="pre">lookup_sparse()</span></code> function is enhanced to calculate the number of non-zero elements dynamically.
For more information, refer to the <a class="reference external" href="https://github.com/alibaba/DeepRec/tree/main/addons/sparse_operation_kit">sparse_operation_kit directory</a> of the DeepRec repository in GitHub.</p></li>
<li><p><strong>Enhancements to 3G Embedding</strong>:
This release includes the following enhancements to 3G embedding:</p>
<ul class="simple">
<li><p>The API is changed.
The <code class="docutils literal notranslate"><span class="pre">EmbeddingPlanner</span></code> class is replaced with the <code class="docutils literal notranslate"><span class="pre">EmbeddingCollectionConfig</span></code> class.
For examples of the API, see the tests in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v4.2/test/embedding_collection_test">test/embedding_collection_test</a> directory of the repository in GitHub.</p></li>
<li><p>The API is enhanced to support dumping and loading weights during the training process.
The methods are <code class="docutils literal notranslate"><span class="pre">Model.embedding_dump(path:</span> <span class="pre">str,</span> <span class="pre">table_names:</span> <span class="pre">list[str])</span></code> and <code class="docutils literal notranslate"><span class="pre">Model.embedding_load(path:</span> <span class="pre">str,</span> <span class="pre">list[str])</span></code>.
The <code class="docutils literal notranslate"><span class="pre">path</span></code> argument is a directory in file system that you can dump weights to or load weights from.
The <code class="docutils literal notranslate"><span class="pre">table_names</span></code> argument is a list of embedding table names as strings.</p></li>
</ul>
</li>
<li><p><strong>New Volatile Database Type for HPS</strong>:
This release adds a <code class="docutils literal notranslate"><span class="pre">db_type</span></code> value of <code class="docutils literal notranslate"><span class="pre">multi_process_hash_map</span></code> to the Hierarchical Parameter Server.
This database type supports sharing embeddings across process boundaries by using shared memory and the <code class="docutils literal notranslate"><span class="pre">/dev/shm</span></code> device file.
Multiple processes running HPS can read and write to the same hash map.
For an example, refer to the <a class="reference internal" href="notebooks/hps_demo.html"><span class="std std-doc">Hierarchcal Parameter Server Demo</span></a> notebook.</p></li>
<li><p><strong>Enhancements to the HPS Redis Backend</strong>:
In this release, the Hierarchical Parameter Server can open multiple connections in parallel to each Redis node.
This enhancement enables HPS to take advantage of overlapped processing optimizations in the I/O module of Redis servers.
In addition, HPS can now take advantage of Redis hash tags to co-locate embedding values and metadata.
This enhancement can reduce the number of accesses to Redis nodes and the number of per-node round trip communications that are needed to complete transactions.
As a result, the enhancement increases the insertion performance.</p></li>
<li><p><strong>MLPLayer is New</strong>:
This release adds an MLP layer with the <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.MLP</span></code> class.
This layer is very flexible and makes it easier to use a group of fused fully-connected layers and enable the related optimizations.
For each fused fully-connected layer in <code class="docutils literal notranslate"><span class="pre">MLPLayer</span></code>, the output dimension, bias, and activation function are all adjustable.
MLPLayer supports FP32, FP16 and TF32 data types.
For an example, refer to the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/v4.2/samples/dlrm/dgx_a100_mlp.py">dgx_a100_mlp.py</a> in the <code class="docutils literal notranslate"><span class="pre">samples/dlrm</span></code> directory of the GitHub repository to learn how to use the layer.</p></li>
<li><p><strong>Sparse Operation Kit installable from PyPi</strong>:
Version <code class="docutils literal notranslate"><span class="pre">1.1.4</span></code> of the Sparse Operation Kit is installable from PyPi in the <a class="reference external" href="https://pypi.org/project/merlin-sok/">merlin-sok</a> package.</p></li>
<li><p><strong>Multi-task Model Support added to the ONNX Model Converter</strong>:
This release adds support for multi-task models to the ONNX converter.
This release also includes an enhancement to the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/v4.2/samples/mmoe/preprocess_census.py">preprocess_census.py</a> script in <code class="docutils literal notranslate"><span class="pre">samples/mmoe</span></code> directory of the GitHub repository.</p></li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>Using the HPS Plugin for TensorFlow with <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> and running the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.2/hierarchical_parameter_server/notebooks/hierarchical_parameter_server_demo.html">Hierarchical Parameter Server Demo</a> notebook triggered an issue with <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext">ReplicaContext</a> and caused a crash.
The issue is fixed and resolves GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/362">issue #362</a>.</p></li>
<li><p>The <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/v4.2/samples/din/utils/4_nvt_process.py">4_nvt_process.py</a> sample in the <code class="docutils literal notranslate"><span class="pre">samples/din/utils</span></code> directory of the GitHub repository is updated to use the latest NVTabular API.
This update resolves GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/364">issue #364</a>.</p></li>
<li><p>An illegal memory access related to 3G embedding and the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v4.2/samples/dlrm/dgx_a100_ib_nvlink.py">dgx_a100_ib_nvlink.py</a> sample in the <code class="docutils literal notranslate"><span class="pre">samples/dlrm</span></code> directory of the GitHub repository is fixed.</p></li>
<li><p>An error in HPS with the <code class="docutils literal notranslate"><span class="pre">lookup_fromdlpack()</span></code> method is fixed.
The error was related to calculating the number of keys and vectors from the corresponding DLPack tensors.</p></li>
<li><p>An error in the HugeCTR backend for Triton Inference Server is fixed.
A crash was triggered when the initial size of the embedding cache is smaller than the allowed minimum size.</p></li>
<li><p>An error related to using a ReLU layer with an odd input size in mixed precision mode could trigger a crash.
The issue is fixed.</p></li>
<li><p>An error related to using an asynchronous reader with the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.2/api/python_interface.html?highlight=io_alignment#asyncparam-class">AsyncParam</a> class and specifying an <code class="docutils literal notranslate"><span class="pre">io_alignment</span></code> value that is smaller than the block device sector size is fixed.
Now, if the specified <code class="docutils literal notranslate"><span class="pre">io_alignment</span></code> value is smaller than the block device sector size, <code class="docutils literal notranslate"><span class="pre">io_alignment</span></code> is automatically set to the block device sector size.</p></li>
<li><p>Unreported memory leaks in the GRU layer and collectives are fixed.</p></li>
<li><p>Several broken documentation links related to HPS are fixed.</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also the NCCL <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">known issue</a> and the GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">issue</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and are reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
<li><p>Dumping Adam optimizer states to AWS S3 is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-4-1">
<h2>What’s New in Version 4.1<a class="headerlink" href="#what-s-new-in-version-4-1" title="Permalink to this heading"></a></h2>
<ul>
<li><p><strong>Simplified Interface for 3G Embedding Table Placement Strategy</strong>:
3G embedding now provides an easier way for you to configure an embedding table placement strategy.
Instead of using JSON, you can configure the embedding table placement strategy by using function arguments.
You only need to provide the <code class="docutils literal notranslate"><span class="pre">shard_matrix</span></code>, <code class="docutils literal notranslate"><span class="pre">table_group_strategy</span></code>, and <code class="docutils literal notranslate"><span class="pre">table_placement_strategy</span></code> arguments.
With these arguments, 3G embedding can group different tables together and place them according to the <code class="docutils literal notranslate"><span class="pre">shard_matrix</span></code> argument.
For an example, refer to <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v4.1/test/embedding_collection_test/dlrm_train.py">dlrm_train.py</a> file in the <code class="docutils literal notranslate"><span class="pre">test/embedding_collection_test</span></code> directory of the repository on GitHub.
For comparison, refer to the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v4.0/test/embedding_collection_test/dlrm_train.py">same file</a> from the v4.0 branch of the repository.</p></li>
<li><p><strong>New MMoE and Shared-Bottom Samples</strong>:
This release includes a new shared-bottom model, an example program, preprocessing scripts, and updates to documentation.
For more information, refer to the <code class="docutils literal notranslate"><span class="pre">README.md</span></code>, <code class="docutils literal notranslate"><span class="pre">mmoe_parquet.py</span></code>, and other files in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v4.1/samples/mmoe"><code class="docutils literal notranslate"><span class="pre">samples/mmoe</span></code></a> directory of the repository on GitHub.
This release also includes a fix to the calculation and reporting of AUC for multi-task models, such as MMoE.</p></li>
<li><p><strong>Support for AWS S3 File System</strong>:
The Parquet DataReader can now read datasets from the Amazon Web Services S3 file system.
You can also load and dump models from and to S3 during training.
The documentation for the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.1/api/python_interface.html#datasourceparams-class"><code class="docutils literal notranslate"><span class="pre">DataSourceParams</span></code></a> class is updated.
To view sample code, refer to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.1/notebooks/training_with_remote_filesystem.html">HugeCTR Training with Remote File System Example</a> class is updated.</p></li>
<li><p><strong>Simplification for File System Usage</strong>:
You no longer ’t need to pass <code class="docutils literal notranslate"><span class="pre">DataSourceParams</span></code> for model loading and dumping.
The <code class="docutils literal notranslate"><span class="pre">FileSystem</span></code> class automatically infers the correct file system type, local, HDFS, or S3, based on the path URI that you specified when you built the model.
For example, the path <code class="docutils literal notranslate"><span class="pre">hdfs://localhost:9000/</span></code> is inferred as an HDFS file system and the path <code class="docutils literal notranslate"><span class="pre">https://mybucket.s3.us-east-1.amazonaws.com/</span></code> is inferred as an S3 file system.</p></li>
<li><p><strong>Support for Loading Models from Remote File Systems to HPS</strong>:
This release enables you to load models from HDFS and S3 remote file systems to HPS during inference.
To use the new feature, specify an HDFS for S3 path URI in <code class="docutils literal notranslate"><span class="pre">InferenceParams</span></code>.</p></li>
<li><p><strong>Support for Exporting Intermediate Tensor Values into a Numpy Array</strong>:
This release adds function <code class="docutils literal notranslate"><span class="pre">check_out_tensor</span></code> to <code class="docutils literal notranslate"><span class="pre">Model</span></code> and <code class="docutils literal notranslate"><span class="pre">InferenceModel</span></code>.
You can use this function to check out the intermediate tensor values using the Python interface.
This function is especially helpful for debugging.
For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.1/api/python_interface.html#check-out-tensor-method"><code class="docutils literal notranslate"><span class="pre">Model.check_out_tensor</span></code></a> and <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.1/api/python_interface.html#id3"><code class="docutils literal notranslate"><span class="pre">InferenceModel.check_out_tensor</span></code></a>.</p></li>
<li><p><strong>On-Device Input Keys for HPS Lookup</strong>:
The HPS lookup supports input embedding keys that are on GPU memory during inference.
This enhancement removes a host-to-device copy by using the DLPack <code class="docutils literal notranslate"><span class="pre">lookup_fromdlpack()</span></code> interface.
By using the interface, the input DLPack capsule of embedding key can be a GPU tensor.</p></li>
<li><p><strong>Documentation Enhancements</strong>:</p>
<ul class="simple">
<li><p>The graphic for the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.1/hierarchical_parameter_server/index.html">Hierarchical Parameter Server</a> library that shows relationship to other software packages is enhanced.</p></li>
<li><p>The sample notebook for <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.1/hierarchical_parameter_server/notebooks/hps_tensorflow_triton_deployment_demo.html">Deploy SavedModel using HPS with Triton TensorFlow Backend</a> is added to the documentation.</p></li>
<li><p>Style updates to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.1/hierarchical_parameter_server/api/index.html">Hierarchical Parameter Server API</a> documentation.</p></li>
</ul>
</li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">InteractionLayer</span></code> class is fixed so that it works correctly with <code class="docutils literal notranslate"><span class="pre">num_feas</span> <span class="pre">&gt;</span> <span class="pre">30</span></code>.</p></li>
<li><p>The cuBLASLt configuration is corrected by increasing the workspace size and adding the epilogue mask.</p></li>
<li><p>The NVTabular based preprocessing script for our samples that demonstrate feature crossing is fixed.</p></li>
<li><p>The async data reader is fixed. Previously, it would hang and cause a corruption issue due to an improper I/O block size and I/O alignment problem.
The <code class="docutils literal notranslate"><span class="pre">AsyncParam</span></code> class is changed to implement the fix.
The <code class="docutils literal notranslate"><span class="pre">io_block_size</span></code> argument is replaced by the <code class="docutils literal notranslate"><span class="pre">max_nr_request</span></code> argument and the actual I/O block size that the async reader uses is computed accordingly.
For more information, refer to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.1/api/python_interface.html#asyncparam"><code class="docutils literal notranslate"><span class="pre">AsyncParam</span></code></a> class documentation.</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also the NCCL <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">known issue</a> and the GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">issue</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and are reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
<li><p>Dumping Adam optimizer states to AWS S3 is not supported.</p></li>
<li><p>Dumping to remote file systems when enabled MPI is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-4-0">
<h2>What’s New in Version 4.0<a class="headerlink" href="#what-s-new-in-version-4-0" title="Permalink to this heading"></a></h2>
<ul>
<li><p><strong>3G Embedding Stablization</strong>:
Since the introduction of the next generation of HugeCTR embedding in v3.7, several updates and enhancements were made, including code refactoring to improve usability.
The enhancements for this release are as follows:</p>
<ul class="simple">
<li><p>Optimized the performance for sparse lookup in terms of inter-warp load imbalance.
Sparse Operation Kit (SOK) takes advantage of the enhancement to improve performance.</p></li>
<li><p>This release includes a fix for determining the maximum embedding vector size in the <code class="docutils literal notranslate"><span class="pre">GlobalEmbeddingData</span></code> and <code class="docutils literal notranslate"><span class="pre">LocalEmbeddingData</span></code> classes.</p></li>
<li><p>Version 1.1.4 of Sparse Operation Kit can be installed with Pip and includes the enhancements mentioned in the preceding bullets.</p></li>
</ul>
</li>
<li><p><strong>Embedding Cache Initialization with Configurable Ratio</strong>:
In previous releases, the default value for the <code class="docutils literal notranslate"><span class="pre">cache_refresh_percentage_per_iteration</span></code> parameter of the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.0/hugectr_parameter_server.html#inference-parameters-and-embedding-cache-configuration">InferenceParams</a> was <code class="docutils literal notranslate"><span class="pre">0.1</span></code>.</p>
<p>In this release, default value is <code class="docutils literal notranslate"><span class="pre">0.0</span></code> and the parameter provides an additional purpose.
If you set the parameter to a value greater than <code class="docutils literal notranslate"><span class="pre">0.0</span></code> and also set <code class="docutils literal notranslate"><span class="pre">use_gpu_embedding_cache</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> for a model, when Hierarchical Parameter Server (HPS) starts, HPS initializes the embedding cache for the model on the GPU by loading a subset of the embedding vectors from the sparse files for the model.
When embedding cache initialization is used, HPS creates log records when it starts at the INFO level.
The logging records are similar to <code class="docutils literal notranslate"><span class="pre">EC</span> <span class="pre">initialization</span> <span class="pre">for</span> <span class="pre">model:</span> <span class="pre">&quot;&lt;model-name&gt;&quot;,</span> <span class="pre">num_tables:</span> <span class="pre">&lt;int&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">EC</span> <span class="pre">initialization</span> <span class="pre">on</span> <span class="pre">device:</span> <span class="pre">&lt;int&gt;</span></code>.
This enhancement reduces the duration of the warm up phase.</p>
</li>
<li><p><strong>Lazy Initialization of HPS Plugin for TensorFlow</strong>:
In this release, when you deploy a <code class="docutils literal notranslate"><span class="pre">SavedModel</span></code> of TensorFlow with Triton Inference Server, HPS is implicitly initialized when the loaded model is executed for the first time.
In previous releases, you needed to run <code class="docutils literal notranslate"><span class="pre">hps.Init(ps_config_file,</span> <span class="pre">global_batch_size)</span></code> explicitly.
For more information, see the API documentation for <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.0/hierarchical_parameter_server/api/initialize.html#hierarchical_parameter_server.Init"><code class="docutils literal notranslate"><span class="pre">hierarchical_parameter_server.Init</span></code></a>.</p></li>
<li><p><strong>Enhancements to the HDFS Backend</strong>:</p>
<ul class="simple">
<li><p>The HDFS Backend is now called IO::HadoopFileSystem.</p></li>
<li><p>This release includes fixes for memory leaks.</p></li>
<li><p>This release includes refactoring to generalize the interface for HDFS and S3 as remote filesystems.</p></li>
<li><p>For more information, see <code class="docutils literal notranslate"><span class="pre">hadoop_filesystem.hpp</span></code> in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v4.0/HugeCTR/include/io"><code class="docutils literal notranslate"><span class="pre">include/io</span></code></a> directory of the repository on GitHub.</p></li>
</ul>
</li>
<li><p><strong>Dependency Clarification for Protobuf and Hadoop</strong>:
Hadoop and Protobuf are true <code class="docutils literal notranslate"><span class="pre">third_party</span></code> modules now.
Developers can now avoid unnecessary and frequent cloning and deletion.</p></li>
<li><p><strong>Finer granularity control for overlap behavior</strong>:
We deperacated the old <code class="docutils literal notranslate"><span class="pre">overlapped_pipeline</span></code> knob and introduces four new knobs <code class="docutils literal notranslate"><span class="pre">train_intra_iteration_overlap</span></code>/<code class="docutils literal notranslate"><span class="pre">train_inter_iteration_overlap</span></code>/<code class="docutils literal notranslate"><span class="pre">eval_intra_iteration_overlap</span></code>/<code class="docutils literal notranslate"><span class="pre">eval_inter_iteration_overlap</span></code> to help user better control the overlap behavior. For more information, see the API documentation for <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.0/api/python_interface.html#createsolver-method"><code class="docutils literal notranslate"><span class="pre">Solver.CreateSolver</span></code></a></p></li>
<li><p><strong>Documentation Improvements</strong>:</p>
<ul class="simple">
<li><p>Removed two deprecated tutorials <code class="docutils literal notranslate"><span class="pre">triton_tf_deploy</span></code> and <code class="docutils literal notranslate"><span class="pre">dump_to_tf</span></code>.</p></li>
<li><p>Previously, the graphics in the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.0/performance.html">Performance</a> page did not appear.
This issue is fixed in this release.</p></li>
<li><p>Previously, the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v4.0/hierarchical_parameter_server/api/index.html">API documentation</a> for the HPS Plugin for TensorFlow did not show the class information. This issue is fixed in this release.</p></li>
</ul>
</li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>Fixed a build error that was triggered in debug mode.
The error was caused by the newly introduced 3G embedding unit tests.</p></li>
<li><p>When using the Parquet DataReader, if a parquet dataset file specified in <code class="docutils literal notranslate"><span class="pre">metadata.json</span></code> does not exist, HugeCTR no longer crashes.
The new behavior is to skip the missing file and display a warning message.
This change relates to GitHub issue <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/321">321</a>.</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also the NCCL <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">known issue</a> and the GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">issue</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and are reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-3-9">
<h2>What’s New in Version 3.9<a class="headerlink" href="#what-s-new-in-version-3-9" title="Permalink to this heading"></a></h2>
<ul>
<li><p><strong>Updates to 3G Embedding</strong>:</p>
<ul class="simple">
<li><p>Sparse Operation Kit (SOK) is updated to use the HugeCTR 3G embedding as a developer preview feature.
For more information, refer to the Python programs in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.9/sparse_operation_kit/sparse_operation_kit/experiment/benchmark/dlrm">sparse_operation_kit/experiment/benchmark/dlrm</a> directory of the repository on GitHub.</p></li>
<li><p>Dynamic embedding table mode is added.
The mode is based on the <a class="reference external" href="https://github.com/NVIDIA/cuCollections">cuCollection</a> with some functionality enhancement.
A dynamic embedding table grows its size when the table is full so that you no longer need to configure the memory usage information for embedding.
For more information, refer to the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.9/HugeCTR/embedding_storage/dynamic_embedding_table">embedding_storage/dynamic_embedding_storage</a> directory of the repository on GitHub.</p></li>
</ul>
</li>
<li><p><strong>Enhancements to the HPS Plugin for TensorFlow</strong>:
This release includes improvements to the interoperability of SOK and HPS.
The plugin now supports the sparse lookup layer.
The documentation for the HPS plugin is enhanced as follows:</p>
<ul class="simple">
<li><p>An <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.9/hierarchical_parameter_server/hps_tf_user_guide.html">introduction</a> to the plugin is new.</p></li>
<li><p>New <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.9/hierarchical_parameter_server/notebooks/index.html">notebooks</a> demonstrate how to use the HPS plugin are added.</p></li>
<li><p><a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.9/hierarchical_parameter_server/api/index.html">API documentation</a> for the plugin is new.</p></li>
</ul>
</li>
<li><p><strong>Enhancements to the HPS Backend for Triton Inference Server</strong>
This release adds support for integrating the <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/main/hps_backend">HPS Backend</a> and the <a class="reference external" href="https://github.com/triton-inference-server/tensorflow_backend">TensorFlow Backend</a> through the ensemble mode with Triton Inference Server.
The enhancement enables deploying a TensorFlow model with large embedding tables with Triton by leveraging HPS.
For more information, refer to the sample programs in the <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/main/hps_backend/samples/hps-triton-ensemble">hps-triton-ensemble</a> directory of the HugeCTR Backend repository in GitHub.</p></li>
<li><p><strong>New Multi-Node Tutorial</strong>:
The <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.9/tutorial/multinode-training">multi-node training tutorial</a> is new.
The additions show how to use HugeCTR to train a model with multiple nodes and is based on our most recent Docker container.
The tutorial should be useful to users who do not have a job-scheduler-installed cluster such as Slurm Workload Manager.
The update addresses a issue that was first reported in GitHub issue <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/305">305</a>.</p></li>
<li><p><strong>Support Offline Inference for MMoE</strong>:
This release includes MMoE offline inference where both per-class AUC and average AUC are provided.
When the number of class AUCs is greater than one, the output includes a line like the following example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[HCTR][08:52:59.254][INFO][RK0][main]: Evaluation, AUC: {0.482141, 0.440781}, macro-averaging AUC: 0.46146124601364136</span>
</pre></div>
</div>
</li>
<li><p><strong>Enhancements to the API for the HPS Database Backend</strong>
This release includes several enhancements to the API for the <code class="docutils literal notranslate"><span class="pre">DatabaseBackend</span></code> class.
For more information, see <code class="docutils literal notranslate"><span class="pre">database_backend.hpp</span></code> and the header files for other database backends in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.9/HugeCTR/include/hps"><code class="docutils literal notranslate"><span class="pre">HugeCTR/include/hps</span></code></a> directory of the repository.
The enhancements are as follows:</p>
<ul class="simple">
<li><p>You can now specify a maximum time budget, in nanoseconds, for queries so that you can build an application that must operate within strict latency limits.
Fetch queries return execution control to the caller if the time budget is exhausted.
The unprocessed entries are indicated to the caller through a callback function.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">dump</span></code> and <code class="docutils literal notranslate"><span class="pre">load_dump</span></code> methods are new.
These methods support saving and loading embedding tables from disk.
The methods support a custom binary format and the RocksDB SST table file format.
These methods enable you to import and export embedding table data between your custom tools and HugeCTR.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">find_tables</span></code> method is new.
The method enables you to discover all table data that is currently stored for a model in a <code class="docutils literal notranslate"><span class="pre">DatabaseBackend</span></code> instance.
A new overloaded method for <code class="docutils literal notranslate"><span class="pre">evict</span></code> is added that can process the results from <code class="docutils literal notranslate"><span class="pre">find_tables</span></code> to quickly and simply drop all the stored information that is related to a model.</p></li>
</ul>
</li>
<li><p><strong>Documentation Enhancements</strong></p>
<ul class="simple">
<li><p>The documentation for the <code class="docutils literal notranslate"><span class="pre">max_all_to_all_bandwidth</span></code> parameter of the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.9/api/python_interface.html#hybridembeddingparam-class"><code class="docutils literal notranslate"><span class="pre">HybridEmbeddingParam</span></code></a> class is clarified to indicate that the bandwidth unit is per-GPU.
Previously, the unit was not specified.</p></li>
</ul>
</li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>Hybrid embedding with <code class="docutils literal notranslate"><span class="pre">IB_NVLINK</span></code> as the <code class="docutils literal notranslate"><span class="pre">communication_type</span></code> of the
<a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.9/api/python_interface.html#hybridembeddingparam-class"><code class="docutils literal notranslate"><span class="pre">HybridEmbeddingParam</span></code></a>
is fixed in this release.</p></li>
<li><p>Training performance is affected by a GPU routine that checks if an input key can be out of the embedding table. If you can guarantee that the input keys can work with the specified <code class="docutils literal notranslate"><span class="pre">workspace_size_per_gpu_in_mb</span></code>, we have a workaround to disable the routine by setting the environment variable <code class="docutils literal notranslate"><span class="pre">HUGECTR_DISABLE_OVERFLOW_CHECK=1</span></code>. The workaround restores the training performance.</p></li>
<li><p>Engineering discovered and fixed a correctness issue with the Softmax layer.</p></li>
<li><p>Engineering removed an inline profiler that was rarely used or updated. This change relates to GitHub issue <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/340">340</a>.</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also the NCCL <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">known issue</a> and the GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">issue</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and are reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-3-8">
<h2>What’s New in Version 3.8<a class="headerlink" href="#what-s-new-in-version-3-8" title="Permalink to this heading"></a></h2>
<ul>
<li><p><strong>Sample Notebook to Demonstrate 3G Embedding</strong>:
This release includes a sample notebook that introduces the Python API of the
embedding collection and the key concepts for using 3G embedding.
You can view <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.8/notebooks/embedding_collection.html">HugeCTR Embedding Collection</a>
from the documentation or access the <code class="docutils literal notranslate"><span class="pre">embedding_collection.ipynb</span></code> file from the
<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.8/notebooks/"><code class="docutils literal notranslate"><span class="pre">notebooks</span></code></a>
directory of the repository.</p></li>
<li><p><strong>DLPack Python API for Hierarchical Parameter Server Lookup</strong>:
This release introduces support for embedding lookup from the Hierarchical
Parameter Server (HPS) using the DLPack Python API.  The new method is
<code class="docutils literal notranslate"><span class="pre">lookup_fromdlpack()</span></code>.  For sample usage, see the
<a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.8/notebooks/hps_demo.html#lookup-the-embedding-vector-from-dlpack">Lookup the Embedding Vector from DLPack</a>
heading in the “Hierarchical Parameter Server Demo” notebook.</p></li>
<li><p><strong>Read Parquet Datasets from HDFS with the Python API</strong>:
This release enhances the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.8/api/python_interface.html#datareaderparams"><code class="docutils literal notranslate"><span class="pre">DataReaderParams</span></code></a>
class with a <code class="docutils literal notranslate"><span class="pre">data_source_params</span></code> argument. You can use the argument to specify
the data source configuration such as the host name of the Hadoop NameNode and the NameNode port number to read from HDFS.</p></li>
<li><p><strong>Logging Performance Improvements</strong>:
This release includes a performance enhancement that reduces the performance impact of logging.</p></li>
<li><p><strong>Enhancements to Layer Classes</strong>:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">FullyConnected</span></code> layer now supports 3D inputs</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">MatrixMultiply</span></code> layer now supports 4D inputs.</p></li>
</ul>
</li>
<li><p><strong>Documentation Enhancements</strong>:</p>
<ul class="simple">
<li><p>An automatically generated table of contents is added to the top of most
pages in the web documentation. The goal is to provide a better experience
for navigating long pages such as the
<a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.8/api/hugectr_layer_book.html">HugeCTR Layer Classes and Methods</a>
page.</p></li>
<li><p>URLs to the Criteo 1TB click logs dataset are updated. For an example, see the
<a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.8/notebooks/hugectr_wdl_prediction.html">HugeCTR Wide and Deep Model with Criteo</a>
notebook.</p></li>
</ul>
</li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>The data generator for the Parquet file type is fixed and produces consistent file names between the <code class="docutils literal notranslate"><span class="pre">_metadata.json</span></code> file and the actual dataset files.
Previously, running the data generator to create synthetic data resulted in a core dump.
This issue was first reported in the GitHub issue <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/321">321</a>.</p></li>
<li><p>Fixed the memory crash in running a large model on multiple GPUs that occurred during AUC warm up.</p></li>
<li><p>Fixed the issue of keyset generation in the ETC notebook.
Refer to the GitHub issue <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/332">332</a> for more details.</p></li>
<li><p>Fixed the inference build error that occurred when building with debug mode.</p></li>
<li><p>Fixed the issue that multi-node training prints duplicate messages.</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>Hybrid embedding with <code class="docutils literal notranslate"><span class="pre">IB_NVLINK</span></code> as the <code class="docutils literal notranslate"><span class="pre">communication_type</span></code> of the
<a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.8/api/python_interface.html#hybridembeddingparam-class"><code class="docutils literal notranslate"><span class="pre">HybridEmbeddingParam</span></code></a>
class does not work currently. We are working on fixing it. The other communication types have no issues.</p></li>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also the NCCL <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">known issue</a> and the GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">issue</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and are reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-3-7">
<h2>What’s New in Version 3.7<a class="headerlink" href="#what-s-new-in-version-3-7" title="Permalink to this heading"></a></h2>
<ul>
<li><p><strong>3G Embedding Developer Preview</strong>:
The 3.7 version introduces next-generation of embedding as a developer preview feature. We call it 3G embedding because it is the new update to the HugeCTR embedding interface and implementation since the unified embedding in v3.1 version, which was the second one.
Compared with the previous embedding, there are three main changes in the embedding collection.</p>
<ul class="simple">
<li><p>First, it allows users to fuse embedding tables with different embedding vector sizes. The previous embedding can only fuse embedding tables with the same embedding vector size.
The enhancement boosts both flexibility and performance.</p></li>
<li><p>Second, it extends the functionality of embedding by supporting the <code class="docutils literal notranslate"><span class="pre">concat</span></code> combiner and supporting different slot lookup on the same embedding table.</p></li>
<li><p>Finally, the embedding collection is powerful enough to support arbitrary embedding table placement which includes data parallel and model parallel.
By providing a plan JSON file, you can configure the table placement strategy as you specify.
See the <code class="docutils literal notranslate"><span class="pre">dlrm_train.py</span></code> file in the  <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.7/test/embedding_collection_test">embedding_collection_test</a> directory of the repository for a more detailed usage example.</p></li>
</ul>
</li>
<li><p><strong>HPS Performance Improvements</strong>:</p>
<ul class="simple">
<li><p><strong>Kafka</strong>: Model parameters are now stored in Kafka in a bandwidth-saving multiplexed data format.
This data format vastly increases throughput. In our lab, we measured transfer speeds up to 1.1 Gbps for each Kafka broker.</p></li>
<li><p><strong>HashMap backend</strong>: Parallel and single-threaded hashmap implementations have been replaced by a new unified implementation.
This new implementation uses a new memory-pool based allocation method that vastly increases upsert performance without diminishing recall performance.
Compared with the previous implementation, you can expect a 4x speed improvement for large-batch insertion operations.</p></li>
<li><p><strong>Suppressed and simplified log</strong>: Most log messages related to HPS have the log level changed to <code class="docutils literal notranslate"><span class="pre">TRACE</span></code> rather than <code class="docutils literal notranslate"><span class="pre">INFO</span></code> or <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code> to reduce logging verbosity.</p></li>
</ul>
</li>
<li><p><strong>Offline Inference Usability Enhancements</strong>:</p>
<ul class="simple">
<li><p>The thread pool size is configurable in the Python interface, which is useful for studying the embedding cache performance in scenarios of asynchronous update. Previously it was set as the minimum value of 16 and <code class="docutils literal notranslate"><span class="pre">std::thread::hardware_concurrency()</span></code>. For more information, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.7/hugectr_parameter_server.html#configuration">Hierarchical Parameter Server Configuration</a>.</p></li>
</ul>
</li>
<li><p><strong>DataGenerator Performance Improvements</strong>:
You can specify the <code class="docutils literal notranslate"><span class="pre">num_threads</span></code> parameter to parallelize a <code class="docutils literal notranslate"><span class="pre">Norm</span></code> dataset generation.</p></li>
<li><p><strong>Evaluation Metric Improvements</strong>:</p>
<ul class="simple">
<li><p>Average loss performance improvement in multi-node environments.</p></li>
<li><p>AUC performance optimization and safer memory management.</p></li>
<li><p>Addition of NDCG and SMAPE.</p></li>
</ul>
</li>
<li><p><strong>Embedding Training Cache Parquet Demo</strong>:
Created a keyset extractor script to generate keyset files for Parquet datasets.
Provided users with an end-to-end demo of how to train a Parquet dataset using the embedding cache mode.
See the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.7/notebooks/embedding_training_cache_example.html">Embedding Training Cache Example</a> notebook.</p></li>
<li><p><strong>Documentation Enhancements</strong>:
The documentation details for <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.7/hugectr_parameter_server.html">HugeCTR Hierarchical Parameter Server Database Backend</a> are updated for consistency and clarity.</p></li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">slot_size_array</span></code> is specified, <code class="docutils literal notranslate"><span class="pre">workspace_size_per_gpu_in_mb</span></code> is no longer required.</p></li>
<li><p>If you build and install HugeCTR from scratch, you can specify the <code class="docutils literal notranslate"><span class="pre">CMAKE_INSTALL_PREFIX</span></code> CMake variable to identify the installation directory for HugeCTR.</p></li>
<li><p>Fixed SOK hang issue when calling <code class="docutils literal notranslate"><span class="pre">sok.Init()</span></code> with a large number of GPUs. See the GitHub issue <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/261">261</a> and <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/302">302</a> for more details.</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also the NCCL <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">known issue</a> and the GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">issue</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and are reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
<li><p>The Criteo 1 TB click logs dataset that is used with many HugeCTR sample programs and notebooks is currently unavailable.
Until the dataset becomes downloadable again, you can run those samples based on our synthetic dataset generator.
For more information, see the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR#getting-started">Getting Started</a> section of the repository README file.</p></li>
<li><p>Data generator of parquet type produces inconsistent file names between _metadata.json and actual dataset files, which will result in core dump fault when using the synthetic dataset.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-3-6">
<h2>What’s New in Version 3.6<a class="headerlink" href="#what-s-new-in-version-3-6" title="Permalink to this heading"></a></h2>
<ul>
<li><p><strong>Concat 3D Layer</strong>:
In previous releases, the <code class="docutils literal notranslate"><span class="pre">Concat</span></code> layer could handle two-dimensional (2D) input tensors only.
Now, the input can be three-dimensional (3D) and you can concatenate the inputs along axis 1 or 2.
For more information, see the API documentation for the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.6/api/hugectr_layer_book.html#concat-layer">Concat Layer</a>.</p></li>
<li><p><strong>Dense Column List Support in Parquet DataReader</strong>:
In previous releases, HugeCTR assumes each dense feature has a single value and it must be the scalar data type <code class="docutils literal notranslate"><span class="pre">float32</span></code>.
Now, you can mix <code class="docutils literal notranslate"><span class="pre">float32</span></code> or <code class="docutils literal notranslate"><span class="pre">list[float32]</span></code> for dense columns.
This enhancement means that each dense feature can have more than one value.
For more information, see the API documentation for the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.6/api/python_interface.html#parquet">Parquet</a> dataset format.</p></li>
<li><p><strong>Support for HDFS is Re-enabled in Merlin Containers</strong>:
Support for HDFS in Merlin containers is an optional dependency now.
For more information, see <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.6/hugectr_core_features.html#hdfs-support">HDFS Support</a>.</p></li>
<li><p><strong>Evaluation Metric Enhancements</strong>:
In previous releases, HugeCTR computes AUC for binary classification only.
Now, HugeCTR supports AUC for multi-label classification.
The implementation is inspired by <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html">sklearn.metrics.roc_auc_score</a> and performs the unweighted macro-averaging strategy that is the default for scikit-learn.
You can specify a value for the <code class="docutils literal notranslate"><span class="pre">label_dim</span></code> parameter of the input layer to enable multi-label classification and HugeCTR will compute the multi-label AUC.</p></li>
<li><p><strong>Log Output Format Change</strong>:
The default log format now includes milliseconds.</p></li>
<li><p><strong>Documentation Enhancements</strong>:</p>
<ul class="simple">
<li><p>These release notes are included in the documentation and are available at <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.6/release_notes.html">https://nvidia-merlin.github.io/HugeCTR/v3.6/release_notes.html</a>.</p></li>
<li><p>The <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.6/hugectr_parameter_server.html#configuration">Configuration</a> section of the Hierarchical Parameter Server information is updated with more information about the parameters in the configuration file.</p></li>
<li><p>The example notebooks that demonstrate how to work with multi-modal data are reorganized in the navigation.
The notebooks are now available under the heading <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.6/notebooks/multi-modal-data/index.html">Multi-Modal Example Notebooks</a>.
This change is intended to make it easier to find the notebooks.</p></li>
<li><p>The documentation in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.6/sparse_operation_kit">sparse_operation_kit</a> directory of the repository on GitHub is updated with several clarifications about SOK.</p></li>
</ul>
</li>
<li><p><strong>Issues Fixed</strong>:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">dlrm_kaggle_fp32.py</span></code> file in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.6/samples/dlrm"><code class="docutils literal notranslate"><span class="pre">samples/dlrm/</span></code></a> directory of the repository is updated to show the correct number of samples.
The <code class="docutils literal notranslate"><span class="pre">num_samples</span></code> value is now set to <code class="docutils literal notranslate"><span class="pre">36672493</span></code>.
This fixes GitHub issue <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/301">301</a>.</p></li>
<li><p>Hierarchical Parameter Server (HPS) would produce a runtime error when the GPU cache was turned off.
This issue is now fixed.</p></li>
</ul>
</li>
<li><p><strong>Known Issues</strong>:</p>
<ul>
<li><p>HugeCTR uses NCCL to share data between ranks and NCCL can require shared system memory for IPC and pinned (page-locked) system memory resources.
If you use NCCL inside a container, increase these resources by specifying the following arguments when you start the container:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>-shm-size<span class="o">=</span>1g<span class="w"> </span>-ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1
</pre></div>
</div>
<p>See also the NCCL <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">known issue</a> and the GitHub <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">issue</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KafkaProducers</span></code> startup succeeds even if the target Kafka broker is unresponsive.
To avoid data loss in conjunction with streaming-model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers are running, operating properly, and are reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be greater than or equal to the number of data reader workers.
Otherwise, different workers are mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint loss training with a regularizer is not supported.</p></li>
<li><p>The Criteo 1 TB click logs dataset that is used with many HugeCTR sample programs and notebooks is currently unavailable.
Until the dataset becomes downloadable again, you can run those samples based on our synthetic dataset generator.
For more information, see the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR#getting-started">Getting Started</a> section of the repository README file.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-3-5">
<h2>What’s New in Version 3.5<a class="headerlink" href="#what-s-new-in-version-3-5" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>HPS interface encapsulation and exporting as library</strong>: We encapsulate the Hierarchical Parameter Server(HPS) interfaces and deliver it as a standalone library. Besides, we prodvide HPS Python APIs and demonstrate the usage with a notebook. For more information, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.5/hugectr_parameter_server.html">Hierarchical Parameter Server</a> and <a class="reference internal" href="notebooks/hps_demo.html"><span class="std std-doc">HPS Demo</span></a>.</p></li>
<li><p><strong>Hierarchical Parameter Server Triton Backend</strong>: The HPS Backend is a framework for embedding vectors looking up on large-scale embedding tables that was designed to effectively use GPU memory to accelerate the looking up by decoupling the embedding tables and embedding cache from the end-to-end inference pipeline of the deep recommendation model. For more information, please refer to the <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/v3.5/samples">samples</a> directory of the HugeCTR backend for Triton Inference Server repository.</p></li>
<li><p><strong>SOK pip release</strong>: SOK pip releases on <a class="reference external" href="https://pypi.org/project/merlin-sok/">https://pypi.org/project/merlin-sok/</a>. Now users can install SOK via <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">merlin-sok</span></code>.</p></li>
<li><p><strong>Joint loss and multi-tasks training support:</strong>: We support joint loss in training so that users can train with multiple labels and tasks with different weights. See the MMoE sample in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.5/samples/mmoe">samples/mmoe</a> directory of the repository to learn the usage.</p></li>
<li><p><strong>HugeCTR documentation on web page</strong>: Now users can visit our <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/">web documentation</a>.</p></li>
<li><p><strong>ONNX converter enhancement:</strong>: We enable converting <code class="docutils literal notranslate"><span class="pre">MultiCrossEntropyLoss</span></code> and <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> layers to ONNX to support multi-label inference. For more information, please refer to the HugeCTR to ONNX Converter information in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.5/onnx_converter">onnx_converter</a> directory of the repository.</p></li>
<li><p><strong>HDFS python API enhancement</strong>:</p>
<ul>
<li><p>Simplified <code class="docutils literal notranslate"><span class="pre">DataSourceParams</span></code> so that users do not need to provide all the paths before they are really necessary. Now users only have to pass <code class="docutils literal notranslate"><span class="pre">DataSourceParams</span></code> once when creating a solver.</p></li>
<li><p>Later paths will be automatically regarded as local paths or HDFS paths depending on the <code class="docutils literal notranslate"><span class="pre">DataSourceParams</span></code> setting. See <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/v3.5/notebooks/training_with_hdfs.html">notebook</a> for usage.</p></li>
</ul>
</li>
<li><p><strong>HPS performance optimization</strong>: We use better method to  determine partition number in database backends in HPS.</p></li>
<li><p><strong>Issues Fixed</strong>: HugeCTR input layer now can take dense_dim greater than 1000.</p></li>
</ul>
</section>
<section id="what-s-new-in-version-3-4-1">
<h2>What’s New in Version 3.4.1<a class="headerlink" href="#what-s-new-in-version-3-4-1" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Support mixed precision inference for dataset with multiple labels</strong>: We enable FP16 for the <code class="docutils literal notranslate"><span class="pre">Softmax</span></code> layer and support mixed precision for multi-label inference. For more information, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#inference-api">Inference API</a>.</p></li>
<li><p><strong>Support multi-GPU offline inference with Python API</strong>: We support multi-GPU offline inference with the Python interface, which can leverage <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hugectr_parameter_server.html">Hierarchical Parameter Server</a> and enable concurrent execution on multiple devices. For more information, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#inference-api">Inference API</a> and <a class="reference internal" href="#notebooks/multi_gpu_offline_inference.ipynb"><span class="xref myst">Multi-GPU Offline Inference Notebook</span></a>.</p></li>
<li><p><strong>Introduction to metadata.json</strong>: We add the introduction to <code class="docutils literal notranslate"><span class="pre">_metadata.json</span></code> for Parquet datasets. For more information, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/index.html">Parquet</a>.</p></li>
<li><p><strong>Documents and tool for workspace size per GPU estimation</strong>: we add a tool that is named the <code class="docutils literal notranslate"><span class="pre">embedding_workspace_calculator</span></code> to help calculate the value for <code class="docutils literal notranslate"><span class="pre">workspace_size_per_gpu_in_mb</span></code> that is required by hugectr.SparseEmbedding. For more information, please refer to the README.md file in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.4.1/tools/embedding_workspace_calculator">tools/embedding_workspace_calculator</a> directory of the repository and <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/QAList.html">QA 24</a> in the documentation.</p></li>
<li><p><strong>Improved Debugging Capability</strong>: The old logging system, which was flagged as deprecated for some time has been removed. All remaining log messages and outputs have been revised and migrated to the new logging system (core23/logging.hpp/cpp). During this revision, we also adjusted log levels for log messages throughout the entire codebase to improve visibility of relevant information.</p></li>
<li><p><strong>Support HDFS Parameter Server in Training</strong>:</p>
<ul>
<li><p>Decoupled HDFS in Merlin containers to make the HDFS support more flexible. Users can now compile HDFS related functionalities optionally.</p></li>
<li><p>Now supports loading and dumping models and optimizer states from HDFS.</p></li>
<li><p>Added a <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.4.1/notebooks/training_with_hdfs.ipynb">notebook</a> to show how to use HugeCTR with HDFS.</p></li>
</ul>
</li>
<li><p><strong>Support Multi-hot Inference on Hugectr Backend</strong>: We support categorical input in multi-hot format for HugeCTR Backend inference.</p></li>
<li><p><strong>Multi-label inference with mixed precision</strong>: Mixed precision training is enabled for softmax layer.</p></li>
<li><p><strong>Python Script and documentation demonstrating how to analyze model files</strong>: In this release, we provide a script to retrieve vocabulary information from model file. Please find more details on the README in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.4.1/tools/model_analyzer">tools/model_analyzer</a> directory of the repository.</p></li>
<li><p><strong>Issues fixed</strong>:</p>
<ul>
<li><p>Mirror strategy bug in SOK (see in <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/291">https://github.com/NVIDIA-Merlin/HugeCTR/issues/291</a>)</p></li>
<li><p>Can’t import sparse operation kit in nvcr.io/nvidia/merlin/merlin-tensorflow-training:22.04 (see in <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/296">https://github.com/NVIDIA-Merlin/HugeCTR/issues/296</a>)</p></li>
<li><p>HPS: Fixed access violation that can occur during initialization when not configuring a volatile DB.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-3-4">
<h2>What’s New in Version 3.4<a class="headerlink" href="#what-s-new-in-version-3-4" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Support for Building HugeCTR with the Unified Merlin Container</strong>: HugeCTR can now be built using our unified Merlin container. For more information, refer to our <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hugectr_contributor_guide.html">Contributor Guide</a>.</p></li>
<li><p><strong>Hierarchical Parameter Server (HPS) Enhancements</strong>:</p>
<ul>
<li><p><strong>New Missing Key (Embedding Table Entries) Insertion Feature</strong>: Using a simple flag, it is now possible to configure HugeCTR with missing keys (embedding table entries). During lookup, these missing keys will automatically be inserted into volatile database layers such as the Redis and Hashmap backends.</p></li>
<li><p><strong>Asynchronous Timestamp Refresh</strong>: To allow time-based eviction to take place, it is now possible to enable timestamp refreshing for frequently used embeddings. Once enabled, refreshing is handled asynchronously using background threads, which won’t block your inference jobs. For most applications, the associated performance impact from enabling this feature is barely noticeable.</p></li>
<li><p><strong>HDFS (Hadoop Distributed File System) Parameter Server Support During Training</strong>:</p>
<ul>
<li><p>We’re introducing a new DataSourceParams API, which is a python API that can be used to specify the file system and paths to data and model files.</p></li>
<li><p>We’ve added support for loading data from HDFS to the local file system for HugeCTR training.</p></li>
<li><p>We’ve added support for dumping trained model and optimizer states into HDFS.</p></li>
</ul>
</li>
<li><p><strong>New Load API Capabilities</strong>: In addition to being able to deploy new models, the HugeCTR Backend’s <a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md#load">Load API</a> can now be used to update the dense parameters for models and corresponding embedding inference cache online.</p></li>
</ul>
</li>
<li><p><strong>Sparse Operation Kit (SOK) Enhancements</strong>:</p>
<ul>
<li><p><strong>Mixed Precision Training</strong>: Enabling mixed precision training using TensorFlow’s pattern to enhance the training performance and lessen memory usage is now possible.</p></li>
<li><p><strong>DLRM Benchmark</strong>: DLRM is a standard benchmark for recommendation model training, so we added a new notebook. Refer to the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.4/sparse_operation_kit/documents/tutorials/DLRM_Benchmark">sparse_operation_kit/documents/tutorials/DLRM_Benchmark</a> directory of the repository. The notebook shows how to address the performance of SOK on this benchmark.</p></li>
<li><p><strong>Uint32_t / int64_t key dtype Support in SOK</strong>: Int64 or uint32 can now be used as the key data type for SOK’s embedding. Int64 is the default.</p></li>
<li><p><strong>TensorFlow Initializers Support</strong>: We now support the TensorFlow native initializer within SOK, such as <code class="docutils literal notranslate"><span class="pre">sok.All2AllDenseEmbedding(embedding_initializer=tf.keras.initializers.RandomUniform())</span></code>. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/embeddings/dense/all2all.html">All2All Dense Embedding</a>.</p></li>
</ul>
</li>
<li><p><strong>Documentation Enhancements</strong></p>
<ul>
<li><p>We’ve revised several of our notebooks and readme files to improve readability and accessibility.</p></li>
<li><p>We’ve revised the SOK docker setup instructions to indicate that HugeCTR setup issues can be resolved using the <code class="docutils literal notranslate"><span class="pre">--shm-size</span></code> setting within docker.</p></li>
<li><p>Although HugeCTR is designed for scalability, having a robust machine is not necessary for smaller workloads and testing. We’ve documented the required specifications for notebook testing environments. For more information, refer to our <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/notebooks/index.html#system-specifications">README for HugeCTR Jupyter Demo Notebooks</a>.</p></li>
</ul>
</li>
<li><p><strong>Inference Enhancements</strong>：We now support HugeCTR inference for managing multiple tasks. When the label dimension is the number of binary classification tasks and <code class="docutils literal notranslate"><span class="pre">MultiCrossEntropyLoss</span></code> is employed during training, the shape of inference results will be <code class="docutils literal notranslate"><span class="pre">(batch_size*num_batches,</span> <span class="pre">label_dim)</span></code>. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#inference-api">Inference API</a>.</p></li>
<li><p><strong>Embedding Cache Issue Resolution</strong>: The embedding cache issue for very small embedding tables has been resolved.</p></li>
</ul>
</section>
<section id="what-s-new-in-version-3-3-1">
<h2>What’s New in Version 3.3.1<a class="headerlink" href="#what-s-new-in-version-3-3-1" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Hierarchical Parameter Server (HPS) Enhancements</strong>:</p>
<ul>
<li><p><strong>HugeCTR Backend Enhancements</strong>: The HugeCTR Backend is now fully compatible with the <a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md">Triton model control protocol</a>, so new model configurations can be simply added to the <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend#independent-inference-hierarchical-parameter-server-configuration">HPS configuration file</a>. The HugeCTR Backend will continue to support online deployments of new models using the Triton Load API. However, with this enhancement, old models can be recycled online using the <a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md#unload">Triton Unload API</a>.</p></li>
<li><p><strong>Simplified Database Backend</strong>: Multi-nodes, single-node, and all other kinds of volatile database backends can now be configured using the same configuration object.</p></li>
<li><p><strong>Multi-Threaded Optimization of Redis Code</strong>: The speedup of HugeCTR version 3.3.1 is 2.3 times faster than HugeCTR version 3.3.</p></li>
<li><p><strong>Additional HPS Enhancements and Fixes</strong>:</p>
<ul>
<li><p>You can now build the HPS test environment and implement unit tests for each component.</p></li>
<li><p>You’ll no longer encounter the access violation issue when updating Apache Kafka online.</p></li>
<li><p>The parquet data reader no longer incorrectly parses the index of categorical features when multiple embedded tables are being used.</p></li>
<li><p>The HPS Redis Backend overflow is now invoked during single insertions.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>New GroupDenseLayer</strong>: We’re introducing a new GroupDenseLayer. It can be used to group fused fully connected layers when constructing the model graph. A simplified Python interface is provided for adjusting the number of layers and specifying the output dimensions in each layer, which makes it easy to leverage the highly-optimized fused fully connected layers in HugeCTR. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/hugectr_layer_book.html#groupdenselayer">GroupDenseLayer</a>.</p></li>
<li><p><strong>Global Fixes</strong>:</p>
<ul>
<li><p>A warning message now appears when attempting to launch a multi-process job before importing the mpi.</p></li>
<li><p>When running with embedding training cache, a massive log is no longer generated.</p></li>
<li><p>Legacy conda information has been removed from the HugeCTR documentation.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-s-new-in-version-3-3">
<h2>What’s New in Version 3.3<a class="headerlink" href="#what-s-new-in-version-3-3" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Hierarchical Parameter Server (HPS) Enhancements</strong>:</p>
<ul>
<li><p><strong>Support for Incremental Model Updates</strong>: HPS now supports incremental model updates via Apache Kafka (a distributed event streaming platform) message queues. With this enhancement, HugeCTR can now be connected with Apache Kafka deployments to update models in real time during training and inference. For more information, refer to the <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/main/samples/hierarchical_deployment/hps_e2e_demo">Demo Notebok</a>.</p></li>
<li><p><strong>Improvements to the Memory Management</strong>: The Redis cluster and CPU memory database backends are now the primary sources for memory management. When performing incremental model updates, these memory database backends will automatically evict infrequently used embeddings as training progresses. The performance of the Redis cluster and CPU memory database backends have also been improved.</p></li>
<li><p><strong>New Asynchronous Refresh Mechanism</strong>: Support for asynchronous refreshing of incremental embedding keys into the embedding cache has been added. The Refresh operation will be triggered when completing the model version iteration or outputting incremental parameters from online training. The Distributed Database and Persistent Database will be updated by Apache Kafka. The GPU embedding cache will then refresh the values of the existing embedding keys and replace them with the latest incremental embedding vectors. For more information, refer to the <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend#hugectr-inference-hierarchical-parameter-server">HPS README</a>.</p></li>
<li><p><strong>Configurable Backend Implementations for Databases</strong>: Backend implementations for databases are now fully configurable.</p></li>
<li><p><strong>Improvements to the JSON Interface Parser</strong>: The JSON interface parser can now handle inaccurate parameterization.</p></li>
<li><p><strong>More Meaningful Jabber</strong>: As requested, we’ve revised the log levels throughout the entire API database backend of the HPS. Selected configuration options are now printed entirely and uniformly to the log. Errors provide more verbose information about pending issues.</p></li>
</ul>
</li>
<li><p><strong>Sparse Operation Kit (SOK) Enhancements</strong>:</p>
<ul>
<li><p><strong>TensorFlow (TF) 1.15 Support</strong>: SOK can now be used with TensorFlow 1.15. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/get_started/get_started.html#tensorflow-1-15">README</a>.</p></li>
<li><p><strong>Dedicated CUDA Stream</strong>: A dedicated CUDA stream is now used for SOK’s Ops, so this may help to eliminate kernel interleaving.</p></li>
<li><p><strong>New pip Installation Option</strong>: SOK can now be installed using the <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">SparseOperationKit</span></code> command. See more in our <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/intro_link.html#installation">instructions</a>). With this install option, root access to compile SOK is no longer required and python scripts don’t need to be copied.</p></li>
<li><p><strong>Visible Device Configuration Support</strong>：<code class="docutils literal notranslate"><span class="pre">tf.config.set_visible_device</span></code> can now be used to set visible GPUs for each process. <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> can also be used. When <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> is used, the <code class="docutils literal notranslate"><span class="pre">tf.config.set_visible_device</span></code> argument shouldn’t be set.</p></li>
</ul>
</li>
<li><p><strong>Hybrid-embedding indices pre-computing</strong>：The indices needed for hybrid embedding are pre-computed ahead of time and are overlapped with previous iterations.</p></li>
<li><p><strong>Cached evaluation indices:</strong>：The hybrid-embedding indices for eval are cached when applicable, hence eliminating the re-computing of the indices at every eval iteration.</p></li>
<li><p><strong>MLP weight/data gradients calculation overlap:</strong>：The weight gradients of MLP are calculated asynchronously with respect to the data gradients, enabling overlap between these two computations.</p></li>
<li><p><strong>Better compute-communication overlap:</strong>：Better overlap between compute and communication has been enabled to improve training throughput.</p></li>
<li><p><strong>Fused weight conversion:</strong>：The FP32-to-FP16 conversion of the weights are now fused into the SGD optimizer, saving trips to memory.</p></li>
<li><p><strong>GraphScheduler:</strong>：GrapScheduler was added to control the timing of cudaGraph launching. With GraphScheduler, the gap between adjacent cudaGraphs is eliminated.</p></li>
<li><p><strong>Multi-Node Training Support Enhancements</strong>：You can now perform multi-node training on the cluster with non-RDMA hardware by setting the <code class="docutils literal notranslate"><span class="pre">AllReduceAlgo.NCCL</span></code> value for the <code class="docutils literal notranslate"><span class="pre">all_reduce_algo</span></code> argument. For more information, refer to the details for the <code class="docutils literal notranslate"><span class="pre">all_reduce_algo</span></code> argument in the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#solver">CreateSolver API</a>.</p></li>
<li><p><strong>Support for Model Naming During Model Dumping</strong>: You can now specify names for models with the <code class="docutils literal notranslate"><span class="pre">CreateSolver</span></code>training API, which will be dumped to the JSON configuration file with the <code class="docutils literal notranslate"><span class="pre">Model.graph_to_json</span></code> API. This will facilitate the Triton deployment of saved HugeCTR models, as well as help to distinguish between models when Apache Kafka sends parameters from training to inference.</p></li>
<li><p><strong>Fine-Grained Control Accessibility Enhancements for Embedding Layers</strong>: We’ve added fine-grained control accessibility to embedding layers. Using the <code class="docutils literal notranslate"><span class="pre">Model.freeze_embedding</span></code> and <code class="docutils literal notranslate"><span class="pre">Model.unfreeze_embedding</span></code> APIs, embedding layer weights can be frozen and unfrozen. Additionally, weights for multiple embedding layers can be loaded independently, making it possible to load pre-trained embeddings for a particular layer. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#model">Model API</a> and <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/v3.3/notebooks/hugectr_criteo.ipynb">Section 3.4 of the HugeCTR Criteo Notebook</a>.</p></li>
</ul>
</section>
<section id="what-s-new-in-version-3-2-1">
<h2>What’s New in Version 3.2.1<a class="headerlink" href="#what-s-new-in-version-3-2-1" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>GPU Embedding Cache Optimization</strong>: The performance of the GPU embedding cache for the standalone module has been optimized. With this enhancement, the performance of small to medium batch sizes has improved significantly. We’re not introducing any changes to the interface for the GPU embedding cache, so don’t worry about making changes to any existing code that uses this standalone module. For more information, refer to the <code class="docutils literal notranslate"><span class="pre">ReadMe.md</span></code> file in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.2.1/gpu_cache">gpu_cache</a> directory of the repository.</p></li>
<li><p><strong>Model Oversubscription Enhancements</strong>: We’re introducing a new host memory cache (HMEM-Cache) component for the model oversubscription feature. When configured properly, incremental training can be efficiently performed on models with large embedding tables that exceed the host memory. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hugectr_core_features.html#embedding-training-cache">Host Memory Cache in MOS</a>. Additionally, we’ve enhanced the Python interface for model oversubscription by replacing the <code class="docutils literal notranslate"><span class="pre">use_host_memory_ps</span></code> parameter with a <code class="docutils literal notranslate"><span class="pre">ps_types</span></code> parameter and adding a <code class="docutils literal notranslate"><span class="pre">sparse_models</span></code> parameter. For more information about these changes, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#solver">HugeCTR Python Interface</a>.</p></li>
<li><p><strong>Debugging Enhancements</strong>: We’re introducing new debugging features such as multi-level logging, as well as kernel debugging functions. We’re also making our error messages more informative so that users know exactly how to resolve issues related to their training and inference code. For more information, refer to the comments in the header files, which are available at HugeCTR/include/base/debug.</p></li>
<li><p><strong>Enhancements to the Embedding Key Insertion Mechanism for the Embedding Cache</strong>: Missing embedding keys can now be asynchronously inserted into the embedding cache. To enable automatically, set the hit rate threshold within the configuration file. When the actual hit rate of the embedding cache is higher than the hit rate threshold that the user set or vice versa, the embedding cache will insert the missing embedding key asynchronously.</p></li>
<li><p><strong>Parameter Server Enhancements</strong>: We’re introducing a new “in memory” database that utilizes the local CPU memory for storing and recalling embeddings and uses multi-threading to accelerate lookup and storage. You can now also use the combined CPU-accessible memory of your Redis cluster to store embeddings. We improved the performance for the “persistent” storage and retrieving embeddings from RocksDB using structured column families, as well as added support for creating hierarchical storage such as Redis as distributed cache. You don’t have to worry about updating your Parameter Server configurations to take advantage of these enhancements.</p></li>
<li><p><strong>Slice Layer Internalization Enhancements</strong>: The Slice layer for the branch toplogy can now be abstracted away in the Python interface. A model graph analysis will be conducted to resolve the tensor dependency and the Slice layer will be internally inserted if the same tensor is consumed more than once to form the branch topology. For more information about how to construct a model graph using branches without the Slice layer, refer to the Getting Started section of the repository README and the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#layers">Slice Layer</a> information.</p></li>
</ul>
</section>
<section id="what-s-new-in-version-3-2">
<h2>What’s New in Version 3.2<a class="headerlink" href="#what-s-new-in-version-3-2" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>New HugeCTR to ONNX Converter</strong>: We’re introducing a new HugeCTR to ONNX converter in the form of a Python package. All graph configuration files are required and model weights must be formatted as inputs. You can specify where you want to save the converted ONNX model. You can also convert sparse embedding models. For more information, refer to the HugeCTR to ONNX Converter information in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.2/onnx_converter">onnx_converter</a> directory and the <a class="reference internal" href="#notebooks/hugectr2onnx_demo.ipynb"><span class="xref myst">HugeCTR2ONNX Demo Notebook</span></a>.</p></li>
<li><p><strong>New Hierarchical Storage Mechanism on the Parameter Server (POC)</strong>: We’ve implemented a hierarchical storage mechanism between local SSDs and CPU memory. As a result, embedding tables no longer have to be stored in the local CPU memory. The distributed Redis cluster is being implemented as a CPU cache to store larger embedding tables and interact with the GPU embedding cache directly. The local RocksDB serves as a query engine to back up the complete embedding table on the local SSDs and assist the Redis cluster with looking up missing embedding keys. For more information about how this works, refer to our <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/blob/main/docs/architecture.md#distributed-deployment-with-hierarchical-hugectr-parameter-server">HugeCTR Backend documentation</a></p></li>
<li><p><strong>Parquet Format Support within the Data Generator</strong>: The HugeCTR data generator now supports the parquet format, which can be configured easily using the Python API. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#data-generator-api">Data Generator API</a>.</p></li>
<li><p><strong>Python Interface Support for the Data Generator</strong>: The data generator has been enabled within the HugeCTR Python interface. The parameters associated with the data generator have been encapsulated into the <code class="docutils literal notranslate"><span class="pre">DataGeneratorParams</span></code> struct, which is required to initialize the <code class="docutils literal notranslate"><span class="pre">DataGenerator</span></code> instance. You can use the data generator’s Python APIs to easily generate the Norm, Parquet, or Raw dataset formats with the desired distribution of sparse keys. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#data-generator-api">Data Generator API</a> and the data generator samples in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.2/tools/data_generator">tools/data_generator</a> directory of the repository.</p></li>
<li><p><strong>Improvements to the Formula of the Power Law Simulator within the Data Generator</strong>: We’ve modified the formula of the power law simulator within the data generator so that a positive alpha value is always produced, which will be needed for most use cases. The alpha values for <code class="docutils literal notranslate"><span class="pre">Long</span></code>, <code class="docutils literal notranslate"><span class="pre">Medium</span></code>, and <code class="docutils literal notranslate"><span class="pre">Short</span></code> within the power law distribution are 0.9, 1.1, and 1.3 respectively. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#data-generator-api">Data Generator API</a>.</p></li>
<li><p><strong>Support for Arbitrary Input and Output Tensors in the Concat and Slice Layers</strong>: The Concat and Slice layers now support any number of input and output tensors. Previously, these layers were limited to a maximum of four tensors.</p></li>
<li><p><strong>New Continuous Training Notebook</strong>: We’ve added a new notebook to demonstrate how to perform continuous training using the embedding training cache (also referred to as Embedding Training Cache) feature. For more information, refer to <a class="reference internal" href="#notebooks/continuous_training.ipynb"><span class="xref myst">HugeCTR Continuous Training</span></a>.</p></li>
<li><p><strong>New HugeCTR Contributor Guide</strong>: We’ve added a new <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hugectr_contributor_guide.html">HugeCTR Contributor Guide</a> that explains how to contribute to HugeCTR, which may involve reporting and fixing a bug, introducing a new feature, or implementing a new or pending feature.</p></li>
<li><p><strong>Sparse Operation Kit (SOK) Enhancements</strong>: SOK now supports TensorFlow 2.5 and 2.6. We also added support for identity hashing, dynamic input, and Horovod within SOK. Lastly, we added a new <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/index.html">SOK docs set</a> to help you get started with SOK.</p></li>
</ul>
</section>
<section id="what-s-new-in-version-3-1">
<h2>What’s New in Version 3.1<a class="headerlink" href="#what-s-new-in-version-3-1" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Hybrid Embedding</strong>: Hybrid embedding is designed to overcome the bandwidth constraint imposed by the embedding part of the embedding train workload by algorithmically reducing the traffic over netwoek. Requirements: The input dataset has only one-hot feature items and the model uses the SGD optimizer.</p></li>
<li><p><strong>FusedReluBiasFullyConnectedLayer</strong>: FusedReluBiasFullyConnectedLayer is one of the major optimizations applied to dense layers. It fuses relu Bias and FullyConnected layers to reduce the memory access on HBM. Requirements: The model uses a layer with separate data / gradient tensors as the bottom layer.</p></li>
<li><p><strong>Overlapped Pipeline</strong>: The computation in the dense input data path is overlapped with the hybrid embedding computation. Requirements: The data reader is asynchronous, hybrid embedding is used, and the model has a feature interaction layer.</p></li>
<li><p><strong>Holistic CUDA Graph</strong>: Packing everything inside a training iteration into a CUDA Graph. Limitations: this option works only if use_cuda_graph is turned off and use_overlapped_pipeline is turned on.</p></li>
<li><p><strong>Python Interface Enhancements</strong>: We’ve enhanced the Python interface for HugeCTR so that you no longer have to manually create a JSON configuration file. Our Python APIs can now be used to create the computation graph. They can also be used to dump the model graph as a JSON object and save the model weights as binary files so that continuous training and inference can take place. We’ve added an Inference API that takes Norm or Parquet datasets as input to facilitate the inference process. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html">HugeCTR Python Interface</a> and <a class="reference internal" href="#notebooks/hugectr_criteo.ipynb"><span class="xref myst">HugeCTR Criteo Notebook</span></a>.</p></li>
<li><p><strong>New Interface for Unified Embedding</strong>: We’re introducing a new interface to simplify the use of embeddings and datareaders. To help you specify the number of keys in each slot, we added <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code> and <code class="docutils literal notranslate"><span class="pre">is_fixed_length</span></code>. You can now directly configure how much memory usage you need by specifying <code class="docutils literal notranslate"><span class="pre">workspace_size_per_gpu_in_mb</span></code> instead of <code class="docutils literal notranslate"><span class="pre">max_vocabulary_size_per_gpu</span></code>. For convenience, <code class="docutils literal notranslate"><span class="pre">mean/sum</span></code> is used in combinators instead of 0 and 1. In cases where you don’t know which embedding type you should use, you can specify <code class="docutils literal notranslate"><span class="pre">use_hash_table</span></code> and let HugeCTR automatically select the embedding type based on your configuration. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html">HugeCTR Python Interface</a>.</p></li>
<li><p><strong>Multi-Node Support for Embedding Training Cache (ETC)</strong>: We’ve enabled multi-node support for the embedding training cache. You can now train a model with a terabyte-size embedding table using one node or multiple nodes even if the entire embedding table can’t fit into the GPU memory. We’re also introducing the host memory (HMEM) based parameter server (PS) along with its SSD-based counterpart. If the sparse model can fit into the host memory of each training node, the optimized HMEM-based PS can provide better model loading and dumping performance with a more effective bandwidth. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html">HugeCTR Python Interface</a>.</p></li>
<li><p><strong>Enhancements to the Multi-Nodes TensorFlow Plugin</strong>: The Multi-Nodes TensorFlow Plugin now supports multi-node synchronized training via tf.distribute.MultiWorkerMirroredStrategy. With minimal code changes, you can now easily scale your single GPU training to multi-node multi GPU training. The Multi-Nodes TensorFlow Plugin also supports multi-node synchronized training via Horovod. The inputs for embedding plugins are now data parallel, so the datareader no longer needs to preprocess data for different GPUs based on concrete embedding algorithms. For more information, see the <code class="docutils literal notranslate"><span class="pre">sparse_operation_kit_demo.ipynb</span></code> notebook in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.2/sparse_operation_kit/notebooks">sparse_operation_kit/notebooks</a> directory of the repository.</p></li>
<li><p><strong>NCF Model Support</strong>: We’ve added support for the NCF model, as well as the GMF and NeuMF variant models. With this enhancement, we’re introducing a new element-wise multiplication layer and HitRate evaluation metric. Sample code was added that demonstrates how to preprocess user-item interaction data and train a NCF model with it. New examples have also been added that demonstrate how to train NCF models using MovieLens datasets.</p></li>
<li><p><strong>DIN and DIEN Model Support</strong>: All of our layers support the DIN model. The following layers support the DIEN model: FusedReshapeConcat, FusedReshapeConcatGeneral, Gather, GRU, PReLUDice, ReduceMean, Scale, Softmax, and Sub. We also added sample code to demonstrate how to use the Amazon dataset to train the DIN model. See our sample programs in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.1/samples/din">samples/din</a> directory of the repository.</p></li>
<li><p><strong>Multi-Hot Support for Parquet Datasets</strong>: We’ve added multi-hot support for parquet datasets, so you can now train models with a paraquet dataset that contains both one hot and multi-hot slots.</p></li>
<li><p><strong>Mixed Precision (FP16) Support in More Layers</strong>: The MultiCross layer now supports mixed precision (FP16). All layers now support FP16.</p></li>
<li><p><strong>Mixed Precision (FP16) Support in Inference</strong>: We’ve added FP16 support for the inference pipeline. Therefore, dense layers can now adopt FP16 during inference.</p></li>
<li><p><strong>Optimizer State Enhancements for Continuous Training</strong>: You can now store optimizer states that are updated during continuous training as files, such as the Adam optimizer’s first moment (m) and second moment (v). By default, the optimizer states are initialized with zeros, but you can specify a set of optimizer state files to recover their previous values. For more information about <code class="docutils literal notranslate"><span class="pre">dense_opt_states_file</span></code> and <code class="docutils literal notranslate"><span class="pre">sparse_opt_states_file</span></code>, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#">Python Interface</a>.</p></li>
<li><p><strong>New Library File for GPU Embedding Cache Data</strong>: We’ve moved the header/source code of the GPU embedding cache data structure into a stand-alone folder. It has been compiled into a stand-alone library file. Similar to HugeCTR, your application programs can now be directly linked from this new library file for future use. For more information, refer to the <code class="docutils literal notranslate"><span class="pre">ReadMe.md</span></code> file in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.1/gpu_cache">gpu_cache</a> directory of the repository.</p></li>
<li><p><strong>Embedding Plugin Enhancements</strong>: We’ve moved all the embedding plugin files into a stand-alone folder. The embedding plugin can be used as a stand-alone python module, and works with TensorFlow to accelerate the embedding training process.</p></li>
<li><p><strong>Adagrad Support</strong>: Adagrad can now be used to optimize your embedding and network. To use it, change the optimizer type in the Optimizer layer and set the corresponding parameters.</p></li>
</ul>
</section>
<section id="what-s-new-in-version-3-0-1">
<h2>What’s New in Version 3.0.1<a class="headerlink" href="#what-s-new-in-version-3-0-1" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>New DLRM Inference Benchmark</strong>: We’ve added two detailed Jupyter notebooks to demonstrate how to train, deploy, and benchmark the performance of a deep learning recommendation model (DLRM) with HugeCTR. For more information, refer to our <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/v3.0.1/samples/dlrm">HugeCTR Inference Notebooks</a>.</p></li>
<li><p><strong>FP16 Optimization</strong>: We’ve optimized the DotProduct, ELU, and Sigmoid layers based on <code class="docutils literal notranslate"><span class="pre">__half2</span></code> vectorized loads and stores, improving their device memory bandwidth utilization. MultiCross, FmOrder2, ReduceSum, and Multiply are the only layers that still need to be optimized for FP16.</p></li>
<li><p><strong>Synthetic Data Generator Enhancements</strong>: We’ve enhanced our synthetic data generator so that it can generate uniformly distributed datasets, as well as power-law based datasets. You can now specify the <code class="docutils literal notranslate"><span class="pre">vocabulary_size</span></code> and <code class="docutils literal notranslate"><span class="pre">max_nnz</span></code> per categorical feature instead of across all categorial features. For more information, refer to our <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hugectr_user_guide.html#generating-synthetic-data-and-benchmarks">user guide</a>.</p></li>
<li><p><strong>Reduced Memory Allocation for Trained Model Exportation</strong>: To prevent the “Out of Memory” error message from displaying when exporting a trained model, which may include a very large embedding table, the amount of memory allocated by the related functions has been significantly reduced.</p></li>
<li><p><strong>Dropout Layer Enhancement</strong>: The Dropout layer is now compatible with CUDA Graph. The Dropout layer is using cuDNN by default so that it can be used with CUDA Graph.</p></li>
</ul>
</section>
<section id="whats-new-in-version-3-0">
<h2>What’s New in Version 3.0<a class="headerlink" href="#whats-new-in-version-3-0" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Inference Support</strong>: To streamline the recommender system workflow, we’ve implemented a custom HugeCTR backend on the <a class="reference external" href="https://developer.nvidia.com/nvidia-triton-inference-server">NVIDIA Triton Inference Server</a>. The HugeCTR backend leverages the embedding cache and parameter server to efficiently manage embeddings of different sizes and models in a hierarchical manner. For more information, refer to <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend">our inference repository</a>.</p></li>
<li><p><strong>New High-Level API</strong>: You can now also construct and train your models using the Python interface with our new high-level API. For more information, refer to our preview example code in the <code class="docutils literal notranslate"><span class="pre">samples/preview</span></code> directory to grasp how this new API works.</p></li>
<li><p><strong><a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/hugectr_core_features.html#mixed-precision-training">FP16 Support</a> in More Layers</strong>: All the layers except <code class="docutils literal notranslate"><span class="pre">MultiCross</span></code> support mixed precision mode. We’ve also optimized some of the FP16 layer implementations based on vectorized loads and stores.</p></li>
<li><p><strong>Enhanced TensorFlow Embedding Plugin</strong>: Our embedding plugin now supports <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingHash</span></code> mode. With this enhancement, the DNN model no longer needs to be split into two parts since it now connects with the embedding op through <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> within the embedding layer. For more information, see the <code class="docutils literal notranslate"><span class="pre">notebooks/embedding_plugin.ipynb</span></code> notebook.</p></li>
<li><p><strong>Extended Embedding Training Cache</strong>: We’ve extended the embedding training cache feature to support <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingHash</span></code> and <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingHashOneHot</span></code>.</p></li>
<li><p><strong>Epoch-Based Training Enhancements</strong>: The <code class="docutils literal notranslate"><span class="pre">num_epochs</span></code> option in the <strong>Solver</strong> clause can now be used with the <code class="docutils literal notranslate"><span class="pre">Raw</span></code> dataset format.</p></li>
<li><p><strong>Deprecation of the <code class="docutils literal notranslate"><span class="pre">eval_batches</span></code> Parameter</strong>: The <code class="docutils literal notranslate"><span class="pre">eval_batches</span></code> parameter has been deprecated and replaced with the <code class="docutils literal notranslate"><span class="pre">max_eval_batches</span></code> and <code class="docutils literal notranslate"><span class="pre">max_eval_samples</span></code> parameters. In epoch mode, these parameters control the maximum number of evaluations. An error message will appear when attempting to use the <code class="docutils literal notranslate"><span class="pre">eval_batches</span></code> parameter.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">MultiplyLayer</span></code> Renamed</strong>: To clarify what the <code class="docutils literal notranslate"><span class="pre">MultiplyLayer</span></code> does, it was renamed to <code class="docutils literal notranslate"><span class="pre">WeightMultiplyLayer</span></code>.</p></li>
<li><p><strong>Optimized Initialization Time</strong>: HugeCTR’s initialization time, which includes the GEMM algorithm search and parameter initialization, was significantly reduced.</p></li>
<li><p><strong>Sample Enhancements</strong>: Our samples now rely upon the <a class="reference external" href="https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/">Criteo 1TB Click Logs dataset</a> instead of the Kaggle Display Advertising Challenge dataset. Our preprocessing scripts (Perl, Pandas, and NVTabular) have also been unified and simplified.</p></li>
<li><p><strong>Configurable DataReader Worker</strong>: You can now specify the number of data reader workers, which run in parallel, with the <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> parameter. Its default value is 12. However, if you are using the Parquet data reader, you can’t configure the <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> parameter since it always corresponds to the number of active GPUs.</p></li>
</ul>
</section>
<section id="what-s-new-in-version-2-3">
<h2>What’s New in Version 2.3<a class="headerlink" href="#what-s-new-in-version-2-3" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>New Python Interface</strong>: To enhance the interoperability with <a class="reference external" href="https://github.com/NVIDIA/NVTabular">NVTabular</a> and other Python-based libraries, we’re introducing a new Python interface for HugeCTR.</p></li>
<li><p><strong>HugeCTR Embedding with Tensorflow</strong>: To help users easily integrate HugeCTR’s optimized embedding into their Tensorflow workflow, we now offer the HugeCTR embedding layer as a Tensorflow plugin. To better understand how to install, use, and verify it, see our Jupyter notebook tutorial in file <code class="docutils literal notranslate"><span class="pre">notebooks/embedding_plugin.ipynb</span></code>. The notebook also demonstrates how you can create a new Keras layer, <code class="docutils literal notranslate"><span class="pre">EmbeddingLayer</span></code>, based on the <code class="docutils literal notranslate"><span class="pre">hugectr.py</span></code> file in the <code class="docutils literal notranslate"><span class="pre">tools/embedding_plugin/python</span></code> directory with the helper code that we provide.</p></li>
<li><p><strong>Embedding Training Cache</strong>: To enable a model with large embedding tables that exceeds the single GPU’s memory limit, we’ve added a new embedding training cache feature, giving you the ability to load a subset of an embedding table into the GPU in a coarse grained, on-demand manner during the training stage.</p></li>
<li><p><strong>TF32 Support</strong>: We’ve added TensorFloat-32 (TF32), a new math mode and third-generation of Tensor Cores, support on Ampere. TF32 uses the same 10-bit mantissa as FP16 to ensure accuracy while providing the same range as FP32 by using an 8-bit exponent. Since TF32 is an internal data type that accelerates FP32 GEMM computations with tensor cores, you can simply turn it on with a newly added configuration option. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#solver">Solver</a>.</p></li>
<li><p><strong>Enhanced AUC Implementation</strong>: To enhance the performance of our AUC computation on multi-node environments, we’ve redesigned our AUC implementation to improve how the computational load gets distributed across nodes.</p></li>
<li><p><strong>Epoch-Based Training</strong>: In addition to the <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> parameter, you can now set the <code class="docutils literal notranslate"><span class="pre">num_epochs</span></code> parameter in the <strong>Solver</strong> clause within the configuration file. This mode can only currently be used with <code class="docutils literal notranslate"><span class="pre">Norm</span></code> dataset formats and their corresponding file lists. All dataset formats will be supported in the future.</p></li>
<li><p><strong>New Multi-Node Training Tutorial</strong>: To better support multi-node training use cases, we’ve added a new step-by-step tutorial to the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/main/tutorial/multinode-training">tutorial/multinode-training</a> directory of our GitHub repository.</p></li>
<li><p><strong>Power Law Distribution Support with Data Generator</strong>: Because of the increased need for generating a random dataset whose categorical features follows the power-law distribution, we’ve revised our data generation tool to support this use case. For additional information, refer to the <code class="docutils literal notranslate"><span class="pre">--long-tail</span></code> description in the Generating Synthetic Data and Benchmarks section of the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/v2.3/docs/hugectr_user_guide.md#generating-synthetic-data-and-benchmarks">docs/hugectr_user_guide.md</a> file in the repository.</p></li>
<li><p><strong>Multi-GPU Preprocessing Script for Criteo Samples</strong>: Multiple GPUs can now be used when preparing the dataset for the programs in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v2.3/samples">samples</a> directory of our GitHub repository. For more information, see how the <code class="docutils literal notranslate"><span class="pre">preprocess_nvt.py</span></code> program in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v2.3/tools/criteo_script">tools/criteo_script</a> directory of the repository is used to preprocess the Criteo dataset for DCN, DeepFM, and W&amp;D samples.</p></li>
</ul>
</section>
<section id="known-issues">
<h2>Known Issues<a class="headerlink" href="#known-issues" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>HugeCTR uses NCCL to share data between ranks, and NCCL may require shared system memory for IPC and pinned (page-locked) system memory resources. When using NCCL inside a container, it is recommended that you increase these resources by issuing: <code class="docutils literal notranslate"><span class="pre">-shm-size=1g</span> <span class="pre">-ulimit</span> <span class="pre">memlock=-1</span></code>
See also <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">NCCL’s known issue</a>. And the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">GitHub issue</a>.</p></li>
<li><p>KafkaProducers startup will succeed, even if the target Kafka broker is unresponsive. In order to avoid data-loss in conjunction with streaming model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers is up, working properly and reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be no less than the number of data reader workers. Otherwise, different workers will be mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint Loss training hasn’t been supported with regularizer.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="QAList.html" class="btn btn-neutral float-left" title="Questions and Answers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hugectr_contributor_guide.html" class="btn btn-neutral float-right" title="Contributing to HugeCTR" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v24.06.00
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../v23.08.00/index.html">v23.08.00</a></dd>
      <dd><a href="../v23.09.00/index.html">v23.09.00</a></dd>
      <dd><a href="../v23.12.00/index.html">v23.12.00</a></dd>
      <dd><a href="../v24.04.00/index.html">v24.04.00</a></dd>
      <dd><a href="release_notes.html">v24.06.00</a></dd>
      <dd><a href="../v25.03.00/index.html">v25.03.00</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../main/index.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>