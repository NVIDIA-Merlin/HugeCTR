<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>HugeCTR Layer Classes and Methods &mdash; Merlin HugeCTR  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Additional Resources" href="../additional_resources.html" />
    <link rel="prev" title="HugeCTR Python Interface" href="python_interface.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">HUGECTR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_embedding_training_cache.html">Embedding Training Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hierarchical_parameter_server/index.html">Hierarchical Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sparse_operation_kit.html">Sparse Operation Kit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API Documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="python_interface.html">Python Interface</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Layer Classes and Methods</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">HugeCTR API Documentation</a> &raquo;</li>
      <li>HugeCTR Layer Classes and Methods</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="tex2jax_ignore mathjax_ignore section" id="hugectr-layer-classes-and-methods">
<h1>HugeCTR Layer Classes and Methods<a class="headerlink" href="#hugectr-layer-classes-and-methods" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#input-layer" id="id1">Input Layer</a></p></li>
<li><p><a class="reference internal" href="#sparse-embedding" id="id2">Sparse Embedding</a></p></li>
<li><p><a class="reference internal" href="#embedding-types-detail" id="id3">Embedding Types Detail</a></p>
<ul>
<li><p><a class="reference internal" href="#distributedslotsparseembeddinghash-layer" id="id4">DistributedSlotSparseEmbeddingHash Layer</a></p></li>
<li><p><a class="reference internal" href="#localizedslotsparseembeddinghash-layer" id="id5">LocalizedSlotSparseEmbeddingHash Layer</a></p></li>
<li><p><a class="reference internal" href="#localizedslotsparseembeddingonehot-layer" id="id6">LocalizedSlotSparseEmbeddingOneHot Layer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#dense-layers" id="id7">Dense Layers</a></p></li>
<li><p><a class="reference internal" href="#dense-layers-usage" id="id8">Dense Layers Usage</a></p>
<ul>
<li><p><a class="reference internal" href="#fullyconnected-layer" id="id9">FullyConnected Layer</a></p></li>
<li><p><a class="reference internal" href="#fusedfullyconnected-layer" id="id10">FusedFullyConnected Layer</a></p></li>
<li><p><a class="reference internal" href="#multicross-layer" id="id11">MultiCross Layer</a></p></li>
<li><p><a class="reference internal" href="#fmorder2-layer" id="id12">FmOrder2 Layer</a></p></li>
<li><p><a class="reference internal" href="#weightmultiply-layer" id="id13">WeightMultiply Layer</a></p></li>
<li><p><a class="reference internal" href="#elementwisemultiply-layer" id="id14">ElementwiseMultiply Layer</a></p></li>
<li><p><a class="reference internal" href="#batchnorm-layer" id="id15">BatchNorm Layer</a></p></li>
<li><p><a class="reference internal" href="#layernorm-layer" id="id16">LayerNorm Layer</a></p></li>
<li><p><a class="reference internal" href="#concat-layer" id="id17">Concat Layer</a></p></li>
<li><p><a class="reference internal" href="#reshape-layer" id="id18">Reshape Layer</a></p></li>
<li><p><a class="reference internal" href="#slice-layer" id="id19">Slice Layer</a></p></li>
<li><p><a class="reference internal" href="#dropout-layer" id="id20">Dropout Layer</a></p></li>
<li><p><a class="reference internal" href="#elu-layer" id="id21">ELU Layer</a></p></li>
<li><p><a class="reference internal" href="#relu-layer" id="id22">ReLU Layer</a></p></li>
<li><p><a class="reference internal" href="#sigmoid-layer" id="id23">Sigmoid Layer</a></p></li>
<li><p><a class="reference internal" href="#interaction-layer" id="id24">Interaction Layer</a></p></li>
<li><p><a class="reference internal" href="#add-layer" id="id25">Add Layer</a></p></li>
<li><p><a class="reference internal" href="#reducesum-layer" id="id26">ReduceSum Layer</a></p></li>
<li><p><a class="reference internal" href="#binarycrossentropyloss" id="id27">BinaryCrossEntropyLoss</a></p></li>
<li><p><a class="reference internal" href="#crossentropyloss" id="id28">CrossEntropyLoss</a></p></li>
<li><p><a class="reference internal" href="#multicrossentropyloss" id="id29">MultiCrossEntropyLoss</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#embedding-collection" id="id30">Embedding Collection</a></p>
<ul>
<li><p><a class="reference internal" href="#about-the-hugectr-embedding-collection" id="id31">About the HugeCTR embedding collection</a></p></li>
<li><p><a class="reference internal" href="#sample-notebook" id="id32">Sample Notebook</a></p></li>
<li><p><a class="reference internal" href="#overview-of-using-the-hugectr-embedding-collection" id="id33">Overview of using the HugeCTR embedding collection</a></p></li>
<li><p><a class="reference internal" href="#embeddingtableconfig" id="id34">EmbeddingTableConfig</a></p></li>
<li><p><a class="reference internal" href="#embeddingplanner" id="id35">EmbeddingPlanner</a></p></li>
<li><p><a class="reference internal" href="#plan-file-and-embedding-table-placement-strategy-etps" id="id36">Plan File and Embedding Table Placement Strategy (ETPS)</a></p></li>
<li><p><a class="reference internal" href="#about-etps-and-benefits" id="id37">About ETPS and Benefits</a></p></li>
<li><p><a class="reference internal" href="#configuring-etps-and-the-embedding-collection" id="id38">Configuring ETPS and the Embedding Collection</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#groupdenselayer" id="id39">GroupDenseLayer</a></p></li>
</ul>
</div>
<p>This document introduces different layer classes and corresponding methods in the Python API of HugeCTR. The description of each method includes its functionality, arguments, and examples of usage.</p>
<div class="section" id="input-layer">
<h2>Input Layer<a class="headerlink" href="#input-layer" title="Permalink to this headline"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hugectr</span><span class="o">.</span><span class="n">Input</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Input</span></code> layer specifies the parameters related to the data input. <code class="docutils literal notranslate"><span class="pre">Input</span></code> layer should be added to the Model instance first so that the following <code class="docutils literal notranslate"><span class="pre">SparseEmbedding</span></code> and <code class="docutils literal notranslate"><span class="pre">DenseLayer</span></code> instances can access the inputs with their specified names.</p>
<p><strong>Arguments</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">label_dim</span></code>: Integer, the label dimension. 1 implies it is a binary label. For example, if an item is clicked or not. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">label_name</span></code>: String, the name of the label tensor to be referenced by following layers. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dense_dim</span></code>: Integer, the number of dense (or continuous) features. If there is no dense feature, set it to 0. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dense_name</span></code>: Integer, the name of the dense input tensor to be referenced by following layers. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data_reader_sparse_param_array</span></code>: List[hugectr.DataReaderSparseParam], the list of the sparse parameters for categorical inputs. Each <code class="docutils literal notranslate"><span class="pre">DataReaderSparseParam</span></code> instance should be constructed with  <code class="docutils literal notranslate"><span class="pre">sparse_name</span></code>, <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code>, <code class="docutils literal notranslate"><span class="pre">is_fixed_length</span></code> and <code class="docutils literal notranslate"><span class="pre">slot_num</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">sparse_name</span></code> is the name of the sparse input tensors to be referenced by following layers. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code> is the maximum number of features for each slot for the specified spare input. The <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code> can be an <code class="docutils literal notranslate"><span class="pre">int</span></code> which means average nnz per slot so the maximum number of features per sample should be <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span> <span class="pre">*</span> <span class="pre">slot_num</span></code>. Or you can use List[int] to initialize <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code>, then the maximum number of features per sample should be <code class="docutils literal notranslate"><span class="pre">sum(nnz_per_slot)</span></code> and in this case, the length of the array <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code> should be the same with <code class="docutils literal notranslate"><span class="pre">slot_num</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_fixed_length</span></code> is used to identify whether categorical inputs has the same length for each slot among all samples. If different samples have the same number of features for each slot, then user can set <code class="docutils literal notranslate"><span class="pre">is_fixed_length</span> <span class="pre">=</span> <span class="pre">True</span></code> and HugeCTR can use this information to reduce data transferring time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">slot_num</span></code> specifies the number of slots used for this sparse input in the dataset.</p></li>
</ul>
</li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">label_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">label_name</span> <span class="o">=</span> <span class="s2">&quot;label&quot;</span><span class="p">,</span>
                        <span class="n">dense_dim</span> <span class="o">=</span> <span class="mi">13</span><span class="p">,</span> <span class="n">dense_name</span> <span class="o">=</span> <span class="s2">&quot;dense&quot;</span><span class="p">,</span>
                        <span class="n">data_reader_sparse_param_array</span> <span class="o">=</span>
                            <span class="p">[</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DataReaderSparseParam</span><span class="p">(</span><span class="s2">&quot;data1&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">26</span><span class="p">)]))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">label_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">label_name</span> <span class="o">=</span> <span class="s2">&quot;label&quot;</span><span class="p">,</span>
                        <span class="n">dense_dim</span> <span class="o">=</span> <span class="mi">13</span><span class="p">,</span> <span class="n">dense_name</span> <span class="o">=</span> <span class="s2">&quot;dense&quot;</span><span class="p">,</span>
                        <span class="n">data_reader_sparse_param_array</span> <span class="o">=</span>
                            <span class="p">[</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DataReaderSparseParam</span><span class="p">(</span><span class="s2">&quot;wide_data&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                            <span class="n">hugectr</span><span class="o">.</span><span class="n">DataReaderSparseParam</span><span class="p">(</span><span class="s2">&quot;deep_data&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">26</span><span class="p">)]))</span>
</pre></div>
</div>
</div>
<div class="section" id="sparse-embedding">
<h2>Sparse Embedding<a class="headerlink" href="#sparse-embedding" title="Permalink to this headline"></a></h2>
<p><strong>SparseEmbedding class</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hugectr</span><span class="o">.</span><span class="n">SparseEmbedding</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">SparseEmbedding</span></code> specifies the parameters related to the sparse embedding layer. One or several <code class="docutils literal notranslate"><span class="pre">SparseEmbedding</span></code> layers should be added to the Model instance after <code class="docutils literal notranslate"><span class="pre">Input</span></code> and before <code class="docutils literal notranslate"><span class="pre">DenseLayer</span></code>.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">embedding_type</span></code>: The embedding type.
Specify one of the following values:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hugectr.Embedding_t.LocalizedSlotSparseEmbeddingOneHot</span></code></p></li>
</ul>
<p>For information about the different embedding types, see <a class="reference internal" href="#embedding-types-detail"><span class="std std-doc">Embedding Types Detail</span></a>.
This argument does not have a default value.
You must specify a value.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">workspace_size_per_gpu_in_mb</span></code>: Integer, the workspace memory size in megabyte per GPU.
This workspace memory must be big enough to hold all the embedding vocabulary and its corresponding optimizer state that is used during the training and evaluation.
To understand how to set this value, see <a class="reference internal" href="../QAList.html#how-to-set-workspace-size-per-gpu-in-mb-and-slot-size-array"><span class="std std-doc">How to set workspace_size_per_gpu_in_mb and slot_size_array</span></a>.
This argument does not have a default value.
You must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embedding_vec_size</span></code>: Integer, the embedding vector size.
This argument does not have a default value.
You must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">combiner</span></code>: String, the intra-slot reduction operation.
Specify <code class="docutils literal notranslate"><span class="pre">sum</span></code> or <code class="docutils literal notranslate"><span class="pre">mean</span></code>.
This argument does not have a default value.
You must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sparse_embedding_name</span></code>: String, the name of the sparse embedding tensor.
This name is referenced by the following layers.
This argument does not have a default value.
You must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bottom_name</span></code>: String, the number of the bottom tensor to consume with this sparse embedding layer.
Please note that the value should be a predefined sparse input name.
This argument does not have a default value.
You must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">slot_size_array</span></code>: List[int], specify the maximum key value from each slot.
It should be consistent with that of the sparse input.
This parameter is used in <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingHash</span></code> and <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingOneHot</span></code>.
The value you specify can help avoid wasting memory that is caused by an imbalanced vocabulary size.
For more information, see <a class="reference internal" href="../QAList.html#how-to-set-workspace-size-per-gpu-in-mb-and-slot-size-array"><span class="std std-doc">How to set workspace_size_per_gpu_in_mb and slot_size_array</span></a>.
This argument does not have a default value.
You must specify a value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optimizer</span></code>: OptParamsPy, the optimizer that is dedicated to this sparse embedding layer.
If you do not specify the optimizer for the sparse embedding, the sparse embedding layer adopts the same optimizer as dense layers.</p></li>
</ul>
</div>
<div class="section" id="embedding-types-detail">
<h2>Embedding Types Detail<a class="headerlink" href="#embedding-types-detail" title="Permalink to this headline"></a></h2>
<div class="section" id="distributedslotsparseembeddinghash-layer">
<h3>DistributedSlotSparseEmbeddingHash Layer<a class="headerlink" href="#distributedslotsparseembeddinghash-layer" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">DistributedSlotSparseEmbeddingHash</span></code> stores embeddings in an embedding table and gets them by using a set of integers or indices. The embedding table can be segmented into multiple slots or feature fields, which spans multiple GPUs and nodes. With <code class="docutils literal notranslate"><span class="pre">DistributedSlotSparseEmbeddingHash</span></code>, each GPU will have a portion of a slot. This type of embedding is useful when there’s an existing load imbalance among slots and OOM issues.</p>
<p><strong>Important Notes</strong>:</p>
<ul class="simple">
<li><p>In a single embedding layer, it is assumed that input integers represent unique feature IDs, which are mapped to unique embedding vectors.
All the embedding vectors in a single embedding layer must have the same size. If you want some input categorical features to have different embedding vector sizes, use multiple embedding layers.</p></li>
<li><p>The input indices’ data type, <code class="docutils literal notranslate"><span class="pre">input_key_type</span></code>, is specified in the solver. By default,  the 32-bit integer (I32) is used, but the 64-bit integer type (I64) is also allowed even if it is constrained by the dataset type. For additional information, see <a class="reference internal" href="python_interface.html#solver"><span class="std std-doc">Solver</span></a>.</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">SparseEmbedding</span><span class="p">(</span>
            <span class="n">embedding_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Embedding_t</span><span class="o">.</span><span class="n">DistributedSlotSparseEmbeddingHash</span><span class="p">,</span>
            <span class="n">workspace_size_per_gpu_in_mb</span> <span class="o">=</span> <span class="mi">23</span><span class="p">,</span>
            <span class="n">embedding_vec_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">combiner</span> <span class="o">=</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span>
            <span class="n">sparse_embedding_name</span> <span class="o">=</span> <span class="s2">&quot;sparse_embedding1&quot;</span><span class="p">,</span>
            <span class="n">bottom_name</span> <span class="o">=</span> <span class="s2">&quot;input_data&quot;</span><span class="p">,</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="localizedslotsparseembeddinghash-layer">
<h3>LocalizedSlotSparseEmbeddingHash Layer<a class="headerlink" href="#localizedslotsparseembeddinghash-layer" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingHash</span></code> layer to store embeddings in an embedding table and get them by using a set of integers or indices. The embedding table can be segmented into multiple slots or feature fields, which spans multiple GPUs and nodes. Unlike the DistributedSlotSparseEmbeddingHash layer, with this type of embedding layer, each individual slot is located in each GPU and not shared. This type of embedding layer provides the best scalability.</p>
<p><strong>Important Notes</strong>:</p>
<ul class="simple">
<li><p>In a single embedding layer, it is assumed that input integers represent unique feature IDs, which are mapped to unique embedding vectors.
All the embedding vectors in a single embedding layer must have the same size. If you want some input categorical features to have different embedding vector sizes, use multiple embedding layers.</p></li>
<li><p>The input indices’ data type, <code class="docutils literal notranslate"><span class="pre">input_key_type</span></code>, is specified in the solver. By default, the 32-bit integer (I32) is used, but the 64-bit integer type (I64) is also allowed even if it is constrained by the dataset type. For additional information, see <a class="reference internal" href="python_interface.html#solver"><span class="std std-doc">Solver</span></a>.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">SparseEmbedding</span><span class="p">(</span>
            <span class="n">embedding_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Embedding_t</span><span class="o">.</span><span class="n">LocalizedSlotSparseEmbeddingHash</span><span class="p">,</span>
            <span class="n">workspace_size_per_gpu_in_mb</span> <span class="o">=</span> <span class="mi">23</span><span class="p">,</span>
            <span class="n">embedding_vec_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">combiner</span> <span class="o">=</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span>
            <span class="n">sparse_embedding_name</span> <span class="o">=</span> <span class="s2">&quot;sparse_embedding1&quot;</span><span class="p">,</span>
            <span class="n">bottom_name</span> <span class="o">=</span> <span class="s2">&quot;input_data&quot;</span><span class="p">,</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="localizedslotsparseembeddingonehot-layer">
<h3>LocalizedSlotSparseEmbeddingOneHot Layer<a class="headerlink" href="#localizedslotsparseembeddingonehot-layer" title="Permalink to this headline"></a></h3>
<p>The LocalizedSlotSparseEmbeddingOneHot layer stores embeddings in an embedding table and gets them by using a set of integers or indices. The embedding table can be segmented into multiple slots or feature fields, which spans multiple GPUs and nodes. This is a performance-optimized version of LocalizedSlotSparseEmbeddingHash for the case where NVSwitch is available and inputs are one-hot categorical features.</p>
<p><strong>Note</strong>: LocalizedSlotSparseEmbeddingOneHot can only be used together with the Raw dataset format. Unlike other types of embeddings, LocalizedSlotSparseEmbeddingOneHot only supports single-node training and can be used only in a NVSwitch equipped system such as DGX-2 and DGX A100.
The input indices’ data type, <code class="docutils literal notranslate"><span class="pre">input_key_type</span></code>, is specified in the solver. By default, the 32-bit integer (I32) is used, but the 64-bit integer type (I64) is also allowed even if it is constrained by the dataset type. For additional information, see <a class="reference internal" href="python_interface.html#solver"><span class="std std-doc">Solver</span></a>.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">SparseEmbedding</span><span class="p">(</span>
            <span class="n">embedding_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Embedding_t</span><span class="o">.</span><span class="n">LocalizedSlotSparseEmbeddingOneHot</span><span class="p">,</span>
            <span class="n">slot_size_array</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1221</span><span class="p">,</span> <span class="mi">754</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
            <span class="n">embedding_vec_size</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
            <span class="n">combiner</span> <span class="o">=</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span>
            <span class="n">sparse_embedding_name</span> <span class="o">=</span> <span class="s2">&quot;sparse_embedding1&quot;</span><span class="p">,</span>
            <span class="n">bottom_name</span> <span class="o">=</span> <span class="s2">&quot;input_data&quot;</span><span class="p">,</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="dense-layers">
<h2>Dense Layers<a class="headerlink" href="#dense-layers" title="Permalink to this headline"></a></h2>
<p><strong>DenseLayer class</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">DenseLayer</span></code> specifies the parameters related to the dense layer or the loss function. HugeCTR currently supports multiple dense layers and loss functions. Please <strong>NOTE</strong> that the final sigmoid function is fused with the loss function to better utilize memory bandwidth.</p>
<p><strong>Arguments</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">layer_type</span></code>: The layer type to be used. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Add</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.BatchNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Cast</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Concat</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Dropout</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.ELU</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.FmOrder2</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.FusedInnerProduct</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.InnerProduct</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Interaction</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.MultiCross</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.ReLU</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.ReduceSum</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Reshape</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Sigmoid</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Slice</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.WeightMultiply</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.ElementwiseMultiply</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.GRU</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Scale</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.FusedReshapeConcat</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.FusedReshapeConcatGeneral</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Softmax</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.PReLU_Dice</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.ReduceMean</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Sub</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.Gather</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.BinaryCrossEntropyLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.CrossEntropyLoss</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Layer_t.MultiCrossEntropyLoss</span></code>. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bottom_names</span></code>: List[str], the list of bottom tensor names to be consumed by this dense layer. Each name in the list should be the predefined tensor name. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_names</span></code>: List[str], the list of top tensor names, which specify the output tensors of this dense layer. There is NO default value and it should be specified by users.</p></li>
<li><p>For details about the usage of each layer type and its parameters, please refer to <a class="reference internal" href="#dense-layers-usage"><span class="std std-doc">Dense Layers Usage</span></a>.</p></li>
</ul>
</div>
<div class="section" id="dense-layers-usage">
<h2>Dense Layers Usage<a class="headerlink" href="#dense-layers-usage" title="Permalink to this headline"></a></h2>
<div class="section" id="fullyconnected-layer">
<h3>FullyConnected Layer<a class="headerlink" href="#fullyconnected-layer" title="Permalink to this headline"></a></h3>
<p>The FullyConnected layer is a densely connected layer (or MLP layer). It is usually made of a <code class="docutils literal notranslate"><span class="pre">InnerProduct</span></code> layer and a <code class="docutils literal notranslate"><span class="pre">ReLU</span></code>.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_output</span></code>: Integer, the number of output elements for the <code class="docutils literal notranslate"><span class="pre">InnerProduct</span></code> or <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> layer. The default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_init_type</span></code>: Specifies how to initialize the weight array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias_init_type</span></code>: Specifies how to initialize the bias array for the <code class="docutils literal notranslate"><span class="pre">InnerProduct</span></code>, <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> or <code class="docutils literal notranslate"><span class="pre">MultiCross</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: (batch_size, num_output)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">InnerProduct</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;relu1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc2&quot;</span><span class="p">],</span>
                            <span class="n">num_output</span><span class="o">=</span><span class="mi">1024</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;relu2&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="fusedfullyconnected-layer">
<h3>FusedFullyConnected Layer<a class="headerlink" href="#fusedfullyconnected-layer" title="Permalink to this headline"></a></h3>
<p>The FusedFullyConnected layer fuses a common case where FullyConnectedLayer and ReLU are used together to save memory bandwidth.</p>
<p><strong>Note</strong>: This layer can only be used with Mixed Precision mode enabled.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_output</span></code>: Integer, the number of output elements for the <code class="docutils literal notranslate"><span class="pre">InnerProduct</span></code> or <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> layer. The default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_init_type</span></code>: Specifies how to initialize the weight array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias_init_type</span></code>: Specifies how to initialize the bias array for the <code class="docutils literal notranslate"><span class="pre">InnerProduct</span></code>, <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> or <code class="docutils literal notranslate"><span class="pre">MultiCross</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.
Input and Output Shapes:</p></li>
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: (batch_size, num_output)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">FusedInnerProduct</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc2&quot;</span><span class="p">],</span>
                            <span class="n">num_output</span><span class="o">=</span><span class="mi">1024</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="multicross-layer">
<h3>MultiCross Layer<a class="headerlink" href="#multicross-layer" title="Permalink to this headline"></a></h3>
<p>The MultiCross layer is a cross network where explicit feature crossing is applied across cross layers.</p>
<p><strong>Note</strong>: This layer doesn’t currently support Mixed Precision mode.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_layers</span></code>: Integer, number of cross layers in the cross network. It should be set as a positive number if you want to use the cross network. The default value is 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_init_type</span></code>: Specifies how to initialize the weight array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias_init_type</span></code>: Specifies how to initialize the bias array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">MultiCross</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice11&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;multicross1&quot;</span><span class="p">],</span>
                            <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="fmorder2-layer">
<h3>FmOrder2 Layer<a class="headerlink" href="#fmorder2-layer" title="Permalink to this headline"></a></h3>
<p>TheFmOrder2 layer is the second-order factorization machine (FM), which models linear and pairwise interactions as dot products of latent vectors.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">out_dim</span></code>: Integer, the output vector size. It should be set as a positive number if you want to use factorization machine. The default value is 0.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: (batch_size, out_dim)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">FmOrder2</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice32&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fmorder2&quot;</span><span class="p">],</span>
                            <span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="weightmultiply-layer">
<h3>WeightMultiply Layer<a class="headerlink" href="#weightmultiply-layer" title="Permalink to this headline"></a></h3>
<p>The Multiply Layer maps input elements into a latent vector space by multiplying each feature with a corresponding weight vector.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weight_dims</span></code>: List[Integer], the shape of the weight matrix (slot_dim, vec_dim) where vec_dim corresponds to the latent vector length for the <code class="docutils literal notranslate"><span class="pre">WeightMultiply</span></code> layer. It should be set correctly if you want to employ the weight multiplication. The default value is [].</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_init_type</span></code>: Specifies how to initialize the weight array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, slot_dim) where slot_dim represents the number of input features</p></li>
<li><p>output: (batch_size, slot_dim * vec_dim)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice32&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fmorder2&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">10</span><span class="p">]),</span>
                            <span class="n">weight_init_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Initializer_t</span><span class="o">.</span><span class="n">XavierUniform</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="elementwisemultiply-layer">
<h3>ElementwiseMultiply Layer<a class="headerlink" href="#elementwisemultiply-layer" title="Permalink to this headline"></a></h3>
<p>The ElementwiseMultiply Layer maps two inputs into a single resulting vector by performing an element-wise multiplication of the two inputs.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: 2x(batch_size, num_elem)</p></li>
<li><p>output: (batch_size, num_elem)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ElementwiseMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice1&quot;</span><span class="p">,</span><span class="s2">&quot;slice2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;eltmultiply1&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="batchnorm-layer">
<h3>BatchNorm Layer<a class="headerlink" href="#batchnorm-layer" title="Permalink to this headline"></a></h3>
<p>The BatchNorm layer implements a cuDNN based batch normalization.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">factor</span></code>: Float, exponential average factor such as runningMean = runningMean*(1-factor) + newMean*factor for the <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> layer. The default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eps</span></code>: Float, epsilon value used in the batch normalization formula for the <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> layer. The default value is 1e-5.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gamma_init_type</span></code>: Specifies how to initialize the gamma (or scale) array for the <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">beta_init_type</span></code>: Specifies how to initialize the beta (or offset) array for the <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elem)</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice32&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fmorder2&quot;</span><span class="p">],</span>
                            <span class="n">factor</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.00001</span><span class="p">,</span>
                            <span class="n">gamma_init_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Initializer_t</span><span class="o">.</span><span class="n">XavierUniform</span><span class="p">,</span>
                            <span class="n">beta_init_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Initializer_t</span><span class="o">.</span><span class="n">XavierUniform</span><span class="p">)</span>
</pre></div>
</div>
<p>When training a model, each BatchNorm layer stores mean and variance in a JSON file using the following format:
“snapshot_prefix” + “<em>dense</em>” + str(iter) + ”.model”</p>
<p>Example: my_snapshot_dense_5000.model<br></p>
<p>In the JSON file, you can find the batch norm parameters as shown below:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;layers&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">        </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;BatchNorm&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;mean&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">-0.192325</span><span class="p">,</span><span class="w"> </span><span class="mf">0.003050</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.323447</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.034817</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.091861</span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;var&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.738942</span><span class="p">,</span><span class="w"> </span><span class="mf">0.410794</span><span class="p">,</span><span class="w"> </span><span class="mf">1.370279</span><span class="p">,</span><span class="w"> </span><span class="mf">1.156337</span><span class="p">,</span><span class="w"> </span><span class="mf">0.638146</span><span class="p">]</span><span class="w"></span>
<span class="w">        </span><span class="p">},</span><span class="w"></span>
<span class="w">        </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;BatchNorm&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;mean&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">-0.759954</span><span class="p">,</span><span class="w"> </span><span class="mf">0.251507</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.648882</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.176316</span><span class="p">,</span><span class="w"> </span><span class="mf">0.515163</span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;var&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">1.434012</span><span class="p">,</span><span class="w"> </span><span class="mf">1.422724</span><span class="p">,</span><span class="w"> </span><span class="mf">1.001451</span><span class="p">,</span><span class="w"> </span><span class="mf">1.756962</span><span class="p">,</span><span class="w"> </span><span class="mf">1.126412</span><span class="p">]</span><span class="w"></span>
<span class="w">        </span><span class="p">},</span><span class="w"></span>
<span class="w">        </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;BatchNorm&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;mean&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.851878</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.837513</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.694674</span><span class="p">,</span><span class="w"> </span><span class="mf">0.791046</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.849544</span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;var&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">1.694500</span><span class="p">,</span><span class="w"> </span><span class="mf">5.405566</span><span class="p">,</span><span class="w"> </span><span class="mf">4.211646</span><span class="p">,</span><span class="w"> </span><span class="mf">1.936811</span><span class="p">,</span><span class="w"> </span><span class="mf">5.659098</span><span class="p">]</span><span class="w"></span>
<span class="w">        </span><span class="p">}</span><span class="w"></span>
<span class="w">      </span><span class="p">]</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</div>
<div class="section" id="layernorm-layer">
<h3>LayerNorm Layer<a class="headerlink" href="#layernorm-layer" title="Permalink to this headline"></a></h3>
<p>The LayerNorm layer implements a layer normalization.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">eps</span></code>: Float, epsilon value used in the batch normalization formula for the <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> layer. The default value is 1e-5.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gamma_init_type</span></code>: Specifies how to initialize the gamma (or scale) array for the <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">beta_init_type</span></code>: Specifies how to initialize the beta (or offset) array for the <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: 2D: (batch_size, num_elem), 3D: (batch_size, seq_len, num_elem), 4D: (head_num, batch_size, seq_len, num_elem)</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice32&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fmorder2&quot;</span><span class="p">],</span>
                            <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.00001</span><span class="p">,</span>
                            <span class="n">gamma_init_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Initializer_t</span><span class="o">.</span><span class="n">XavierUniform</span><span class="p">,</span>
                            <span class="n">beta_init_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Initializer_t</span><span class="o">.</span><span class="n">XavierUniform</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="concat-layer">
<h3>Concat Layer<a class="headerlink" href="#concat-layer" title="Permalink to this headline"></a></h3>
<p>The Concat layer concatenates a list of inputs.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">axis</span></code>:  Integer, the dimension to concat for the <code class="docutils literal notranslate"><span class="pre">Concat</span></code> layer. If the input is N-dimensional, 0 &lt;= axis &lt; N. The default value is 1.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: 3D: {(batch_size, num_feas_0, num_elems_0), (batch_size, num_feas + 1, num_elems_1), …} or 2D: {(batch_size, num_elems_0), (batch_size, num_elems_1), …}</p></li>
<li><p>output: 3D and axis=1: (batch_size, num_feas_0+num_feas_1+…, num_elems). 3D and axis=2: (batch_size, num_feas, num_elems_0+num_elems_1+…). 2D: (batch_size, num_elems_0+num_elems_1+…)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Concat</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reshape3&quot;</span><span class="p">,</span><span class="s2">&quot;weight_multiply2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;concat2&quot;</span><span class="p">],</span>
                            <span class="n">axis</span> <span class="o">=</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="reshape-layer">
<h3>Reshape Layer<a class="headerlink" href="#reshape-layer" title="Permalink to this headline"></a></h3>
<p>The Reshape layer reshapes a 3D input tensor into 2D shape.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">leading_dim</span></code>: Integer, the innermost dimension of the output tensor. It must be the multiple of the total number of input elements. If it is unspecified, n_slots * num_elems (see below) is used as the default leading_dim.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">time_step</span></code>: Integer, the second dimension of the 3D output tensor. It must be the multiple of the total number of input elements and must be defined with leading_dim.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">selected</span></code>: Boolean, whether to use the selected mode for the <code class="docutils literal notranslate"><span class="pre">Reshape</span></code> layer. The default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">selected_slots</span></code>: List[int], the selected slots for the <code class="docutils literal notranslate"><span class="pre">Reshape</span></code> layer. It will be ignored if <code class="docutils literal notranslate"><span class="pre">selected</span></code> is False. The default value is [].</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, n_slots, num_elems)</p></li>
<li><p>output: (tailing_dim, leading_dim) where tailing_dim is batch_size * n_slots * num_elems / leading_dim</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Reshape</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sparse_embedding1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reshape1&quot;</span><span class="p">],</span>
                            <span class="n">leading_dim</span><span class="o">=</span><span class="mi">416</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="slice-layer">
<h3>Slice Layer<a class="headerlink" href="#slice-layer" title="Permalink to this headline"></a></h3>
<p>The Slice layer extracts multiple output tensors from a 2D input tensors.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ranges</span></code>: List[Tuple[int, int]], used for the Slice layer. A list of tuples in which each one represents a range in the input tensor to generate the corresponding output tensor. For example, (2, 8) indicates that 6 elements starting from the second element in the input tensor are used to create an output tensor. Note that the start index is inclusive and the end index is exclusive. The number of tuples corresponds to the number of output tensors. Ranges are allowed to overlap unless it is a reverse or negative range. The default value is [].</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elems)</p></li>
<li><p>output: {(batch_size, b-a), (batch_size, d-c), …) where ranges ={[a, b), [c, d), …} and len(ranges) &lt;= 5</p></li>
</ul>
<p>Example:</p>
<p>You can apply the Slice layer to actually slicing a tensor. In this case, it must be explicitly added with Python API.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Slice</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice21&quot;</span><span class="p">,</span> <span class="s2">&quot;slice22&quot;</span><span class="p">],</span>
                            <span class="n">ranges</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">),(</span><span class="mi">10</span><span class="p">,</span><span class="mi">13</span><span class="p">)]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice21&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply1&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice22&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply2&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<p>The Slice layer can also be employed to create copies of a tensor, which helps to express a branch topology in your model graph.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Slice</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice21&quot;</span><span class="p">,</span> <span class="s2">&quot;slice22&quot;</span><span class="p">],</span>
                            <span class="n">ranges</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="mi">13</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">13</span><span class="p">)]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice21&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply1&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">10</span><span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice22&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply2&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<p>From HugeCTR v.3.3, the aforementioned, Slice layer based branching can be abstracted away. When the same tensor is referenced multiple times in constructing a model in Python, the HugeCTR parser can internally add a Slice layer to handle such a situation. Thus, the example below behaves as the same as the one above whilst simplifying the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply1&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">10</span><span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">WeightMultiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight_multiply2&quot;</span><span class="p">],</span>
                            <span class="n">weight_dims</span><span class="o">=</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="dropout-layer">
<h3>Dropout Layer<a class="headerlink" href="#dropout-layer" title="Permalink to this headline"></a></h3>
<p>The Dropout layer randomly zeroizes or drops some of the input elements.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dropout_rate</span></code>: Float, The dropout rate to be used for the <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> layer. It should be between 0 and 1. Setting it to 0 indicates that there is no dropped element at all. The default value is 0.5.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elems)</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Dropout</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;relu1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dropout1&quot;</span><span class="p">],</span>
                            <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="elu-layer">
<h3>ELU Layer<a class="headerlink" href="#elu-layer" title="Permalink to this headline"></a></h3>
<p>The ELU layer represents the Exponential Linear Unit.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">elu_alpha</span></code>: Float, the scalar that decides the value where this ELU function saturates for negative values. The default value is 1.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ELU</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;elu1&quot;</span><span class="p">],</span>
                            <span class="n">elu_alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="relu-layer">
<h3>ReLU Layer<a class="headerlink" href="#relu-layer" title="Permalink to this headline"></a></h3>
<p>The ReLU layer represents the Rectified Linear Unit.</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;relu1&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="sigmoid-layer">
<h3>Sigmoid Layer<a class="headerlink" href="#sigmoid-layer" title="Permalink to this headline"></a></h3>
<p>The Sigmoid layer represents the Sigmoid Unit.</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sigmoid1&quot;</span><span class="p">]))</span>
</pre></div>
</div>
<p><strong>Note</strong>: The final sigmoid function is fused with the loss function to better utilize memory bandwidth, so do NOT add a Sigmoid layer before the loss layer.</p>
</div>
<div class="section" id="interaction-layer">
<h3>Interaction Layer<a class="headerlink" href="#interaction-layer" title="Permalink to this headline"></a></h3>
<p>The interaction layer is used to explicitly capture second-order interactions between features.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: {(batch_size, num_elems), (batch_size, num_feas, num_elems)} where the first tensor typically represents a fully connected layer and the second is an embedding.</p></li>
<li><p>output: (batch_size, output_dim) where output_dim = num_elems + (num_feas + 1) * (num_feas + 2 ) / 2 - (num_feas + 1) + 1</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Interaction</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;layer1&quot;</span><span class="p">,</span> <span class="s2">&quot;layer3&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;interaction1&quot;</span><span class="p">]))</span>
</pre></div>
</div>
<p><strong>Important Notes</strong>:
There are optimizations that can be employed on the <code class="docutils literal notranslate"><span class="pre">Interaction</span></code> layer and the following <code class="docutils literal notranslate"><span class="pre">GroupFusedInnerProduct</span></code> layer during fp16 training. In this case, you should specify two output tensor names for the <code class="docutils literal notranslate"><span class="pre">Interaction</span></code> layer, and use them as the input tensors for the following <code class="docutils literal notranslate"><span class="pre">GroupFusedInnerProduct</span></code> layer. Please refer to the example of <a class="reference internal" href="#groupdenselayer"><span class="std std-doc">GroupDenseLayer</span></a> for the detailed usage.</p>
</div>
<div class="section" id="add-layer">
<h3>Add Layer<a class="headerlink" href="#add-layer" title="Permalink to this headline"></a></h3>
<p>The Add layer adds up an arbitrary number of tensors that have the same size in an element-wise manner.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: Nx(batch_size, num_elems) where N is the number of input tensors</p></li>
<li><p>output: (batch_size, num_elems)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Add</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc4&quot;</span><span class="p">,</span> <span class="s2">&quot;reducesum1&quot;</span><span class="p">,</span> <span class="s2">&quot;reducesum2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="reducesum-layer">
<h3>ReduceSum Layer<a class="headerlink" href="#reducesum-layer" title="Permalink to this headline"></a></h3>
<p>The ReduceSum Layer sums up all the elements across a specified dimension.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">axis</span></code>:  Integer, the dimension to reduce for the <code class="docutils literal notranslate"><span class="pre">ReduceSum</span></code> layer. If the input is N-dimensional, 0 &lt;= axis &lt; N. The default value is 1.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, …) where … represents any number of elements with an arbitrary number of dimensions</p></li>
<li><p>output: Dimension corresponding to axis is set to 1. The others remain the same as the input.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fmorder2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reducesum1&quot;</span><span class="p">],</span>
                            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="section" id="gru-layer">
<h4>GRU Layer<a class="headerlink" href="#gru-layer" title="Permalink to this headline"></a></h4>
<p>The GRU layer is Gated Recurrent Unit.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_output</span></code>: Number of output elements.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batchsize</span></code>: Number of batchsize.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SeqLength</span></code>: Length of the sequence.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vector_size</span></code>: size of the input vector.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_init_type</span></code>: Specifies how to initialize the weight array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias_init_type</span></code>: Specifies how to initialize the bias array. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Uniform</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.XavierUniform</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Zero</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Initializer_t.Default</span></code>.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (1, batch_size<em>SeqLength</em>embedding_vec_size)</p></li>
<li><p>output: (1, batch_size<em>SeqLength</em>embedding_vec_size)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">GRU</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;GRU1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;conncat1&quot;</span><span class="p">],</span>
                            <span class="n">num_output</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                            <span class="n">batchsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span>
                            <span class="n">SeqLength</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                            <span class="n">vector_size</span><span class="o">=</span><span class="mi">20</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="preludice-layer">
<h4>PReLUDice Layer<a class="headerlink" href="#preludice-layer" title="Permalink to this headline"></a></h4>
<p>The PReLUDice layer represents the Parametric Rectified Linear Unit, which adaptively adjusts the rectified point according to distribution of input data.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">elu_alpha</span></code>: A scalar that decides the value where this activation function saturates for negative values.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eps</span></code>: Epsilon value used in the PReLU/Dice formula.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, *) where * represents any number of elements</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">PReLU_Dice</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc_din_i1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dice_1&quot;</span><span class="p">],</span>
                            <span class="n">elu_alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="scale-layer">
<h4>Scale Layer<a class="headerlink" href="#scale-layer" title="Permalink to this headline"></a></h4>
<p>The Scale layer scales the input 2D tensor to specific size on the designate axis.</p>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">axis</span></code>: Along the designate axis to scale the tensor. The designate axis could be axis 0, 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">factor</span> </code>: scale factor.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elems)</p></li>
<li><p>output: if axis = 0; (batch_size, num_elems * factor), if axis = 1; (batch_size * factor, num_elems)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Scale</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;item1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Scale_item&quot;</span><span class="p">],</span>
                            <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">factor</span> <span class="o">=</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="fusedreshapeconcat-layer">
<h4>FusedReshapeConcat Layer<a class="headerlink" href="#fusedreshapeconcat-layer" title="Permalink to this headline"></a></h4>
<p>The FusedReshapeConcat layer cross combines the input tensors and outputs item tensor, AD tensor.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: {(batch_size, num_feas + 1, num_elems_0), (batch_size, num_feas + 1, num_elems_1), …}, the input tensors are embeddings.</p></li>
<li><p>output: {(batch_size x num_feas, (num_elems_0 + num_elems_1 + …)), (batch_size, (num_elems_0 + num_elems_1 + …))}.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">FusedReshapeConcat</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sparse_embedding_good&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_embedding_cate&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;FusedReshapeConcat_item_his_em&quot;</span><span class="p">,</span> <span class="s2">&quot;FusedReshapeConcat_item&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="fusedreshapeconcatgeneral-layer">
<h4>FusedReshapeConcatGeneral Layer<a class="headerlink" href="#fusedreshapeconcatgeneral-layer" title="Permalink to this headline"></a></h4>
<p>The FusedReshapeConcatGeneral layer cross combines the input tensors and outputs item tensor, AD tensor.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: {(batch_size, num_feas, num_elems_0), (batch_size, num_feas, num_elems_1), …}, the input tensors are embeddings.</p></li>
<li><p>output: (batch_size x num_feas, (num_elems_0 + num_elems_1 + …)).</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">FusedReshapeConcatGeneral</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sparse_embedding_good&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_embedding_cate&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;FusedReshapeConcat_item_his_em&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="softmax-layer">
<h4>Softmax Layer<a class="headerlink" href="#softmax-layer" title="Permalink to this headline"></a></h4>
<p>The Softmax layer computes softmax activations.</p>
<p>Parameter: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elems)</p></li>
<li><p>output: same as input</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Softmax</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reshape1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;softmax_i&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="sub-layer">
<h4>Sub Layer<a class="headerlink" href="#sub-layer" title="Permalink to this headline"></a></h4>
<p>Inputs: x tensor, y tensor in same size.
Produce x - y in element wise manner.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: Nx(batch_size, num_elems) where N is the number of input tensors</p></li>
<li><p>output: (batch_size, num_elems)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Sub</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Scale_item1&quot;</span><span class="p">,</span> <span class="s2">&quot;item_his1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sub_ih&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="reducemean-layer">
<h4>ReduceMean Layer<a class="headerlink" href="#reducemean-layer" title="Permalink to this headline"></a></h4>
<p>The ReduceMean Layer computes the mean of elements across a specified dimension.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">axis</span></code>: The dimension to reduce. If the input is N-dimensional, 0 &lt;= axis &lt; N.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, …) where … represents any number of elements with an arbitrary number of dimensions</p></li>
<li><p>output: Dimension corresponding to axis is set to 1. The others remain the same as the input.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fmorder2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reducemean1&quot;</span><span class="p">],</span>
                            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="matrixmutiply-layer">
<h4>MatrixMutiply Layer<a class="headerlink" href="#matrixmutiply-layer" title="Permalink to this headline"></a></h4>
<p>The MatrixMutiply Layer is a binary operation that produces a matrix output from two matrix inputs by performing matrix mutiplication.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: 2D: (m, n), (n, k) or 3D: (batch_size, m, n), (batch_size, n, k)</p></li>
<li><p>output: 2D: (m, k) or 3D: (batch_size, m, k)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">MatrixMutiply</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;slice1&quot;</span><span class="p">,</span><span class="s2">&quot;slice2&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;MatrixMutiply1&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="multiheadattention-layer">
<h4>MultiHeadAttention Layer<a class="headerlink" href="#multiheadattention-layer" title="Permalink to this headline"></a></h4>
<p>The MultiHeadAttention Layer is a binary operation that produces a matrix output from two matrix inputs by performing matrix mutiplication.</p>
<p>Parameters: None</p>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: 4D: (head_num, batch_size, m, n), (head_num, batch_size, k, n)</p></li>
<li><p>output: 4D: (head_num, batch_size, m, k)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">MultiHeadAttention</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">,</span><span class="s2">&quot;key&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;AttentionOut1&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="gather-layer">
<h4>Gather Layer<a class="headerlink" href="#gather-layer" title="Permalink to this headline"></a></h4>
<p>The Gather layer gather multiple output tensor slices from an input tensors on the last dimension.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">indices</span></code>: A list of indices in which each one represents an index in the input tensor to generate the corresponding output tensor. For example, [2, 8] indicates the second and eights tensor slice in the input tensor which are used to create an output tensor.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: (batch_size, num_elems)</p></li>
<li><p>output: (num_indices, num_elems)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Gather</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reshape1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gather1&quot;</span><span class="p">],</span>
                            <span class="n">indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="binarycrossentropyloss">
<h3>BinaryCrossEntropyLoss<a class="headerlink" href="#binarycrossentropyloss" title="Permalink to this headline"></a></h3>
<p>BinaryCrossEntropyLoss calculates loss from labels and predictions where each label is binary. The final sigmoid function is fused with the loss function to better utilize memory bandwidth.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code>: Boolean, whether to use regulariers. THe default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">regularizer_type</span></code>: The regularizer type for the <code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropyLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> or <code class="docutils literal notranslate"><span class="pre">MultiCrossEntropyLoss</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L2</span></code>. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code> is False. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lambda</span></code>: Float, the lambda value of the regularization term. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularier</span></code> is False. The default value is 0.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: [(batch_size, 1), (batch_size, 1)] where the first tensor represents the predictions while the second tensor represents the labels</p></li>
<li><p>output: (batch_size, 1)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">BinaryCrossEntropyLoss</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="crossentropyloss">
<h3>CrossEntropyLoss<a class="headerlink" href="#crossentropyloss" title="Permalink to this headline"></a></h3>
<p>CrossEntropyLoss calculates loss from labels and predictions between the forward propagation phases and backward propagation phases. It assumes that each label is two-dimensional.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code>: Boolean, whether to use regulariers. THe default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">regularizer_type</span></code>: The regularizer type for the <code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropyLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> or <code class="docutils literal notranslate"><span class="pre">MultiCrossEntropyLoss</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L2</span></code>. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code> is False. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lambda</span></code>: Float, the lambda value of the regularization term. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularier</span></code> is False. The default value is 0.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: [(batch_size, 2), (batch_size, 2)] where the first tensor represents the predictions while the second tensor represents the labels</p></li>
<li><p>output: (batch_size, 2)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span>
                            <span class="n">use_regularizer</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                            <span class="n">regularizer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Regularizer_t</span><span class="o">.</span><span class="n">L2</span><span class="p">,</span>
                            <span class="k">lambda</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="multicrossentropyloss">
<h3>MultiCrossEntropyLoss<a class="headerlink" href="#multicrossentropyloss" title="Permalink to this headline"></a></h3>
<p>MultiCrossEntropyLoss calculates loss from labels and predictions between the forward propagation phases and backward propagation phases. It allows labels in an arbitrary dimension, but all the labels must be in the same shape.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code>: Boolean, whether to use regulariers. THe default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">regularizer_type</span></code>: The regularizer type for the <code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropyLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> or <code class="docutils literal notranslate"><span class="pre">MultiCrossEntropyLoss</span></code> layer. The supported types include <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code> and <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L2</span></code>. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularizer</span></code> is False. The default value is <code class="docutils literal notranslate"><span class="pre">hugectr.Regularizer_t.L1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lambda</span></code>: Float, the lambda value of the regularization term. It will be ignored if <code class="docutils literal notranslate"><span class="pre">use_regularier</span></code> is False. The default value is 0.</p></li>
</ul>
<p>Input and Output Shapes:</p>
<ul class="simple">
<li><p>input: [(batch_size, *), (batch_size, *)] where the first tensor represents the predictions while the second tensor represents the labels. * represents any even number of elements.</p></li>
<li><p>output: (batch_size, *)</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">MultiCrossEntropyLoss</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span>
                            <span class="n">use_regularizer</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                            <span class="n">regularizer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Regularizer_t</span><span class="o">.</span><span class="n">L1</span><span class="p">,</span>
                            <span class="k">lambda</span> <span class="o">=</span> <span class="mf">0.1</span>
                            <span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="embedding-collection">
<h2>Embedding Collection<a class="headerlink" href="#embedding-collection" title="Permalink to this headline"></a></h2>
<div class="section" id="about-the-hugectr-embedding-collection">
<h3>About the HugeCTR embedding collection<a class="headerlink" href="#about-the-hugectr-embedding-collection" title="Permalink to this headline"></a></h3>
<p>Embedding collection is introduced in the v3.7 release.
The embedding collection enables you to use embeddings with different vector sizes, optimizers, and arbitrary table placement strategy.
Compared with the <code class="docutils literal notranslate"><span class="pre">hugectr.SparseEmbedding</span></code> class, the embedding collection has three key advantages:</p>
<ol class="arabic simple">
<li><p>The embedding collection can fuse embedding tables with different embedding vector sizes.
The previous embedding can only fuse embedding tables with the same embedding vector size.
The enhancement boosts both flexibility and performance.</p></li>
<li><p>The embedding collection extends the functionality of embedding by supporting the <code class="docutils literal notranslate"><span class="pre">concat</span></code> combiner and supporting different slot lookups on the same embedding table.</p></li>
<li><p>The embedding collection supports arbitrary embedding table placement, such as data parallel and model parallel.
You provide a plan JSON file and specify the table placement strategy that you want.</p></li>
</ol>
</div>
<div class="section" id="sample-notebook">
<h3>Sample Notebook<a class="headerlink" href="#sample-notebook" title="Permalink to this headline"></a></h3>
<p>The <a class="reference internal" href="../notebooks/embedding_collection.html"><span class="doc std std-doc">HugeCTR Embedding Collection</span></a> sample notebook demonstrates the following:</p>
<ul class="simple">
<li><p>Introduces the API of the embedding collection.</p></li>
<li><p>Introduces the embedding table placement strategy (ETPS) and how to configure ETPS in embedding collection.</p></li>
<li><p>Shows how to use an embedding collection in a DLRM model with the Criteo dataset for training and evaluation.
The notebook shows two different ETPS as reference.</p></li>
</ul>
</div>
<div class="section" id="overview-of-using-the-hugectr-embedding-collection">
<h3>Overview of using the HugeCTR embedding collection<a class="headerlink" href="#overview-of-using-the-hugectr-embedding-collection" title="Permalink to this headline"></a></h3>
<p>To use an embedding collection, you need the following items:</p>
<ul class="simple">
<li><p>A list of <code class="docutils literal notranslate"><span class="pre">hugectr.EmbeddingTableConfig</span></code> objects that represent the embedding tables such as the size, maximum key, and an optional optimizer.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">hugectr.EmbeddingPlanner</span></code> object that uses the embedding table config objects to organize the lookup operations between the input data and the embedding tables.
The embedding planner also accepts a JSON-formatted plan file that describes the embedding table placement strategy.</p></li>
</ul>
<p>After those items are in place, you run the <code class="docutils literal notranslate"><span class="pre">create_embedding_collection()</span></code> method on the embedding planner and you receive a
<code class="docutils literal notranslate"><span class="pre">hugectr.EmbeddingCollection</span></code>.
Specify the embedding collection as an argument to <code class="docutils literal notranslate"><span class="pre">Model.add()</span></code> to use the embedding collection.</p>
</div>
<div class="section" id="embeddingtableconfig">
<h3>EmbeddingTableConfig<a class="headerlink" href="#embeddingtableconfig" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">hugectr.EmbeddingTableConfig</span></code> class enables you to specify the attributes of an embedding table.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">table_id</span></code>: Integer, typically it is the index from the corresponding <code class="docutils literal notranslate"><span class="pre">slot_size_array</span></code> variable when you create a new embedding table.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_vocabulary_size</span></code>: Integer, specifies the vocabulary size of this table.
If positive, then the value indicates the number of embedding vectors that this table contains.
If you specify the value incorrectly and exceed the value during training or evaluation, you will cause an overflow and receive an error.
If you do not know the exact size of the embedding table, you can specify <code class="docutils literal notranslate"><span class="pre">-1</span></code> to use a dynamic embedding table with a size that can be extended dynamically during training or evaluation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ev_size</span></code>: Integer, specifies the embedding vector size that this embedding consists of.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">min_key</span></code>: Integer, the minimum value of the input key.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_key</span></code>: Integer, the maximum value of the input key.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">opt_params</span></code>: Optional, <code class="docutils literal notranslate"><span class="pre">hugectr.Optimizer</span></code>, the optimizer you want to use for this embedding table.
If not specified, the embedding table uses the optimizer specified in <code class="docutils literal notranslate"><span class="pre">hugectr.Model</span></code>.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the embedding table.</span>
<span class="n">slot_size_array</span> <span class="o">=</span> <span class="p">[</span><span class="mi">203931</span><span class="p">,</span> <span class="mi">18598</span><span class="p">,</span> <span class="mi">14092</span><span class="p">,</span> <span class="mi">7012</span><span class="p">,</span> <span class="mi">18977</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6385</span><span class="p">,</span> <span class="mi">1245</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span>
                   <span class="mi">186213</span><span class="p">,</span> <span class="mi">71328</span><span class="p">,</span> <span class="mi">67288</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">2168</span><span class="p">,</span> <span class="mi">7338</span><span class="p">,</span> <span class="mi">61</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">932</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span>
                   <span class="mi">204515</span><span class="p">,</span> <span class="mi">141526</span><span class="p">,</span> <span class="mi">199433</span><span class="p">,</span> <span class="mi">60919</span><span class="p">,</span> <span class="mi">9137</span><span class="p">,</span> <span class="mi">71</span><span class="p">,</span> <span class="mi">34</span><span class="p">]</span>
<span class="n">embedding_table_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">slot_size_array</span><span class="p">):</span>
    <span class="n">embedding_table_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">hugectr</span><span class="o">.</span><span class="n">EmbeddingTableConfig</span><span class="p">(</span>
            <span class="n">table_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
            <span class="n">max_vocabulary_size</span><span class="o">=</span><span class="n">slot_size_array</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">ev_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">min_key</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">max_key</span><span class="o">=</span><span class="n">slot_size_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="embeddingplanner">
<h3>EmbeddingPlanner<a class="headerlink" href="#embeddingplanner" title="Permalink to this headline"></a></h3>
<p>Create a <code class="docutils literal notranslate"><span class="pre">hugectr.EmbeddingPlanner</span></code> instance to construct the lookup operation and create a <code class="docutils literal notranslate"><span class="pre">hugectr.EmbeddingCollection</span></code>.
The constructor for the embedding planner class does not accept any parameters.</p>
<div class="section" id="embedding-lookup-method">
<h4>embedding_lookup method<a class="headerlink" href="#embedding-lookup-method" title="Permalink to this headline"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">embedding_lookup</span></code> method enables you to specify the lookup operations between the input data and an embedding table.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">table_config</span></code> : <code class="docutils literal notranslate"><span class="pre">hugectr.EmbeddingTableConfig</span></code>, the embedding table for the lookup operation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bottom_name</span></code>: str, the bottom tensor name.
Specify a tensor that is compatible with the <code class="docutils literal notranslate"><span class="pre">data_reader_sparse_param_array</span></code> parameter of <a class="reference internal" href="#input-layer"><span class="std std-doc"><code class="docutils literal notranslate"><span class="pre">hugectr.Input</span></code></span></a> in <code class="docutils literal notranslate"><span class="pre">hugectr.Model</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_name</span></code>: str, the output tensor name.
The shape of output tensor is (<code class="docutils literal notranslate"><span class="pre">&lt;batch</span> <span class="pre">size&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;embedding</span> <span class="pre">vector</span> <span class="pre">size&gt;</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">combiner</span></code>: str, specifies the combiner operation.
Specify <code class="docutils literal notranslate"><span class="pre">mean</span></code>, <code class="docutils literal notranslate"><span class="pre">sum</span></code>, or <code class="docutils literal notranslate"><span class="pre">concat</span></code>.</p></li>
</ul>
</div>
<div class="section" id="create-embedding-collection-method">
<h4>create_embedding_collection method<a class="headerlink" href="#create-embedding-collection-method" title="Permalink to this headline"></a></h4>
<p>After configuring the embedding planner with the embedding table information by running <code class="docutils literal notranslate"><span class="pre">embedding_lookup()</span></code> for each embedding table, you can run the <code class="docutils literal notranslate"><span class="pre">create_embedding_collection()</span></code> method to create a <code class="docutils literal notranslate"><span class="pre">hugectr.EmbeddingCollection</span></code> instance.
You can use the <code class="docutils literal notranslate"><span class="pre">add()</span></code> method from <code class="docutils literal notranslate"><span class="pre">hugectr.Model</span></code> to use the embedding collection for training and evaluation.</p>
<p>Parameter:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">plan_file</span></code>: str, specifies the path to a JSON file that describes the embedding table placement strategy.
Example:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_planner</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">EmbeddingPlanner</span><span class="p">()</span>
<span class="n">emb_vec_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">slot_size_array</span><span class="p">)):</span>
     <span class="n">embedding_planner</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
        <span class="n">table_config</span><span class="o">=</span><span class="n">embedding_table_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
        <span class="n">bottom_name</span><span class="o">=</span><span class="s2">&quot;data</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
        <span class="n">top_name</span><span class="o">=</span><span class="s2">&quot;emb_vec</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
        <span class="n">combiner</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span>
    <span class="p">)</span>
<span class="n">embedding_collection</span> <span class="o">=</span> <span class="n">embedding_planner</span><span class="o">.</span><span class="n">create_embedding_collection</span><span class="p">(</span><span class="s2">&quot;./plan.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="plan-file-and-embedding-table-placement-strategy-etps">
<h3>Plan File and Embedding Table Placement Strategy (ETPS)<a class="headerlink" href="#plan-file-and-embedding-table-placement-strategy-etps" title="Permalink to this headline"></a></h3>
</div>
<div class="section" id="about-etps-and-benefits">
<h3>About ETPS and Benefits<a class="headerlink" href="#about-etps-and-benefits" title="Permalink to this headline"></a></h3>
<p>In the recommendation system, the embedding table is usually so large that a single GPU is not able to hold all embedding tables.
One strategy for addressing the challenge is to use sharding to distribute the embedding tables across multiple GPUs.
We call this sharding strategy the embedding table placement strategy (ETPS).</p>
<p>ETPS can significantly boost the performance of embedding because different sharding strategies influence the communication between GPUs.
The optimal strategy is highly dependent on your dataset and your lookup operation.
If optimal performance is a concern, then configure an ETPS for the specific use case.</p>
</div>
<div class="section" id="configuring-etps-and-the-embedding-collection">
<h3>Configuring ETPS and the Embedding Collection<a class="headerlink" href="#configuring-etps-and-the-embedding-collection" title="Permalink to this headline"></a></h3>
<p>HugeCTR provides a configurable ETPS interface so that you can adjust the embedding table placement strategy according your own use case.
You create a JSON file that describes the ETPS in all GPUs.
This file is called a <em>plan file</em>.</p>
<p>For example, consider the following scenario that has four embedding tables and five lookup operations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">slot_size_array</span> <span class="o">=</span> <span class="p">[</span><span class="o">...</span><span class="p">]</span>
<span class="n">embedding_table_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">slot_size_array</span><span class="p">)):</span>
    <span class="n">embedding_table_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">hugectr</span><span class="o">.</span><span class="n">EmbeddingTableConfig</span><span class="p">(</span>
            <span class="n">table_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
            <span class="n">max_vocabulary_size</span><span class="o">=</span><span class="n">slot_size_array</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">ev_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">min_key</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">max_key</span><span class="o">=</span><span class="n">slot_size_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="p">)</span>
    <span class="p">)</span>

<span class="n">embedding_planner</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">EmbeddingPlanner</span><span class="p">()</span>
<span class="n">embedding_planner</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embedding_table_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;data0&quot;</span><span class="p">,</span> <span class="s2">&quot;emb_vec0&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">)</span> <span class="c1"># lookup 0, table 0</span>
<span class="n">embedding_planner</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embedding_table_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;data1&quot;</span><span class="p">,</span> <span class="s2">&quot;emb_vec1&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">)</span> <span class="c1"># lookup 1, table 1</span>
<span class="n">embedding_planner</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embedding_table_list</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s2">&quot;data2&quot;</span><span class="p">,</span> <span class="s2">&quot;emb_vec2&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">)</span> <span class="c1"># lookup 2, table 2</span>
<span class="n">embedding_planner</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embedding_table_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;data3&quot;</span><span class="p">,</span> <span class="s2">&quot;emb_vec3&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">)</span> <span class="c1"># lookup 3, table 1</span>
<span class="n">embedding_planner</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embedding_table_list</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="s2">&quot;data4&quot;</span><span class="p">,</span> <span class="s2">&quot;emb_vec4&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">)</span> <span class="c1"># lookup 4, table 3</span>
</pre></div>
</div>
<p>The next step is to configure the ETPS through a plan file.
In the plan file, you can group several lookup operations together and to perform sharding.
You can specify the configuration in fine detail, down to the lookup operation, the GPU, and a portion of the embedding table.</p>
<p>The basic principle is one embedding table can only be sharded in a single way.
For example, if lookup <code class="docutils literal notranslate"><span class="pre">0</span></code> and lookup <code class="docutils literal notranslate"><span class="pre">3</span></code> take place on the same embedding table, then lookup <code class="docutils literal notranslate"><span class="pre">0</span></code> and lookup <code class="docutils literal notranslate"><span class="pre">3</span></code> should be grouped together and sharded in the same way.</p>
<div class="section" id="sample-etps-plan-file-data-parallel">
<h4>Sample ETPS Plan File: Data Parallel<a class="headerlink" href="#sample-etps-plan-file-data-parallel" title="Permalink to this headline"></a></h4>
<p>The following sample plan file shows how to use two GPUs and a data parallel ETPS in four embedding tables:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="w"></span>
<span class="w">  </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;local_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;global_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="w"></span>
<span class="w">              </span><span class="p">],</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="w"></span>
<span class="w">              </span><span class="p">]</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;shards_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;shard_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;table_placement_strategy&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;dp&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">],</span><span class="w"></span>
<span class="w">  </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;local_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;global_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="w"></span>
<span class="w">              </span><span class="p">],</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="w"></span>
<span class="w">              </span><span class="p">]</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;shards_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;shard_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;table_placement_strategy&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;dp&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">]</span><span class="w"></span>
<span class="p">]</span><span class="w"></span>
</pre></div>
</div>
<p>The plan file consists of a list that describes the table placement strategy in each GPU, in order.
For each GPU, a list describes the multiple groups of sharded lookup operations.
Each group of sharded lookup operation is a dictionary with the following fields:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">local_embedding_list</span></code>: a list of integers, specifies the lookup operations for the current GPU.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">global_embedding_list</span></code>: a list of lists of integers, specifies the current group lookup operations in all GPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shards_count</span></code>: an integer, specifies the total number of shards for the current group lookup operations.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shard_id</span></code>: an integer, the index of the current group lookup operations.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">table_placement_strategy</span></code>: str, can be <code class="docutils literal notranslate"><span class="pre">mp</span></code> or <code class="docutils literal notranslate"><span class="pre">dp</span></code>. <code class="docutils literal notranslate"><span class="pre">mp</span></code> indicates model parallel and <code class="docutils literal notranslate"><span class="pre">dp</span></code> indicates data parallel.</p></li>
</ul>
</div>
<div class="section" id="sample-etps-plan-file-model-parallel-and-data-parallel-combined">
<h4>Sample ETPS Plan File: Model Parallel and Data Parallel Combined<a class="headerlink" href="#sample-etps-plan-file-model-parallel-and-data-parallel-combined" title="Permalink to this headline"></a></h4>
<p>You can apply more complex strategies for ETPS.
The following sample plan file demonstrates how to shared lookups <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code>, and <code class="docutils literal notranslate"><span class="pre">3</span></code> across two GPUs and lookup <code class="docutils literal notranslate"><span class="pre">4</span></code> is data parallel:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="w"></span>
<span class="w">  </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;local_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="mi">0</span><span class="p">,</span><span class="w"></span>
<span class="w">              </span><span class="mi">2</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;global_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">0</span><span class="p">,</span><span class="w"></span>
<span class="w">                  </span><span class="mi">2</span><span class="w"></span>
<span class="w">              </span><span class="p">],</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">1</span><span class="p">,</span><span class="w"></span>
<span class="w">                  </span><span class="mi">3</span><span class="w"></span>
<span class="w">              </span><span class="p">]</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;table_placement_strategy&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;mp&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">},</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;local_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="mi">4</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;global_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">4</span><span class="w"></span>
<span class="w">              </span><span class="p">],</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">4</span><span class="w"></span>
<span class="w">              </span><span class="p">]</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;table_placement_strategy&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;dp&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">],</span><span class="w"></span>
<span class="w">  </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;local_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="mi">1</span><span class="p">,</span><span class="w"></span>
<span class="w">              </span><span class="mi">3</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;global_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">0</span><span class="p">,</span><span class="w"></span>
<span class="w">                  </span><span class="mi">2</span><span class="w"></span>
<span class="w">              </span><span class="p">],</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">1</span><span class="p">,</span><span class="w"></span>
<span class="w">                  </span><span class="mi">3</span><span class="w"></span>
<span class="w">              </span><span class="p">]</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;table_placement_strategy&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;mp&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">},</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;local_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="mi">4</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;global_embedding_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">4</span><span class="w"></span>
<span class="w">              </span><span class="p">],</span><span class="w"></span>
<span class="w">              </span><span class="p">[</span><span class="w"></span>
<span class="w">                  </span><span class="mi">4</span><span class="w"></span>
<span class="w">              </span><span class="p">]</span><span class="w"></span>
<span class="w">          </span><span class="p">],</span><span class="w"></span>
<span class="w">          </span><span class="nt">&quot;table_placement_strategy&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;dp&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">]</span><span class="w"></span>
<span class="p">]</span><span class="w"></span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="groupdenselayer">
<h2>GroupDenseLayer<a class="headerlink" href="#groupdenselayer" title="Permalink to this headline"></a></h2>
<p><strong>DenseLayer class</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hugectr</span><span class="o">.</span><span class="n">GroupDenseLayer</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">GroupDenseLayer</span></code> specifies the parameters related to a group of dense layers. HugeCTR currently supports only <code class="docutils literal notranslate"><span class="pre">GroupFusedInnerProduct</span></code>, which is comprised of multiple <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> layers. Please <strong>NOTE</strong> that the <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> layer only supports fp16.</p>
<p><strong>Arguments</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">group_layer_type</span></code>: The layer type to be used. There is only one supported type, i.e., <code class="docutils literal notranslate"><span class="pre">hugectr.GroupLayer_t.GroupFusedInnerProduct</span></code>. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bottom_name_list</span></code>: List[str], the list of bottom tensor names for the first dense layer in this group. Currently, the <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> layer at the head position can take one or two input tensors. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_name_list</span></code>: List[str], the list of top tensor names of each dense layer in the group. There should be only one name for each layer. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_outputs</span></code>: List[Integer], the number of output elements for each <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> layer in the group. There is NO default value and it should be specified by users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">last_act_type</span></code>: The activation type of the last <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> layer in the group. The supported types include <code class="docutils literal notranslate"><span class="pre">Activation_t.Relu</span></code> and <code class="docutils literal notranslate"><span class="pre">Activation_t.Non</span></code>. Except the last layer, the activation type of the other <code class="docutils literal notranslate"><span class="pre">FusedInnerProduct</span></code> layers in the group must be and will be automatically set as <code class="docutils literal notranslate"><span class="pre">Activation_t.Relu</span></code>, which do not allow any configurations. The default value is <code class="docutils literal notranslate"><span class="pre">Activation_t.Relu</span></code>.</p></li>
</ul>
<p><strong>NOTE</strong>: There should be at least two layers in the group, and the size of <code class="docutils literal notranslate"><span class="pre">top_name_list</span></code> and <code class="docutils literal notranslate"><span class="pre">num_outputs</span></code> should both be equal to the number of layers.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">GroupDenseLayer</span><span class="p">(</span><span class="n">group_layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">GroupLayer_t</span><span class="o">.</span><span class="n">GroupFusedInnerProduct</span><span class="p">,</span>
                                  <span class="n">bottom_name</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">],</span>
                                  <span class="n">top_name_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc1&quot;</span><span class="p">,</span> <span class="s2">&quot;fc2&quot;</span><span class="p">,</span> <span class="s2">&quot;fc3&quot;</span><span class="p">],</span>
                                  <span class="n">num_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span>
                                  <span class="n">last_act_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Activation_t</span><span class="o">.</span><span class="n">Relu</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">Interaction</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc3&quot;</span><span class="p">,</span><span class="s2">&quot;sparse_embedding1&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;interaction1&quot;</span><span class="p">,</span> <span class="s2">&quot;interaction1_grad&quot;</span><span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">GroupDenseLayer</span><span class="p">(</span><span class="n">group_layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">GroupLayer_t</span><span class="o">.</span><span class="n">GroupFusedInnerProduct</span><span class="p">,</span>
                            <span class="n">bottom_name_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;interaction1&quot;</span><span class="p">,</span> <span class="s2">&quot;interaction1_grad&quot;</span><span class="p">],</span>
                            <span class="n">top_name_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc4&quot;</span><span class="p">,</span> <span class="s2">&quot;fc5&quot;</span><span class="p">,</span> <span class="s2">&quot;fc6&quot;</span><span class="p">,</span> <span class="s2">&quot;fc7&quot;</span><span class="p">,</span> <span class="s2">&quot;fc8&quot;</span><span class="p">],</span>
                            <span class="n">num_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                            <span class="n">last_act_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Activation_t</span><span class="o">.</span><span class="n">Non</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hugectr</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">layer_type</span> <span class="o">=</span> <span class="n">hugectr</span><span class="o">.</span><span class="n">Layer_t</span><span class="o">.</span><span class="n">BinaryCrossEntropyLoss</span><span class="p">,</span>
                            <span class="n">bottom_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fc8&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">],</span>
                            <span class="n">top_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="python_interface.html" class="btn btn-neutral float-left" title="HugeCTR Python Interface" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../additional_resources.html" class="btn btn-neutral float-right" title="Additional Resources" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v4.1.1
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../v3.8/api/hugectr_layer_book.html">v3.8</a></dd>
      <dd><a href="../../v3.9/api/hugectr_layer_book.html">v3.9</a></dd>
      <dd><a href="../../v3.9.1/api/hugectr_layer_book.html">v3.9.1</a></dd>
      <dd><a href="../../v4.0/api/hugectr_layer_book.html">v4.0</a></dd>
      <dd><a href="../../v4.1/api/hugectr_layer_book.html">v4.1</a></dd>
      <dd><a href="hugectr_layer_book.html">v4.1.1</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>