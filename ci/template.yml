stages:
  - build_from_scratch
  - format_check
  - build
  - pre_test
  - test
  - sok_benchmark
  - wdl_benchmark
  - dcn_benchmark
  - deepfm_benchmark
  - dlrm_benchmark
  - post_test

.python_format:
  stage: format_check
  tags:
    - nvidia.com/cuda.driver.major=470
    - $BUILD_TAG
  extends:
    - .format:rules:check
  script:
    - pwd
    - ls -all
    - docker pull python:3.8-alpine;
    - docker run -d --rm --name python_${CI_PIPELINE_ID} ${EXTRA_DOCKER_RUN_ARGS} -w /src python:3.8-alpine sleep infinity
    - docker cp $(pwd) python_${CI_PIPELINE_ID}:/src
    - docker exec python_${CI_PIPELINE_ID} sh -c 'pip install black==22.12.0 && pwd && ls -all . '
    - docker exec python_${CI_PIPELINE_ID} sh -c "black --line-length 100 --check --diff --color --extend-exclude \"$EXCLUDE\" ./hugectr"
  after_script:
    - docker stop python_${CI_PIPELINE_ID}
  allow_failure: false
  timeout: 15 minutes

.clang_format:
  stage: format_check
  tags:
    - nvidia.com/cuda.driver.major=470
    - $BUILD_TAG
  extends:
    - .format:rules:check
  script:
    - pwd
    - ls -all
    - docker login -u ${CI_PRIVATE_USER} -p "${CI_PRIVATE_KEY}" "${CI_REGISTRY}"
    - docker run -d --rm --name clang_${CI_PIPELINE_ID} ${EXTRA_DOCKER_RUN_ARGS} -w /src gitlab-master.nvidia.com:5005/dl/hugectr/hugectr/clang-format-lint-new sleep infinity
    - docker cp $(pwd) clang_${CI_PIPELINE_ID}:/src
    - docker exec clang_${CI_PIPELINE_ID} sh -c "cd ./hugectr && /run-clang-format.py --clang-format-executable /clang-format/$EXECUTABLE -r --exclude $EXCLUDE --style $STYLE --extensions $EXTENSIONS ."
  after_script:
    - docker stop clang_${CI_PIPELINE_ID}
  allow_failure: false
  timeout: 15 minutes

.codespell_check:
  stage: format_check
  tags:
    - nvidia.com/cuda.driver.major=470
    - $BUILD_TAG
  extends:
    - .format:rules:check
  script:
    - pwd
    - ls -all
    - docker pull ${PRE_COM_IMAGE}
    - docker run -d --rm --name codespell_${CI_PIPELINE_ID} ${EXTRA_DOCKER_RUN_ARGS} -w /src ${PRE_COM_IMAGE} sleep infinity
    - docker cp $(pwd) codespell_${CI_PIPELINE_ID}:/src
    - docker exec codespell_${CI_PIPELINE_ID} sh -c "cd ./hugectr && pre-commit run --all-files"
  after_script:
    - docker stop codespell_${CI_PIPELINE_ID}
  allow_failure: false
  timeout: 15 minutes

.build_nightly:
  stage: build_from_scratch
  tags:
    - nvidia.com/cuda.driver.major=470
    - $BUILD_TAG
  script:
    - docker login -u "\$oauthtoken" -p "${NVSTAGE_KEY}" "${NVSTAGE_REGISTRY}"
    - docker login -u ${CI_PRIVATE_USER} -p "${CI_PRIVATE_KEY}" "${CI_REGISTRY}"
    - docker login -u ${CI_PRIVATE_USER} -p "${CI_PRIVATE_KEY}" gitlab-master.nvidia.com
    - if [[ "$MERLIN_REMOTE_REPO" == "" ]]; then
        git clone $REMOTE_REPO;
      else
        git clone $MERLIN_REMOTE_REPO;
      fi
    - if [[ "$OPTIMIZED" == 1 ]]; then
        cd optimized/recommendation/hugectr;
      else
        cd Merlin/docker;
      fi
    - if [[ "$MERLIN_REMOTE_BRANCH" != "" ]]; then
        git checkout $MERLIN_REMOTE_BRANCH;
      fi
    - if [[ "$TEST_NEW_IMAGE" == "1" ]]; then
        DST_IMAGE=${DST_IMAGE}.new_image;
      fi
    - docker build --pull
      -t ${DST_IMAGE}
      -f ${DOCKER_FILE}
      $BUILD_ARGS
      --no-cache 
      . ;
    - docker push ${DST_IMAGE}
  allow_failure: false
  rules:
    - if: $NIGHTLY == "1"
      when: always
    - if: $TEST_NEW_IMAGE == "1"
      when: always
    - when: never
  timeout: 5 hours

# nightly build for sok tf1
.build_nightly_tf1:
  stage: build_from_scratch
  tags:
    - nvidia.com/cuda.driver.major=470
  script:
    - docker login -u ${CI_PRIVATE_USER} -p "${CI_PRIVATE_KEY}" "${CI_REGISTRY}"
    - cd tools/dockerfiles
    - docker build --pull
      -t ${DST_IMAGE}
      -f ${DOCKER_FILE}
      $BUILD_ARGS
      --no-cache
      . ;
    - docker push ${DST_IMAGE}
  allow_failure: false
  rules:
    - if: $NIGHTLY == "1"
      when: always
    - if: $TEST_NEW_IMAGE == "1"
      when: always
    - when: never
  timeout: 2 hours

.build:
  stage: build
  tags:
    - nvidia.com/cuda.driver.major=470
    - $BUILD_TAG
  script:
    - export JOB_DOCKERFILE="Dockerfile.${CI_JOB_NAME%%--*}.${CI_PIPELINE_ID}" && echo ${JOB_DOCKERFILE}
    - echo "BUILD_HUGECTR=${BUILD_HUGECTR}"
    - echo "BUILD_HUGECTR2ONNX=${BUILD_HUGECTR2ONNX}"
    - echo "BUILD_SOK=${BUILD_SOK}"
    - echo "BUILD_TF_PLUGIN=${BUILD_TF_PLUGIN}"
    - echo "BUILD_TORCH_PLUGIN=${BUILD_TORCH_PLUGIN}"
    #- git submodule update --init --recursive
    - if [[ "$TEST_NEW_IMAGE" == "1" ]]; then
        echo "FROM ${FROM_IMAGE}.new_image" > ${JOB_DOCKERFILE};
      else
        echo "FROM ${FROM_IMAGE}" > ${JOB_DOCKERFILE};
      fi
    - echo "WORKDIR /workdir" >> ${JOB_DOCKERFILE}
    - echo "RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/lib/libcuda.so.1" >> ${JOB_DOCKERFILE}
    - echo "COPY . ." >> ${JOB_DOCKERFILE}
    - echo "RUN git log -n 1" >> ${JOB_DOCKERFILE}
    - if [[ "$BUILD_HUGECTR" == 1 ]]; then
        echo "RUN cd /workdir && rm -rf build && mkdir -p build && cd build && cmake ${CMAKE_OPTION} .. && make -j\$(nproc) && make install" >> ${JOB_DOCKERFILE};
      fi
    - if [[ "$BUILD_SOK" == 1 ]]; then
        echo "RUN cd /workdir/sparse_operation_kit/ && SOK_COMPILE_GPU_SM=\"${CMAKE_OPTION}\" python setup.py install" >> ${JOB_DOCKERFILE};
        echo "RUN pip install nvtx" >> ${JOB_DOCKERFILE};
        echo "ENV LD_LIBRARY_PATH=/usr/local/hugectr/lib:/usr/local/lib:\$LD_LIBRARY_PATH" >> ${JOB_DOCKERFILE};
        echo "ENV LIBRARY_PATH=/usr/local/hugectr/lib:/usr/local/lib:\$LIBRARY_PATH" >> ${JOB_DOCKERFILE};
        echo "ENV PYTHONPATH=/workdir/sparse_operation_kit:\$PYTHONPATH" >> ${JOB_DOCKERFILE};
      fi
    - if [[ "$BUILD_HUGECTR2ONNX" == 1 ]]; then
        echo "RUN cd /workdir/onnx_converter && python3 setup.py install" >> ${JOB_DOCKERFILE};
      fi
    - if [[ "$BUILD_HDFS" == 1 ]]; then
        echo "RUN apt update -y && apt install -y --no-install-recommends sudo" >> ${JOB_DOCKERFILE};
      fi
    - echo "RUN rm /usr/local/lib/libcuda.so.1" >> ${JOB_DOCKERFILE};
    - cat ${JOB_DOCKERFILE}
    - docker login -u ${CI_PRIVATE_USER} -p "${CI_PRIVATE_KEY}" "${CI_REGISTRY}"
    - if [[ "$TEST_NEW_IMAGE" == "1" ]]; then
        docker pull ${FROM_IMAGE}.new_image;
      else
        docker pull ${FROM_IMAGE};
      fi
    - source sbin/docker_buildx.sh
    - BUILDX_ARGS="--push --no-cache -t ${DST_IMAGE} -f ${JOB_DOCKERFILE}"
    - docker_buildx::docker_buildx "$CI_RUNNER_ID" "${BUILDX_ARGS}"
  variables:
    GIT_SUBMODULE_STRATEGY: recursive
  allow_failure: false
  rules:
    - if: $CI_PIPELINE_SOURCE =~ /^(push|web|merge_request_event|trigger)$/
      when: always
    - if: $TEST_NEW_IMAGE == "1"
      when: always
    - when: never
  timeout: 5 hours

.test_local:
  stage: test
  extends:
    - .default:rules:weekly-test
  tags:
    - local_test
  script:
    - docker login -u ${CI_PRIVATE_USER} -p "${CI_PRIVATE_KEY}" "${CI_REGISTRY}"
    - echo "Container_Name=${CI_JOB_NAME}_${CI_PIPELINE_ID}"
    - echo "Image=${CONT}"
    - echo "Mounts=${MOUNTS}"
    - echo "Command=${CMD}"
    - docker pull ${CONT}
    - if [[ "${MOUNTS}" == "" ]]; then
        docker run --rm --name ${CI_JOB_NAME}_${CI_PIPELINE_ID} --net=host --gpus=all --privileged --runtime=nvidia --ulimit memlock=-1 --ulimit stack=67108864 --shm-size 16g -u root ${CONT} bash -cx "${CMD}";
      else
        docker run --rm --name ${CI_JOB_NAME}_${CI_PIPELINE_ID} --net=host --gpus=all --privileged --runtime=nvidia --ulimit memlock=-1 --ulimit stack=67108864 --shm-size 16g ${MOUNTS} -u root ${CONT} bash -cx "${CMD}";
      fi
#    - docker logs -f ${CI_JOB_NAME}_${CI_PIPELINE_ID}
  after_script:
    - docker stop ${CI_JOB_NAME}_${CI_PIPELINE_ID}

.build_hugectr:
  extends:
    - .build
    - .hugectr:rules:build

.build_hugectr_daily:
  extends:
    - .build
    - .default:rules:daily-test

.build_sok:
  extends:
    - .build
    - .sok:rules:build

.cluster_test_job:
  extends:
    - .selene_luna_job
    - .hugectr:rules:sanity-test
  variables:
    SLURM_ACCOUNT: coreai_devtech_hugectr
    OLD_SLURM_ACCOUNT: "devtech"
    GPFSFOLDER: "/lustre/fsw/$OLD_SLURM_ACCOUNT"
    GPFSFOLDER: "/lustre/fsw/$OLD_SLURM_ACCOUNT"
    DGXNNODES: 1
  allow_failure: false

.cluster_test_job_daily:
  extends:
    - .draco_oci_test_job
    - .default:rules:daily-test
  variables:
    DRACO_OCI_LOGDIR: /lustre/fsw/portfolios/coreai/users/svcnvdlfw/hugectr_ci/${CI_PIPELINE_ID}

.selene_test_job:
  extends:
    - .selene_luna_job
    - .hugectr:rules:test_in_child
  variables:
    SLURM_ACCOUNT: coreai_devtech_hugectr
    OLD_SLURM_ACCOUNT: "devtech"
    GPFSFOLDER: "/lustre/fsw/$OLD_SLURM_ACCOUNT"
    DGXNNODES: 1
    WALLTIME: "00:30:00"
    GIT_CLONE_PATH: /lustre/fsw/devtech/hpc-hugectr/hugectr-ci/$CI_CONCURRENT_ID/$CI_PROJECT_NAME
  allow_failure: false

.dracorno_test_job:
  extends:
    - .dracorno_job
    - .hugectr:rules:test_in_child
  variables:
    CLUSTER:       "dracorno"
    SLURM_ACCOUNT: "coreai_devtech_hugectr"
    SLURM_PARTITION: ${DRACO_SLURM_PARTITION}
    DATA_PREFIX: ${DRACO_DATA_PREFIX}
    WALLTIME:      "02:00:00"
    SBATCH_OTHER_PARAMS: "--nv-meta ml-model.hugectr --gpus-per-node=8"
  tags:
    - $RUNNER_TAG
  allow_failure: false

.draco_oci_test_job:
  extends:
    - .hugectr:rules:test_in_child
  script:
    - echo ${MOUNTS}
    - export LOG_DIR="${DRACO_OCI_LOGDIR}/${CI_JOB_NAME}/${CI_JOB_ID}/results"
    - echo ${LOG_DIR}
    - mkdir -p "${LOG_DIR}"
    - export LOG_FILE="${LOG_DIR}/${DATESTAMP}.log"
    - export CONT=$(echo "${CONT}" | sed 's/:5005//g')
    - chmod +x ${TEST_CMD}
    - bash ${TEST_CMD} | tee ${LOG_FILE}
  variables:
    CI_SLURM_ACCOUNT: "coreai_devtech_all" #svcnvdlfw only attends account "coreai_devtech_all"
    CI_SCHEDULER_TYPE: "slurm"
    CI_SLURM_PARTITION: "batch_block1"
    CI_SLURM_GPUS_PER_NODE: "8"
    DRACO_OCI_PREFIX: "/lustre/fsw/portfolios/coreai/projects/coreai_devtech_all/hugectr/hpc-hugectr"
    SLURM_JOB_NUM_NODES: 1
    CI_SLURM_TIME: "00:30:00"
    GIT_DEPTH: "1"
  tags:
    - draco_oci_generic
  allow_failure: false

.draco_oci_sok_test_job:
  extends:
      - .sok:rules:test_in_child
  script:
    - echo ${MOUNTS}
    - export LOG_DIR="${DRACO_OCI_LOGDIR}/${CI_JOB_NAME}/${CI_JOB_ID}/results"
    - echo ${LOG_DIR}
    - mkdir -p "${LOG_DIR}"
    - export LOG_FILE="${LOG_DIR}/${DATESTAMP}.log"
    - export CONT=$(echo "${CONT}" | sed 's/:5005//g')
    - chmod +x ${TEST_CMD}
    - bash ${TEST_CMD} | tee ${LOG_FILE}
  variables:
    CI_SLURM_ACCOUNT: "coreai_devtech_all" #svcnvdlfw only attends account "coreai_devtech_all""
    CI_SCHEDULER_TYPE: "slurm"
    CI_SLURM_PARTITION: "batch_block1"
    CI_SLURM_GPUS_PER_NODE: "8"
    DRACO_OCI_PREFIX: "/lustre/fsw/portfolios/coreai/projects/coreai_devtech_all/hugectr/hpc-hugectr"
    SLURM_JOB_NUM_NODES: 1
    CI_SLURM_TIME:      "00:30:00"
    GIT_DEPTH: "1"
  tags:
    - draco_oci_generic
  allow_failure: false

.dlcluster_test_job:
  extends:
    - .dlcluster_job
    - .hugectr:rules:sanity-test
  allow_failure: false

.dlcluster_test_job_daily:
  extends:
    - .dlcluster_job
    - .default:rules:daily-test
  allow_failure: false

.computelab_test_job_daily:
  extends:
    - .dlcluster_job
    - .default:rules:daily-test
  variables:
    CI_SLURM_PARTITION: "a100-pcie-80gb-product,h100-pcie-80gb-preprod,h100-pcie-80gb-production"
    CI_SLURM_ACCOUNT: "cag"
    CI_SCHEDULER_TYPE: docker
    GIT_DEPTH: "1"
    WALLTIME:      "02:00:00"
  tags:
    - computelab_generic
  allow_failure: false

.sok_test_job:
  extends:
    - .selene_luna_job
    - .sok:rules:sanity-test
  allow_failure: false

.selene_sok_test_job:
  extends:
    - .selene_luna_job
    - .sok:rules:test_in_child
  variables:
    SLURM_ACCOUNT: coreai_devtech_hugectr
    OLD_SLURM_ACCOUNT: "devtech"
    GPFSFOLDER: "/lustre/fsw/$OLD_SLURM_ACCOUNT"
    DGXNNODES: 1
    WALLTIME: "00:30:00"
    GIT_CLONE_PATH: /lustre/fsw/devtech/hpc-hugectr/hugectr-ci/$CI_CONCURRENT_ID/$CI_PROJECT_NAME
  allow_failure: false

.dracorno_sok_test_job:
  extends:
    - .dracorno_job
    - .sok:rules:test_in_child
  variables:
    CLUSTER:       "dracorno"
    SLURM_ACCOUNT: "coreai_devtech_hugectr"
    DATA_PREFIX: ${DRACO_DATA_PREFIX}
    SLURM_PARTITION: ${DRACO_SLURM_PARTITION}
    WALLTIME:      "02:00:00"
    SBATCH_OTHER_PARAMS: "--nv-meta ml-model.hugectr --gpus-per-node=8"
  tags:
    - $RUNNER_TAG
  allow_failure: false

.sok_test_job_daily:
  extends:
    - .selene_luna_job
    - .default:rules:daily-test
  allow_failure: false

.cluster_post_test_job:
  extends:
    - .cluster_test_job
    - .hugectr:rules:sanity-test
  stage: post_test

.selene_post_test_job:
  extends:
    - .selene_test_job
    - .hugectr:rules:test_in_child
  variables:
    SLURM_ACCOUNT: coreai_devtech_hugectr
    OLD_SLURM_ACCOUNT: "devtech"
    GPFSFOLDER: "/lustre/fsw/$OLD_SLURM_ACCOUNT"
    DGXNNODES: 1
    WALLTIME: "00:30:00"
    GIT_CLONE_PATH: /lustre/fsw/devtech/hpc-hugectr/hugectr-ci/$CI_CONCURRENT_ID/$CI_PROJECT_NAME
  stage: post_test

.dracorno_post_test_job:
  extends:
    - .dracorno_test_job
    - .hugectr:rules:test_in_child
  variables:
    WALLTIME: "00:30:00"
  stage: post_test

.draco_oci_post_test_job:
  extends:
    - .draco_oci_test_job
    - .hugectr:rules:test_in_child
  variables:
    WALLTIME: "00:30:00"
  stage: post_test

.cluster_post_test_job_daily:
  extends:
    - .draco_oci_test_job
    - .default:rules:daily-test
  variables:
    DRACO_OCI_LOGDIR: /lustre/fsw/portfolios/coreai/users/svcnvdlfw/hugectr_ci/${CI_PIPELINE_ID}
  stage: post_test

.sok_benchmark:
  extends:
    - .selene_luna_job
    - .benchmark:rules
  stage: sok_benchmark
  before_script:
    - export PARAM=$(echo ${CI_JOB_NAME} | awk -F-- '{print $2}')
    - export BZ=$(echo ${PARAM} | awk -Fx '{print $1}')
    - export GPU_NUM=$(echo ${PARAM} | awk -Fx '{print $2}')
    - export GPFSFOLDER=$LOGDIR/sok_benchmark_${BZ}x${GPU_NUM}
  variables:
    GPFSFOLDER: $LOGDIR/sok_benchmark
    CONT: ${UNIFIED_TF_LATEST}
    MOUNTS: /lustre/fsw/mlperf/mlperft-dlrm/datasets/terabyte_portion_csv/:/dataset,${CI_PROJECT_DIR}:/hugectr
    SLURM_ACCOUNT: coreai_devtech_hugectr
    OLD_SLURM_ACCOUNT: "devtech"
    GPFSFOLDER: "/lustre/fsw/$OLD_SLURM_ACCOUNT"
    WALLTIME: "00:45:00"
    DGXNNODES: 1
    TEST_CMD: ./ci/benchmark/sok/sok_dlrm.sub

.train_benchmark:
  extends:
    - .selene_luna_job
    - .benchmark:rules
  before_script:
    - export BENCHMARK=$(echo ${CI_JOB_NAME} | awk -F-- '{print $2}')
    - export PARAM=$(echo ${CI_JOB_NAME} | awk -F-- '{print $3}')
    - export NODE_NUM=$(echo ${PARAM} | awk -Fx '{print $1}')
    - export GPU_NUM=$(echo ${PARAM} | awk -Fx '{print $2}')
    - export BZ_PER_GPU=$(echo ${PARAM} | awk -Fx '{print $3}')
    - export MIXED_PRECISION=$(echo ${PARAM} | awk -Fx '{print $4}')
    - export DGXNNODES=${NODE_NUM}
    - export GPFSFOLDER=$LOGDIR/train_benchmark--${BENCHMARK}--${NODE_NUM}x${GPU_NUM}x${BZ_PER_GPU}x${MIXED_PRECISION}
  variables:
    GIT_CLONE_PATH: ${GIT_CLONE_PATH_SELENE}
    CONT: ${UNIFIED_CTR_LATEST}
    MOUNTS: ${DATASET_NEW_CRITEO_SELENE}:${NEW_CRITEO_MOUNT},${DATASET_CRITEO_SELENE}:${CRITEO_MOUNT},/raid:/raid,${CI_PROJECT_DIR}:/hugectr
    SLURM_ACCOUNT: coreai_devtech_hugectr
    OLD_SLURM_ACCOUNT: "devtech"
    GPFSFOLDER: "/lustre/fsw/$OLD_SLURM_ACCOUNT"
    WALLTIME: "00:15:00"
    TEST_CMD: ./ci/benchmark/train_benchmark/run.sub

collect_benchmark_result:
  extends:
    - .selene_luna_job
    - .benchmark:rules
  stage: post_test
  variables:
    GPFSFOLDER: $LOGDIR/collect_benchmark_result
    GIT_CLONE_PATH: ${GIT_CLONE_PATH_SELENE}
    CONT: ${UNIFIED_CTR_LATEST}
    MOUNTS: $LOGDIR:/logs,${CI_PROJECT_DIR}:/hugectr
    SLURM_ACCOUNT: coreai_devtech_hugectr
    OLD_SLURM_ACCOUNT: "devtech"
    GPFSFOLDER: "/lustre/fsw/$OLD_SLURM_ACCOUNT"
    WALLTIME: "00:15:00"
    TEST_CMD: ./ci/post_test/collect_benchmark.sub
