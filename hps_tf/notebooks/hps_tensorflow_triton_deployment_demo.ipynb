{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfec37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7adf4cf",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hps-hps-tensorflow-triton-deployment/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Deploy SavedModel using HPS with Triton TensorFlow Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac24a16",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to deploy the SavedModel that leverages HPS with [Triton TensorFlow backend](https://github.com/triton-inference-server/tensorflow_backend). It also shows how to apply [TF-TRT](https://github.com/tensorflow/tensorrt) optimization to SavedModel whose embedding lookup is based on HPS. It is recommended to run [hierarchical_parameter_server_demo.ipynb](hierarchical_parameter_server_demo.ipynb) before diving into this notebook.\n",
    "\n",
    "For more details about HPS APIs, please refer to [HPS APIs](https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/api/index.html). For more details about HPS, please refer to [HugeCTR Hierarchical Parameter Server (HPS)](https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435079b1",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Get HPS from NGC\n",
    "\n",
    "The HPS Python module is preinstalled in the 22.12 and later [Merlin TensorFlow Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow): `nvcr.io/nvidia/merlin/merlin-tensorflow:22.12`.\n",
    "\n",
    "You can check the existence of the required libraries by running the following Python code after launching this container.\n",
    "\n",
    "```bash\n",
    "$ python3 -c \"import hierarchical_parameter_server as hps\"\n",
    "```\n",
    "\n",
    "The Triton TensorFlow backend is also available in this container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a74924",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "First of all we specify the required configurations, e.g., the arguments needed for generating the dataset, the paths to save the model and the model parameters. We will use a deep neural network (DNN) model which has one embedding table and several dense layers in this notebook. Please note that there are two inputs here, one is the key tensor (one-hot) while the other is the dense feature tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3531ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] hierarchical_parameter_server is imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import hierarchical_parameter_server as hps\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import struct\n",
    "\n",
    "args = dict()\n",
    "\n",
    "args[\"gpu_num\"] = 1                               # the number of available GPUs\n",
    "args[\"iter_num\"] = 10                             # the number of training iteration\n",
    "args[\"slot_num\"] = 5                              # the number of feature fields in this embedding layer\n",
    "args[\"embed_vec_size\"] = 16                       # the dimension of embedding vectors\n",
    "args[\"global_batch_size\"] = 1024                  # the globally batchsize for all GPUs\n",
    "args[\"max_vocabulary_size\"] = 50000\n",
    "args[\"vocabulary_range_per_slot\"] = [[0,10000],[10000,20000],[20000,30000],[30000,40000],[40000,50000]]\n",
    "args[\"dense_dim\"] = 10\n",
    "\n",
    "args[\"dense_model_path\"] = \"hps_tf_triton_dense.model\"\n",
    "args[\"ps_config_file\"] = \"hps_tf_triton.json\"\n",
    "args[\"embedding_table_path\"] = \"hps_tf_triton_sparse_0.model\"\n",
    "args[\"saved_path\"] = \"hps_tf_triton_tf_saved_model\"\n",
    "args[\"np_key_type\"] = np.int64\n",
    "args[\"np_vector_type\"] = np.float32\n",
    "args[\"tf_key_type\"] = tf.int64\n",
    "args[\"tf_vector_type\"] = tf.float32\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(args[\"gpu_num\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58bf8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_samples(num_samples, vocabulary_range_per_slot, dense_dim, key_dtype = args[\"np_key_type\"]):\n",
    "    keys = list()\n",
    "    for vocab_range in vocabulary_range_per_slot:\n",
    "        keys_per_slot = np.random.randint(low=vocab_range[0], high=vocab_range[1], size=(num_samples, 1), dtype=key_dtype)\n",
    "        keys.append(keys_per_slot)\n",
    "    keys = np.concatenate(np.array(keys), axis = 1)\n",
    "    dense_features = np.random.random((num_samples, dense_dim)).astype(np.float32)\n",
    "    labels = np.random.randint(low=0, high=2, size=(num_samples, 1))\n",
    "    return keys, dense_features, labels\n",
    "\n",
    "def tf_dataset(keys, dense_features, labels, batchsize):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((keys, dense_features, labels))\n",
    "    dataset = dataset.batch(batchsize, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d974399",
   "metadata": {},
   "source": [
    "## Train with native TF layers\n",
    "\n",
    "We define the model graph for training with native TF layers, i.e., `tf.nn.embedding_lookup` and `tf.keras.layers.Dense`. Besides, the embedding weights are stored in `tf.Variable`. We can then train the model and extract the trained weights of the embedding table. As for the dense layers, they are saved as a separate model graph, which can be loaded directly during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19779cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel(tf.keras.models.Model):\n",
    "    def __init__(self,\n",
    "                 init_tensors,\n",
    "                 slot_num,\n",
    "                 embed_vec_size,\n",
    "                 dense_dim,\n",
    "                 **kwargs):\n",
    "        super(TrainModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.slot_num = slot_num\n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        self.dense_dim = dense_dim\n",
    "        self.init_tensors = init_tensors\n",
    "        self.params = tf.Variable(initial_value=tf.concat(self.init_tensors, axis=0))\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=1, name=\"concatenate\")\n",
    "        self.fc_1 = tf.keras.layers.Dense(units=256, activation=None,\n",
    "                                                 kernel_initializer=\"ones\",\n",
    "                                                 bias_initializer=\"zeros\",\n",
    "                                                 name='fc_1')\n",
    "        self.fc_2 = tf.keras.layers.Dense(units=1, activation=None,\n",
    "                                                 kernel_initializer=\"ones\",\n",
    "                                                 bias_initializer=\"zeros\",\n",
    "                                                 name='fc_2')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        keys, dense_features = inputs[0], inputs[1]\n",
    "        embedding_vector = tf.nn.embedding_lookup(params=self.params, ids=keys)\n",
    "        embedding_vector = tf.reshape(embedding_vector, shape=[-1, self.slot_num * self.embed_vec_size])\n",
    "        concated_features = self.concat([embedding_vector, dense_features])\n",
    "        logit = self.fc_2(self.fc_1(concated_features))\n",
    "        return logit\n",
    "\n",
    "    def summary(self):\n",
    "        inputs = [tf.keras.Input(shape=(self.slot_num, ), dtype=args[\"tf_key_type\"]),\n",
    "                  tf.keras.Input(shape=(self.dense_dim, ), dtype=tf.float32)]\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8231b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    init_tensors = np.ones(shape=[args[\"max_vocabulary_size\"], args[\"embed_vec_size\"]], dtype=args[\"np_vector_type\"])\n",
    "    \n",
    "    model = TrainModel(init_tensors, args[\"slot_num\"], args[\"embed_vec_size\"], args[\"dense_dim\"])\n",
    "    model.summary()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "    \n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    def _train_step(inputs, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logit = model(inputs)\n",
    "            loss = loss_fn(labels, logit)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return logit, loss\n",
    "\n",
    "    keys, dense_features, labels = generate_random_samples(args[\"global_batch_size\"]  * args[\"iter_num\"], args[\"vocabulary_range_per_slot\"], args[\"dense_dim\"])\n",
    "    dataset = tf_dataset(keys, dense_features, labels, args[\"global_batch_size\"])\n",
    "    for i, (keys, dense_features, labels) in enumerate(dataset):\n",
    "        inputs = [keys, dense_features]\n",
    "        _, loss = _train_step(inputs, labels)\n",
    "        print(\"-\"*20, \"Step {}, loss: {}\".format(i, loss),  \"-\"*20)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c6f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 01:36:13.919938: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 01:36:14.444040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30991 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.compat.v1.nn.embedding_lookup), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(50000, 16) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.nn.embedding_look  (None, 5, 16)       0           ['input_1[0][0]']                \n",
      " up (TFOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 80)           0           ['tf.compat.v1.nn.embedding_looku\n",
      "                                                                 p[0][0]']                        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 90)           0           ['tf.reshape[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " fc_1 (Dense)                   (None, 256)          23296       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " fc_2 (Dense)                   (None, 1)            257         ['fc_1[0][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,553\n",
      "Trainable params: 23,553\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "-------------------- Step 0, loss: 10934.333984375 --------------------\n",
      "-------------------- Step 1, loss: 9218.0703125 --------------------\n",
      "-------------------- Step 2, loss: 7060.255859375 --------------------\n",
      "-------------------- Step 3, loss: 5094.876953125 --------------------\n",
      "-------------------- Step 4, loss: 3605.475830078125 --------------------\n",
      "-------------------- Step 5, loss: 2593.270751953125 --------------------\n",
      "-------------------- Step 6, loss: 1741.0677490234375 --------------------\n",
      "-------------------- Step 7, loss: 1045.5091552734375 --------------------\n",
      "-------------------- Step 8, loss: 541.4227905273438 --------------------\n",
      "-------------------- Step 9, loss: 242.8596649169922 --------------------\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 80)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 90)           0           ['input_3[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " fc_1 (Dense)                   (None, 256)          23296       ['concatenate[1][0]']            \n",
      "                                                                                                  \n",
      " fc_2 (Dense)                   (None, 1)            257         ['fc_1[1][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,553\n",
      "Trainable params: 23,553\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_1 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: hps_tf_triton_dense.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: hps_tf_triton_dense.model/assets\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(args)\n",
    "weights_list = trained_model.get_weights()\n",
    "embedding_weights = weights_list[-1]\n",
    "dense_model = tf.keras.Model(trained_model.get_layer(\"concatenate\").input,\n",
    "                             trained_model.get_layer(\"fc_2\").output)\n",
    "dense_model.summary()\n",
    "dense_model.save(args[\"dense_model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aaa551",
   "metadata": {},
   "source": [
    "## Create the inference graph with HPS LookupLayer\n",
    "In order to use HPS in the inference stage, we need to create a inference model graph which is almost the same as the train graph except that `tf.nn.embedding_lookup` is replaced by `hps.LookupLayer`. The trained dense model graph can be loaded directly, while the embedding weights should be converted to the formats required by HPS. \n",
    "\n",
    "We can then save the inference model graph, which will be ready to be loaded for inference deployment. Please note that the inference SavedModel that leverages HPS will be deployed with the Triton TensorFlow backend, thus implicit initialization of HPS should be enabled by specifying `ps_config_file` and `global_batch_size` in the constructor of  `hps.LookupLayer`. For more information, please refer to [HPS Initialize](https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/api/initialize.html).\n",
    "\n",
    "To this end, we need to create a JSON configuration file and specify the details of the embedding tables for the models to be deployed. We only show how to deploy a model that has one embedding table here, and it can support multiple models with multiple embedding tables actually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5bff930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hps_tf_triton.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile hps_tf_triton.json\n",
    "{\n",
    "    \"supportlonglong\": true,\n",
    "    \"models\": [{\n",
    "        \"model\": \"hps_tf_triton\",\n",
    "        \"sparse_files\": [\"/hugectr/hps_tf/notebooks/model_repo/hps_tf_triton/hps_tf_triton_sparse_0.model\"],\n",
    "        \"num_of_worker_buffer_in_pool\": 3,\n",
    "        \"embedding_table_names\":[\"sparse_embedding1\"],\n",
    "        \"embedding_vecsize_per_table\": [16],\n",
    "        \"maxnum_catfeature_query_per_table_per_sample\": [5],\n",
    "        \"default_value_for_each_table\": [1.0],\n",
    "        \"deployed_device_list\": [0],\n",
    "        \"max_batch_size\": 1024,\n",
    "        \"cache_refresh_percentage_per_iteration\": 0.2,\n",
    "        \"hit_rate_threshold\": 1.0,\n",
    "        \"gpucacheper\": 1.0,\n",
    "        \"gpucache\": true\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb527dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_model_repo = \"/hugectr/hps_tf/notebooks/model_repo/hps_tf_triton/\"\n",
    "\n",
    "class InferenceModel(tf.keras.models.Model):\n",
    "    def __init__(self,\n",
    "                 slot_num,\n",
    "                 embed_vec_size,\n",
    "                 dense_dim,\n",
    "                 dense_model_path,\n",
    "                 **kwargs):\n",
    "        super(InferenceModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.slot_num = slot_num\n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        self.dense_dim = dense_dim\n",
    "        self.lookup_layer = hps.LookupLayer(model_name = \"hps_tf_triton\", \n",
    "                                            table_id = 0,\n",
    "                                            emb_vec_size = self.embed_vec_size,\n",
    "                                            emb_vec_dtype = args[\"tf_vector_type\"],\n",
    "                                            ps_config_file = triton_model_repo + args[\"ps_config_file\"],\n",
    "                                            global_batch_size = args[\"global_batch_size\"],\n",
    "                                            name = \"lookup\")\n",
    "        self.dense_model = tf.keras.models.load_model(dense_model_path)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        keys, dense_features = inputs[0], inputs[1]\n",
    "        embedding_vector = self.lookup_layer(keys)\n",
    "        embedding_vector = tf.reshape(embedding_vector, shape=[-1, self.slot_num * self.embed_vec_size])\n",
    "        logit = self.dense_model([embedding_vector, dense_features])\n",
    "        return logit\n",
    "\n",
    "    def summary(self):\n",
    "        inputs = [tf.keras.Input(shape=(self.slot_num, ), dtype=args[\"tf_key_type\"]),\n",
    "                  tf.keras.Input(shape=(self.dense_dim, ), dtype=tf.float32)]\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde93765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_inference_graph(args): \n",
    "    model = InferenceModel(args[\"slot_num\"], args[\"embed_vec_size\"], args[\"dense_dim\"], args[\"dense_model_path\"])\n",
    "    model.summary()\n",
    "    _ = model([tf.keras.Input(shape=(args[\"slot_num\"], ), dtype=args[\"tf_key_type\"]),\n",
    "               tf.keras.Input(shape=(args[\"dense_dim\"], ), dtype=tf.float32)])\n",
    "    model.save(args[\"saved_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95633b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sparse_model(embeddings_weights, embedding_table_path, embedding_vec_size):\n",
    "    os.system(\"mkdir -p {}\".format(embedding_table_path))\n",
    "    with open(\"{}/key\".format(embedding_table_path), 'wb') as key_file, \\\n",
    "        open(\"{}/emb_vector\".format(embedding_table_path), 'wb') as vec_file:\n",
    "      for key in range(embeddings_weights.shape[0]):\n",
    "        vec = embeddings_weights[key]\n",
    "        key_struct = struct.pack('q', key)\n",
    "        vec_struct = struct.pack(str(embedding_vec_size) + \"f\", *vec)\n",
    "        key_file.write(key_struct)\n",
    "        vec_file.write(vec_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bcbf9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " lookup (LookupLayer)           (None, 5, 16)        0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.reshape_1 (TFOpLambda)      (None, 80)           0           ['lookup[0][0]']                 \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 1)            23553       ['tf.reshape_1[0][0]',           \n",
      "                                                                  'input_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,553\n",
      "Trainable params: 23,553\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: hps_tf_triton_tf_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: hps_tf_triton_tf_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "convert_to_sparse_model(embedding_weights, args[\"embedding_table_path\"], args[\"embed_vec_size\"])\n",
    "create_and_save_inference_graph(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ed354",
   "metadata": {},
   "source": [
    "## Deploy SavedModel using HPS with Triton TensorFlow Backend\n",
    "\n",
    "In order to deploy the inference SavedModel with the Triton TensorFlow backend, we need to create the model repository and define the `config.pbtxt` first. Please note that some required portions (i.e., the input and output tensors) of the model configuration are generated automatically by Triton (see [Auto-Generated Model Configuration](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#auto-generated-model-configuration)), so you do NOT need to specify them explicitly in `config.pbtxt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4823c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repo/hps_tf_triton/1\n",
    "!mv hps_tf_triton_tf_saved_model model_repo/hps_tf_triton/1/model.savedmodel\n",
    "!mv hps_tf_triton_sparse_0.model model_repo/hps_tf_triton\n",
    "!mv hps_tf_triton.json model_repo/hps_tf_triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd41ec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_repo/hps_tf_triton/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_repo/hps_tf_triton/config.pbtxt\n",
    "name: \"hps_tf_triton\"\n",
    "platform: \"tensorflow_savedmodel\"\n",
    "max_batch_size:1024\n",
    "input [\n",
    "  {\n",
    "    name: \"input_6\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [5]\n",
    "  },\n",
    "  {\n",
    "    name: \"input_7\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [10]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1]\n",
    "  }\n",
    "]\n",
    "version_policy: {\n",
    "        specific:{versions: 1}\n",
    "},\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus: [0]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "945efc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmodel_repo/hps_tf_triton\u001b[00m\n",
      "├── \u001b[01;34m1\u001b[00m\n",
      "│   └── \u001b[01;34mmodel.savedmodel\u001b[00m\n",
      "│       ├── \u001b[01;34massets\u001b[00m\n",
      "│       ├── keras_metadata.pb\n",
      "│       ├── saved_model.pb\n",
      "│       └── \u001b[01;34mvariables\u001b[00m\n",
      "│           ├── variables.data-00000-of-00001\n",
      "│           └── variables.index\n",
      "├── config.pbtxt\n",
      "├── hps_tf_triton.json\n",
      "└── \u001b[01;34mhps_tf_triton_sparse_0.model\u001b[00m\n",
      "    ├── emb_vector\n",
      "    └── key\n",
      "\n",
      "5 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree model_repo/hps_tf_triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb10e4",
   "metadata": {},
   "source": [
    "We can then launch the Triton inference server using the TensorFlow backend. Please note that `LD_PRELOAD` is utilized to load the custom TensorFlow operations (i.e., HPS related operations) into Triton. For more information, please refer to [TensorFlow Custom Operations in Triton](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/custom_operations.md#tensorflow).\n",
    "\n",
    "Note: `Since Background processes not supported by Jupyter, please launch the Triton Server according to the following command independently in the background`.\n",
    "\n",
    "> **LD_PRELOAD=/usr/local/lib/python3.8/dist-packages/merlin_hps-1.0.0-py3.8-linux-x86_64.egg/hierarchical_parameter_server/lib/libhierarchical_parameter_server.so tritonserver --model-repository=/hugectr/hps_tf/notebooks/model_repo --backend-config=tensorflow,version=2 --load-model=hps_tf_triton --model-control-mode=explicit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cb59e",
   "metadata": {},
   "source": [
    "We can then send the requests to the Triton inference server using the HTTP client. Please note that HPS will be initialized implicitly when the first request is processed at the server side, and the latency can be higher than that of later requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b5cc4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"hps_tf_triton\",\"platform\":\"tensorflow_savedmodel\",\"backend\":\"tensorflow\",\"version_policy\":{\"specific\":{\"versions\":[1]}},\"max_batch_size\":1024,\"input\":[{\"name\":\"input_6\",\"data_type\":\"TYPE_INT64\",\"format\":\"FORMAT_NONE\",\"dims\":[5],\"is_shape_tensor\":false,\"allow_ragged_batch\":false,\"optional\":false},{\"name\":\"input_7\",\"data_type\":\"TYPE_FP32\",\"format\":\"FORMAT_NONE\",\"dims\":[10],\"is_shape_tensor\":false,\"allow_ragged_batch\":false,\"optional\":false}],\"output\":[{\"name\":\"output_1\",\"data_type\":\"TYPE_FP32\",\"dims\":[1],\"label_filename\":\"\",\"is_shape_tensor\":false}],\"batch_input\":[],\"batch_output\":[],\"optimization\":{\"priority\":\"PRIORITY_DEFAULT\",\"input_pinned_memory\":{\"enable\":true},\"output_pinned_memory\":{\"enable\":true},\"gather_kernel_buffer_threshold\":0,\"eager_batching\":false},\"dynamic_batching\":{\"preferred_batch_size\":[1024],\"max_queue_delay_microseconds\":0,\"preserve_ordering\":false,\"priority_levels\":0,\"default_priority_level\":0,\"priority_queue_policy\":{}},\"instance_group\":[{\"name\":\"hps_tf_triton_0\",\"kind\":\"KIND_GPU\",\"count\":1,\"gpus\":[0],\"secondary_devices\":[],\"profile\":[],\"passive\":false,\"host_policy\":\"\"}],\"default_model_filename\":\"model.savedmodel\",\"cc_model_filenames\":{},\"metric_tags\":{},\"parameters\":{},\"model_warmup\":[]}"
     ]
    }
   ],
   "source": [
    "!curl localhost:8000/v2/models/hps_tf_triton/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e482a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_gpu = 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(num_gpu)))\n",
    "\n",
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import *\n",
    "\n",
    "\n",
    "def send_inference_requests(num_requests, num_samples):\n",
    "    triton_client = httpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    triton_client.is_server_live()\n",
    "    triton_client.get_model_repository_index()\n",
    "\n",
    "    for i in range(num_requests):\n",
    "        print(\"--------------------------Request {}--------------------------\".format(i))\n",
    "        key_tensor, dense_tensor, _ = generate_random_samples(num_samples, args[\"vocabulary_range_per_slot\"], args[\"dense_dim\"])\n",
    "\n",
    "        inputs = [\n",
    "            httpclient.InferInput(\"input_6\", \n",
    "                                  key_tensor.shape,\n",
    "                                  np_to_triton_dtype(np.int64)),\n",
    "            httpclient.InferInput(\"input_7\", \n",
    "                                  dense_tensor.shape,\n",
    "                                  np_to_triton_dtype(np.float32)),\n",
    "        ]\n",
    "\n",
    "        inputs[0].set_data_from_numpy(key_tensor)\n",
    "        inputs[1].set_data_from_numpy(dense_tensor)\n",
    "        outputs = [\n",
    "            httpclient.InferRequestedOutput(\"output_1\")\n",
    "        ]\n",
    "\n",
    "        # print(\"Input key tensor is \\n{}\".format(key_tensor))\n",
    "        # print(\"Input dense tensor is \\n{}\".format(dense_tensor))\n",
    "        model_name = \"hps_tf_triton\"\n",
    "        with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "            response = client.infer(model_name,\n",
    "                                    inputs,\n",
    "                                    outputs=outputs)\n",
    "            result = response.get_response()\n",
    "\n",
    "            print(\"Response details:\\n{}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60b6e299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '56'}>\n",
      "bytearray(b'[{\"name\":\"hps_tf_triton\",\"version\":\"1\",\"state\":\"READY\"}]')\n",
      "--------------------------Request 0--------------------------\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '1', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [128, 1], 'parameters': {'binary_data_size': 512}}]}\n",
      "--------------------------Request 1--------------------------\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '1', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [128, 1], 'parameters': {'binary_data_size': 512}}]}\n",
      "--------------------------Request 2--------------------------\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '1', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [128, 1], 'parameters': {'binary_data_size': 512}}]}\n",
      "--------------------------Request 3--------------------------\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '1', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [128, 1], 'parameters': {'binary_data_size': 512}}]}\n",
      "--------------------------Request 4--------------------------\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '1', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [128, 1], 'parameters': {'binary_data_size': 512}}]}\n"
     ]
    }
   ],
   "source": [
    "send_inference_requests(num_requests = 5, num_samples = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c064c266",
   "metadata": {},
   "source": [
    "## Deploy TF-TRT SavedModel using HPS with Triton TensorFlow Backend\n",
    "We can leverage TF-TRT to optimize the above inference TF SavedModel. The `hps.LookupLayer` will fall back to the TF ops while the TensorRT engine will be built to execute the dense network. The optimized TF-TRT SavedModel can still be deployed with Triton TensorFlow backend.\n",
    "\n",
    "The TF-TRT SavedModel is be placed in the folder `\"model_repo/hps_tf_triton/2/\"` and the `config.pbtxt` file is updated correspondingly to load the version 2 of the inference model, i.e., the TF-TRT optimized one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9000bbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Linked TensorRT version: (8, 4, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Linked TensorRT version: (8, 4, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loaded TensorRT version: (8, 4, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loaded TensorRT version: (8, 4, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing prior device assignments in loaded saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 01:37:22.924379: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2022-11-23 01:37:22.924537: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-11-23 01:37:22.928272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30991 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "INFO:tensorflow:Clearing prior device assignments in loaded saved model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Automatic mixed precision has been deactivated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Automatic mixed precision has been deactivated.\n",
      "2022-11-23 01:37:23.028482: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2022-11-23 01:37:23.028568: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-11-23 01:37:23.031909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30991 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2022-11-23 01:37:23.048593: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:198] Calibration with FP32 or FP16 is not implemented. Falling back to use_calibration = False.Note that the default value of use_calibration is True.\n",
      "2022-11-23 01:37:23.049761: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:952] \n",
      "\n",
      "################################################################################\n",
      "TensorRT unsupported/non-converted OP Report:\n",
      "\t- NoOp -> 2x\n",
      "\t- Placeholder -> 2x\n",
      "\t- Identity -> 1x\n",
      "\t- Init -> 1x\n",
      "\t- Lookup -> 1x\n",
      "\t- Reshape -> 1x\n",
      "--------------------------------------------------------------------------------\n",
      "\t- Total nonconverted OPs: 8\n",
      "\t- Total nonconverted OP Types: 6\n",
      "For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops.\n",
      "################################################################################\n",
      "\n",
      "2022-11-23 01:37:23.049860: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:1280] The environment variable TF_TRT_MAX_ALLOWED_ENGINES=20 has no effect since there are only 1 TRT Engines with  at least minimum_segment_size=3 nodes.\n",
      "2022-11-23 01:37:23.049893: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:799] Number of TensorRT candidate segments: 1\n",
      "2022-11-23 01:37:23.050667: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:916] Replaced segment 0 consisting of 9 nodes by TRTEngineOp_000_000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRTEngineOP Name                 Device        # Nodes # Inputs      # Outputs     Input DTypes       Output Dtypes      Input Shapes       Output Shapes     \n",
      "================================================================================================================================================================\n",
      "TRTEngineOp_000_000              device:GPU:0  10      2             1             ['float32', 'f ... ['float32']        [[-1, 80], [-1 ... [[-1, 1]]         \n",
      "\n",
      "\t- BiasAdd: 2x\n",
      "\t- ConcatV2: 1x\n",
      "\t- Const: 5x\n",
      "\t- MatMul: 2x\n",
      "\n",
      "================================================================================================================================================================\n",
      "[*] Total number of TensorRT engines: 1\n",
      "[*] % of OPs Converted: 50.00% [10/20]\n",
      "\n",
      "=====================================================HPS Parse====================================================\n",
      "[HCTR][01:37:23.329][INFO][RK0][main]: dense_file is not specified using default: \n",
      "[HCTR][01:37:23.329][INFO][RK0][main]: num_of_refresher_buffer_in_pool is not specified using default: 1\n",
      "[HCTR][01:37:23.329][INFO][RK0][main]: maxnum_des_feature_per_sample is not specified using default: 26\n",
      "[HCTR][01:37:23.329][INFO][RK0][main]: refresh_delay is not specified using default: 0\n",
      "[HCTR][01:37:23.329][INFO][RK0][main]: refresh_interval is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][01:37:23.329][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][01:37:23.329][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][01:37:23.329][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][01:37:23.329][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][01:37:23.329][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][01:37:23.745][INFO][RK0][main]: Table: hps_et.hps_tf_triton.sparse_embedding1; cached 50000 / 50000 embeddings in volatile database (HashMapBackend); load: 50000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][01:37:23.745][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][01:37:23.745][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][01:37:23.753][INFO][RK0][main]: Model name: hps_tf_triton\n",
      "[HCTR][01:37:23.753][INFO][RK0][main]: Number of embedding tables: 1\n",
      "[HCTR][01:37:23.753][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 1.000000\n",
      "[HCTR][01:37:23.753][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][01:37:23.753][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000\n",
      "[HCTR][01:37:23.753][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][01:37:23.753][INFO][RK0][main]: The size of worker memory pool: 3\n",
      "[HCTR][01:37:23.753][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][01:37:23.753][INFO][RK0][main]: The refresh percentage : 0.200000\n",
      "[HCTR][01:37:23.778][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][01:37:23.814][INFO][RK0][main]: EC initialization for model: \"hps_tf_triton\", num_tables: 1\n",
      "[HCTR][01:37:23.814][INFO][RK0][main]: EC initialization on device: 0\n",
      "[HCTR][01:37:23.815][INFO][RK0][main]: Creating lookup session for hps_tf_triton on device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 01:37:23.818078: I tensorflow/compiler/tf2tensorrt/common/utils.cc:104] Linked TensorRT version: 8.4.2\n",
      "2022-11-23 01:37:23.818150: I tensorflow/compiler/tf2tensorrt/common/utils.cc:106] Loaded TensorRT version: 8.4.2\n",
      "2022-11-23 01:37:28.749149: I tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:1275] [TF-TRT] Sparse compute capability is enabled.\n",
      "2022-11-23 01:37:28.814132: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:86] DefaultLogger 1: [wrapper.cpp::CublasWrapper::85] Error Code 1: Cublas (Could not initialize cublas. Please check CUDA installation.)\n",
      "2022-11-23 01:37:28.817575: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1061] TF-TRT Warning: Engine creation for TRTEngineOp_000_000 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine\n",
      "2022-11-23 01:37:28.817694: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:894] TF-TRT Warning: Engine retrieval for input shapes: [[1024,80], [1024,10]] failed. Running native segment for TRTEngineOp_000_000\n",
      "2022-11-23 01:37:28.823806: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:894] TF-TRT Warning: Engine retrieval for input shapes: [[1024,80], [1024,10]] failed. Running native segment for TRTEngineOp_000_000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_repo/hps_tf_triton/2/model.savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_repo/hps_tf_triton/2/model.savedmodel/assets\n"
     ]
    }
   ],
   "source": [
    "# Build TF-TRT SavedModel\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "\n",
    "ORIGINAL_MODEL_PATH = \"model_repo/hps_tf_triton/1/model.savedmodel\"\n",
    "NEW_MODEL_PATH = \"model_repo/hps_tf_triton/2/model.savedmodel\"\n",
    "\n",
    "# Instantiate the TF-TRT converter\n",
    "converter = trt.TrtGraphConverterV2(\n",
    "   input_saved_model_dir=ORIGINAL_MODEL_PATH,\n",
    "   precision_mode=trt.TrtPrecisionMode.FP32\n",
    ")\n",
    "\n",
    "# Convert the model into TRT compatible segments\n",
    "trt_func = converter.convert()\n",
    "converter.summary()\n",
    "\n",
    "keys, dense_features, _ = generate_random_samples(args[\"global_batch_size\"], args[\"vocabulary_range_per_slot\"], args[\"dense_dim\"])\n",
    "keys  = tf.convert_to_tensor(keys)\n",
    "dense_features = tf.convert_to_tensor(dense_features)\n",
    "def input_fn():\n",
    "   yield [keys, dense_features]\n",
    "\n",
    "converter.build(input_fn=input_fn)\n",
    "converter.save(output_saved_model_dir=NEW_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca60fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_repo/hps_tf_triton/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_repo/hps_tf_triton/config.pbtxt\n",
    "name: \"hps_tf_triton\"\n",
    "platform: \"tensorflow_savedmodel\"\n",
    "max_batch_size:1024\n",
    "input [\n",
    "  {\n",
    "    name: \"input_6\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [5]\n",
    "  },\n",
    "  {\n",
    "    name: \"input_7\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [10]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1]\n",
    "  }\n",
    "]\n",
    "version_policy: {\n",
    "        specific:{versions: 2}\n",
    "},\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus: [0]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13db2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmodel_repo/hps_tf_triton\u001b[00m\r\n",
      "├── \u001b[01;34m1\u001b[00m\r\n",
      "│   └── \u001b[01;34mmodel.savedmodel\u001b[00m\r\n",
      "│       ├── \u001b[01;34massets\u001b[00m\r\n",
      "│       ├── keras_metadata.pb\r\n",
      "│       ├── saved_model.pb\r\n",
      "│       └── \u001b[01;34mvariables\u001b[00m\r\n",
      "│           ├── variables.data-00000-of-00001\r\n",
      "│           └── variables.index\r\n",
      "├── \u001b[01;34m2\u001b[00m\r\n",
      "│   └── \u001b[01;34mmodel.savedmodel\u001b[00m\r\n",
      "│       ├── \u001b[01;34massets\u001b[00m\r\n",
      "│       │   └── trt-serialized-engine.TRTEngineOp_000_000\r\n",
      "│       ├── saved_model.pb\r\n",
      "│       └── \u001b[01;34mvariables\u001b[00m\r\n",
      "│           ├── variables.data-00000-of-00001\r\n",
      "│           └── variables.index\r\n",
      "├── config.pbtxt\r\n",
      "├── hps_tf_triton.json\r\n",
      "└── \u001b[01;34mhps_tf_triton_sparse_0.model\u001b[00m\r\n",
      "    ├── emb_vector\r\n",
      "    └── key\r\n",
      "\r\n",
      "9 directories, 12 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree model_repo/hps_tf_triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6a6c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release the occupied GPU memory by TensorFlow and Keras\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086abead",
   "metadata": {},
   "source": [
    "We can then launch the Triton inference server using the TensorFlow backend using the same command in the background. Please remember to kill the previous `tritonserver` process completely before launching it again. Otherwise, there could be out of memory errors.\n",
    "\n",
    "When the triton server is succesfully launched, we can then send the requests to it using the HTTP client again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "762f68de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"hps_tf_triton\",\"platform\":\"tensorflow_savedmodel\",\"backend\":\"tensorflow\",\"version_policy\":{\"specific\":{\"versions\":[2]}},\"max_batch_size\":1024,\"input\":[{\"name\":\"input_6\",\"data_type\":\"TYPE_INT64\",\"format\":\"FORMAT_NONE\",\"dims\":[5],\"is_shape_tensor\":false,\"allow_ragged_batch\":false,\"optional\":false},{\"name\":\"input_7\",\"data_type\":\"TYPE_FP32\",\"format\":\"FORMAT_NONE\",\"dims\":[10],\"is_shape_tensor\":false,\"allow_ragged_batch\":false,\"optional\":false}],\"output\":[{\"name\":\"output_1\",\"data_type\":\"TYPE_FP32\",\"dims\":[1],\"label_filename\":\"\",\"is_shape_tensor\":false}],\"batch_input\":[],\"batch_output\":[],\"optimization\":{\"priority\":\"PRIORITY_DEFAULT\",\"input_pinned_memory\":{\"enable\":true},\"output_pinned_memory\":{\"enable\":true},\"gather_kernel_buffer_threshold\":0,\"eager_batching\":false},\"dynamic_batching\":{\"preferred_batch_size\":[1024],\"max_queue_delay_microseconds\":0,\"preserve_ordering\":false,\"priority_levels\":0,\"default_priority_level\":0,\"priority_queue_policy\":{}},\"instance_group\":[{\"name\":\"hps_tf_triton_0\",\"kind\":\"KIND_GPU\",\"count\":1,\"gpus\":[0],\"secondary_devices\":[],\"profile\":[],\"passive\":false,\"host_policy\":\"\"}],\"default_model_filename\":\"model.savedmodel\",\"cc_model_filenames\":{},\"metric_tags\":{},\"parameters\":{},\"model_warmup\":[]}"
     ]
    }
   ],
   "source": [
    "!curl localhost:8000/v2/models/hps_tf_triton/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e31311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '56'}>\n",
      "bytearray(b'[{\"name\":\"hps_tf_triton\",\"version\":\"2\",\"state\":\"READY\"}]')\n",
      "--------------------------Request 0--------------------------\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '2', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [128, 1], 'parameters': {'binary_data_size': 512}}]}\n",
      "--------------------------Request 1--------------------------\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '2', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [128, 1], 'parameters': {'binary_data_size': 512}}]}\n",
      "--------------------------Request 2--------------------------\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '2', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [128, 1], 'parameters': {'binary_data_size': 512}}]}\n",
      "--------------------------Request 3--------------------------\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '2', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [128, 1], 'parameters': {'binary_data_size': 512}}]}\n",
      "--------------------------Request 4--------------------------\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '2', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [128, 1], 'parameters': {'binary_data_size': 512}}]}\n"
     ]
    }
   ],
   "source": [
    "send_inference_requests(num_requests = 5, num_samples = 128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
