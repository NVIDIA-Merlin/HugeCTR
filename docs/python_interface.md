# About HugeCTR Python Interface
 
HugeCTR Python Interface includes low-level training API and inference API. Currently, both APIs rely on the configuration file in JSON format. Please refer to [Configuration File Setup](./configuration_file_setup.md) if you want to get detailed information on how to configure the JSON file. Besides, we have high-level training API which does not require JSON configuration file. The high-level training API is friendly to users who are already familiar with other deep learning frameworks like Keras and it is worthwhile to switch to it from low-level training API.

## Table of Contents
* [High-level Training API](#high-level-training-api)
* [Low-level Training API](#low-level-training-api)
* [Inference API](#inference-api)
* [Sample Code](#sample-code)

## High-level Training API ##
For HugeCTR high-level training API, the core data structures are `SolverParser`, `Optimizer`, `Input`, `SparseEmbedding`, `DenseLayer` and `Model`. You can create a `Model` instance with `SolverParser` and `Optimizer` instances, and then add an instance of `Input`, `SparseEmbedding` or `DenseLayer` to it. After compiling the model with the `Model.compile()` method, you can start the epoch mode or non-epoch mode training by simply calling the `Model.fit()` method. Moreover, the `Model.summary()` method provides you with an overview of the model structure.

### SolverParser ###
**solver_parser_helper method**
```bash
hugectr.solver_parser_helper()
```
`solver_parser_helper` returns an `SolverParser` object according to the custom argument values，which specify the training resource and task items.

**Arguments**
* `seed`: A random seed to be specified. The default value is 0.

* `max_eval_batches`: Maximum number of batches used in evaluation. It is recommended that the number is equal to or bigger than the actual number of bathces in the evaluation dataset. The default value is 100.

* `batchsize_eval`: Minibatch size used in evaluation. The default value is 2048.

* `batchsize`: Minibatch size used in training. The default value is 2048.

* `model_file`: Trained dense model file to be loaded. If you train a model from scratch, it is not necessary.

* `dense_opt_states_file`: Dense otimizer states file to be loaded. If your optimizer doesn't have any dynamic states to be saved, an empty file is created. Make sure that you use the same type of optimizer used to generate this file. If you train a model from scratch or you don't want to use it, it is unnecessary.

* `embedding_files`: A trained embeding table (or sparse model) or their list to be loaded. If you train a model from scratch, it is not necessary.

* `sparse_opt_states_file`: Sparse otimizer states file(s) to be loaded. The behavior is as the same as `dense_opt_states_file`.

* `vvgpu`: GPU indices used in the training process, which has two levels. For example: [[0,1],[1,2]] indicates that two nodes are used. In the first node, GPUs 0 and 1 are used while GPUs 1 and 2 are used for the second node. It is also possible to specify non-continuous GPU indices such as [0, 2, 4, 7]. The default value is [[0]].

* `use_mixed_precision`: Whether to enable mixed precision training. The default value is `False`.

* `enable_tf32_compute`: If you want to accelerate FP32 matrix multiplications within the FullyConnectedLayer and InteractionLayer, set this value to `True`. The default value is `False`.

* `scaler`: The scaler to be used when mixed precision training is enabled. Only 128, 256, 512, and 1024 scalers are supported for mixed precision training. The default value is 1.0, which corresponds to no mixed precision training.

* `i64_input_key`: If your dataset format is `Norm`, you can choose the data type of each input key. For the `Parquet` format dataset generated by NVTabular, only I64 is allowed. For the `Raw` dataset format, only I32 is allowed. Set this value to `True` when you need to use I64 input key. The default value is `False`.

* `use_algorithm_search`: Whether to use algorithm search for cublasGemmEx within the FullyConnectedLayer. The default value is `True`.

* `use_cuda_graph`: Whether to enable cuda graph for dense network forward and backward propagation. The default value is `True`.

* `repeat_dataset`: Whether to repeat the dataset for training. If the value is `True`, non-epoch mode training will be employed. Otherwise, epoch mode training will be adopted. The default value is `True`.

* `max_iter`: Integer, the maximum iteration of non-epoch mode training. It will be ignored if `repeat_dataset` is `False`. The default value is 0.

* `num_epochs`: Integer, the number of epochs for epoch mode training. It will be ignored if `repeat_dataset` is `True`. The default value is 0.

* `display`: Integer, the interval of iterations at which the training loss will be displayed. The default value is 200.

* `snapshot`: Integer, the interval of iterations at which the snapshot model will be saved to files. The default value is 10000.

* `eval_interval`: Integer, the interval of iterations at which the evaluation will be executed. The default value is 1000.

* `use_model_oversubscriber`: Boolean, whether to use the feature of [Model Oversubscription](./hugectr_user_guide.md#model-oversubscription).

* `temp_embedding_dir`: String，where to store the temporary embedding table files. The path needs to have write permission to support the features of ModelOversubscriber. The default value is `''`.

### Optimizer ###
**CreateOptimizer method**
```bash
hugectr.optimizer.CreateOptimizer()
```
`CreateOptimizer` returns an `OptParamsBase` object according to the custom argument values，which specify the optimizer type and the corresponding hyperparameters.

**Arguments**
* `optimizer_type`: The optimizer type to be used. The supported types include `hugectr.Optimizer_t.Adam`, `hugectr.Optimizer_t.MomentumSGD`, `hugectr.Optimizer_t.Nesterov` and `hugectr.Optimizer_t.SGD`. The default value is `hugectr.Optimizer_t.Adam`.

* `update_type`: The update type for the embedding. The supported types include `hugectr.Update_t.Global`, `hugectr.Update_t.LazyGlobal` and `hugectr.Update_t.Local`.  Please refer to [Optimizer](./configuration_file_setup.md#optimizer) if you want to get detailed information about the embedding update types. The default value is `hugectr.Update_t.Global`.

* `learning_rate`: The initial learning rate for training. The default value is 0.001.

* `warmup_steps`: The warmup steps for `LearningRateScheduler` when using SGD optimizer. The default value is 1.

* `decay_start`: The step at which the learning rate decay starts for `LearningRateScheduler` when using SGD optimizer. The default value is 0.

* `decay_steps`: The number of steps of the learning rate decay for `LearningRateScheduler` when using SGD optimizer. The default value is 1.

* `decay_power`: The power of the learning rate decay for `LearningRateScheduler` when using SGD optimizer. The default value is 2.

* `end_lr`: The final learning rate for `LearningRateScheduler` when using SGD optimizer. The default value is 0. Please refer to [SGD Optimizer and Learning Rate Scheduling](./hugectr_user_guide.md#sgd-optimizer-and-learning-rate-scheduling) if you want to get detailed information about LearningRateScheduler.

* `beta1`: The `beta1` value when using Adam optimizer. The default value is 0.9.

* `beta2`: The `beta2` value when using Adam optimizer. The default value is 0.999.

* `epsilon`: The `epsilon` value when using Adam optimizer. The default value is 1e-7.

* `momentum_factor`: The `momentum_factor` value when using MomentumSGD or Nesterov optimizer. The default value is 0.

* `atomic_update`: Whether to employ atomic update when using SGD optimizer. The default value is True. 

* `use_mixed_precision`: Whether to use mixed precision for the optimizer. It should be consistent with that within `solver_parser_helper`. Only in this way can the instances of `SolverParser` and `OptParamsBase` be used to create a `Model` instance. There is NO default value and it should be specified by users.

### Input ###
**Input class**
```bash
hugectr.Input()
```
`Input` specifies the parameters related to the data input. HugeCTR currently supports three dataset formats, i.e., `Norm`, `Raw` and `Parquet`. An `Input` instance should be added to the Model instance first so that the following `SparseEmbedding` and `DenseLayer` instances can access the inputs with their specified names. Please refer to [Data Layers](./configuration_file_setup.md#data-layers) if you want to get detailed information about Input.

**Arguments**
* `data_reader_type`: The type of the data reader which should be consistent with the dataset format. The supported types include `hugectr.DataReaderType_t.Norm`, `hugectr.DataReaderType_t.Raw` and `hugectr.DataReaderType_t.Parquet`. There is NO default value and it should be specified by users.

* `source`: String, the training dataset source. For Norm or Parquet dataset, it should be the file list of training data. For Raw dataset, it should be a single training file. There is NO default value and it should be specified by users.

* `eval_source`: String, the evaluation dataset source. For Norm or Parquet dataset, it should be the file list of evaluation data. For Raw dataset, it should be a single evaluation file. There is NO default value and it should be specified by users.

* `check_type`: The data error detection mechanism. The supported types include `hugectr.Check_t.Sum` (CheckSum) and `hugectr.Check_t.Non` (no detection). There is NO default value and it should be specified by users.

* `cache_eval_data`: Integer, the cache size of evaluation data on device, set this parameter greater than zero to restrict the memory that will be used. The default value is 0.

* `label_dim`: Integer, the label dimension. 1 implies it is a binary label. For example, if an item is clicked or not. There is NO default value and it should be specified by users.

* `label_name`: String, the name of the label tensor to be referenced by following layers. There is NO default value and it should be specified by users.

* `dense_dim`: Integer, the number of dense (or continuous) features. If there is no dense feature, set it to 0. There is NO default value and it should be specified by users.

* `dense_name`: Integer, the name of the dense input tensor to be referenced by following layers. There is NO default value and it should be specified by users.

* `num_samples`: Integer, the number of samples in the traning dataset. This is ONLY valid for Raw dataset. The default value is 0.

* `eval_num_samples`: Integer, the number of samples in the evaluation dataset. This is ONLY valid for Raw dataset. The default value is 0.

* `float_label_dense`: Boolean, this is valid only for the Raw dataset format. If its value is set to `True`, the label and dense features for each sample are interpreted as float values. Otherwise, they are read as integer values while the dense features are preprocessed with log(dense[i] + 1.f). The default value is `False`.

* `num_workers`: Integer, the number of data reader workers that concurrently load data. You can empirically decide the best one based on your dataset, training environment. The default value is 12.

* `slot_size_array`: List[int], the list of categorical feature cardinalities of each slot. This is valid for Raw and Parquet dataset. The default value is `[]`.

* `data_reader_sparse_param_array`: List[hugectr.DataReaderSparseParam], the list of the sparse parameters for categorical inputs. Each `DataReaderSparseParam` instance should be constructed with `hugectr.DataReaderSparse_t`, `max_feature_num`, `max_nnz` and `slot_num`. The supported types of `hugectr.DataReaderSparse_t` include `hugectr.DataReaderSparse_t.Distributed` and `hugectr.DataReaderSparse_t.Localized`. The maximum number of features per sample for the specified spare input can be specified by `max_feature_num`. For `max_nnz`, if it is set to 1, the dataset is specified as one-hot so that the memory consumption can be reduced. As for `slot_num`, it specifies the number of slots used for this sparse input in the dataset. The total number of categorical inputs is exactly the length of `data_reader_sparse_param_array`. There is NO default value and it should be specified by users.

* `sparse_names`: List[str], the list of names of the sparse input tensors to be referenced by following layers. The order of the names should be consistent with sparse parameters in `data_reader_sparse_param_array`. There is NO default value and it should be specified by users.

### SparseEmbedding ###
**SparseEmbedding class**
```bash
hugectr.SparseEmbedding()
```
`SparseEmbedding` specifies the parameters related to the sparse embedding layer. One or several `SparseEmbedding` layers should be added to the Model instance after `Input` and before `DenseLayer`. Please refer to [Embedding Layers](./configuration_file_setup.md#embedding-layers) if you want to get detailed information about SparseEmbedding.

**Arguments**
* `embedding_type`: The embedding type to be used. The supported types include `hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash`, `hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash` and `hugectr.Embedding_t.LocalizedSlotSparseEmbeddingOneHot`. There is NO default value and it should be specified by users.

* `max_vocabulary_size_per_gpu`: Integer, the maximum vocabulary size or cardinality across all the input features. There is NO default value and it should be specified by users.

* `embedding_vec_size`: Integer, the embedding vector size. There is NO default value and it should be specified by users.

* `combiner`: Integer, the intra-slot reduction operation (0=sum, 1=average). There is NO default value and it should be specified by users.

* `sparse_embedding_name`: String, the name of the sparse embedding tensor to be referenced by following layers. There is NO default value and it should be specified by users.

* `bottom_name`: String, the number of the bottom tensor to be consumed by this sparse embedding layer. Please note that it should be a predefined sparse input name. There is NO default value and it should be specified by users.

* `slot_size_array`: List[int], the cardinality array of input features. It should be consistent with that of the sparse input. If `max_vocabulary_size_per_gpu` is specified, this parameter is ignored. There is NO default value and it should be specified by users.


### DenseLayer ###
**DenseLayer class**
```bash
hugectr.DenseLayer()
```
`DenseLayer` specifies the parameters related to the dense layer or the loss function. HugeCTR currently supports multiple dense layers and loss functions, Please refer to [Dense Layers](./configuration_file_setup.md#dense-layers) and [Losses](./configuration_file_setup.md#losses) if you want to get detailed information about dense layers and loss functions.

**Arguments**
* `layer_type`: The layer type to be used. The supported types include `hugectr.Layer_t.Add`, `hugectr.Layer_t.BatchNorm`, `hugectr.Layer_t.Cast`, `hugectr.Layer_t.Concat`, `hugectr.Layer_t.DotProduct`, `hugectr.Layer_t.Dropout`, `hugectr.Layer_t.ELU`, `hugectr.Layer_t.FmOrder2`, `hugectr.Layer_t.FusedInnerProduct`, `hugectr.Layer_t.InnerProduct`, `hugectr.Layer_t.Interaction`, `hugectr.Layer_t.MultiCross`, `hugectr.Layer_t.ReLU`, `hugectr.Layer_t.ReduceSum`, `hugectr.Layer_t.Reshape`, `hugectr.Layer_t.Sigmoid`, `hugectr.Layer_t.Slice`, `hugectr.Layer_t.WeightMultiply`, `hugectr.Layer_t.BinaryCrossEntropyLoss`, `hugectr.Layer_t.CrossEntropyLoss` and `hugectr.Layer_t.MultiCrossEntropyLoss`. There is NO default value and it should be specified by users.

* `bottom_names`: List[str], the list of bottom tensor names to be consumed by this dense layer. Each name in the list should be the predefined tensor name. There is NO default value and it should be specified by users.

* `top_names`: List[str], the list of top tensor names, which specify the output tensors of this dense layer. There is NO default value and it should be specified by users.

* `factor`: Float, exponential average factor such as runningMean = runningMean*(1-factor) + newMean*factor for the `BatchNorm` layer. The default value is 1.

* `eps`: Float, epsilon value used in the batch normalization formula for the `BatchNorm` layer. The default value is 1e-5.

* `gamma_init_type`: Specifies how to initialize the gamma (or scale) array for the `BatchNorm` layer. The supported types include `hugectr.Initializer_t.Default`, `hugectr.Initializer_t.Uniform`, `hugectr.Initializer_t.XavierNorm`, `hugectr.Initializer_t.XavierUniform` and `hugectr.Initializer_t.Zero`. The default value is `hugectr.Initializer_t.Default`.

* `beta_init_type`: Specifies how to initialize the beta (or offset) array for the `BatchNorm` layer. The supported types include `hugectr.Initializer_t.Default`, `hugectr.Initializer_t.Uniform`, `hugectr.Initializer_t.XavierNorm`, `hugectr.Initializer_t.XavierUniform` and `hugectr.Initializer_t.Zero`. The default value is `hugectr.Initializer_t.Default`.

* `dropout_rate`: Float, The dropout rate to be used for the `Dropout` layer. It should be between 0 and 1. Setting it to 1 indicates that there is no dropped element at all. The default value is 0.5.

* `elu_alpha`: Float, the scalar that decides the value where this `ELU` function saturates for negative values. The default value is 1.

* `num_output`: Integer, the number of output elements for the `InnerProduct` or `FusedInnerProduct` layer. The default value is 1.

* `weight_init_type`: Specifies how to initialize the weight array for the `InnerProduct`, `FusedInnerProduct`, `MultiCross` or `WeightMultiply` layer. The supported types include `hugectr.Initializer_t.Default`, `hugectr.Initializer_t.Uniform`, `hugectr.Initializer_t.XavierNorm`, `hugectr.Initializer_t.XavierUniform` and `hugectr.Initializer_t.Zero`. The default value is `hugectr.Initializer_t.Default`.

* `bias_init_type`: Specifies how to initialize the bias array for the `InnerProduct`, `FusedInnerProduct` or `MultiCross` layer. The supported types include `hugectr.Initializer_t.Default`, `hugectr.Initializer_t.Uniform`, `hugectr.Initializer_t.XavierNorm`, `hugectr.Initializer_t.XavierUniform` and `hugectr.Initializer_t.Zero`. The default value is `hugectr.Initializer_t.Default`.

* `num_layers`: Integer, the Number of cross layers for the `MultiCross` layer. It should be set as a positive number if you want to use the cross network. The default value is 0.

* `leading_dim`: Integer, the innermost dimension of the output tensor for the `Reshape` layer. It must be the multiple of the total number of input elements. The default value is 1.

* `selected`: Boolean, whether to use the selected mode for the `Reshape` layer. The default value is False.

* `selected_slots`: List[int], the selected slots for the `Reshape` layer. It will be ignored if `selected` is False. The default value is [].

* `ranges`: List[Tuple[int, int]], used for the `Slice` layer. A list of tuples in which each one represents a range in the input tensor to generate the corresponding output tensor. For example, (2, 8) indicates that 8 elements starting from the second element in the input tensor are used to create an output tensor. The number of tuples corresponds to the number of output tensors. Ranges are allowed to overlap unless it is a reverse or negative range. The default value is [].

* `weight_dims`: List[int], the shape of the weight matrix (slot_dim, vec_dim) where vec_dim corresponds to the latent vector length for the `WeightMultiply` layer. It should be set correctly if you want to employ the weight multiplication. The default value is [].

* `out_dim`: Integer, the output vector size for the `FmOrder2` layer. It should be set as a positive number if your want to use factorization machine. The default value is 0.

* `axis`: Integer, the dimension to reduce for the `ReduceSum` layer. If the input is N-dimensional, 0 <= axis < N. The default value is 1.

* `target_weight_vec`: List[float], the target weight vector for the `MultiCrossEntropyLoss` layer. The default value is [].

* `use_regularizer`: Boolean, whether to use the regularizer for the `BinaryCrossEntropyLoss`, `CrossEntropyLoss` or `MultiCrossEntropyLoss` layer. The default value is False.

* `regularizer_type`: The regularizer type for the `BinaryCrossEntropyLoss`, `CrossEntropyLoss` or `MultiCrossEntropyLoss` layer. The supported types include `hugectr.Regularizer_t.L1` and `hugectr.Regularizer_t.L2`. It will be ignored if `use_regularizer` is False. The default value is `hugectr.Regularizer_t.L1`.

* `lambda`: Float, the lambda value of the regularization term for the `BinaryCrossEntropyLoss`, `CrossEntropyLoss` or `MultiCrossEntropyLoss` layer. It will be ignored if `use_regularizer` is False. The default value is 0.

### Model ###
**Model class**
```bash
hugectr.Model()
```
`Model` groups data input, embeddings and dense network into an object with traning features. The construction of `Model` requires a `SolverParser` instance and an `OptParamsBase` instance, which can be created by `hugectr.solver_parser_helper` and `hugectr.optimizer.CreateOptimizer` respectively. Please note that the value of `use_mixed_precision` should be the same for `SolverParser` and `OptParamsBase`. Otherwise, the `Model` object cannot be created correctly.

**Arguments**
* `solver_parser`: A hugectr.SolverParser object, the solver configuration for the model.

* `opt_params`: A hugectr.OptParamsBase object, the optimizer configuration for the model.

**add method**
```bash
hugectr.Model.add()
```
The `add` method of Model adds an instance of Input, SparseEmbedding or DenseLayer to the created Model object. Typically, a Model object is comprised of one Input, several SparseEmbedding and a series of DenseLayer instances. Please note that the loss function for HugeCTR model training is taken as a DenseLayer instance.

**Arguments**
* `input` or `sparse_embedding` or `dense_layer`: This method is an overloaded method that can accept `hugectr.Input`, `hugectr.SparseEmbedding` or `hugectr.DenseLayer` as an argument. It allows the users to construct their model flexibly without the JSON configuration file.
***

**compile method**
```bash
hugectr.Model.compile()
```
This method takes no extra arguments. It allocates the internal buffer and initializes the model.
***

**fit method**
```bash
hugectr.Model.fit()
```
This method takes no extra arguments. It trains the model for a fixed number of epochs (epoch mode) or iterations (non-epoch mode). You can switch the mode of training through different configurations within `solver_parser_helper`. To use epoch mode training, `repeat_dataset` should be set as `False` and `num_epochs` should be set as a positive number. To use non-epoch mode training, `repeat_dataset` should be set as `True` and `max_iter` should be set as a positive number.
***

**summary method**
```bash
hugectr.Model.summary()
```
This method takes no extra arguments and prints a string summary of the model. Users can have an overview of the model structure with this method.
***

## Low-level Training API ##
For HugeCTR low-level training API, the core data structures are `SolverParser`, `LearningRateScheduler`, `DataReader`, `ModelOversubscriber` and `Session`. HugeCTR currently supports both epoch mode training and non-epoch mode training for dataset in Norm and Raw formats, and only supports non-epoch mode training for dataset in Parquet format. While introducing the API usage, we will elaborate how to employ these two modes of training.
 
### SolverParser ###
Please refer to the [above](./python_interface.md#solverparser). Please NOTE that `max_iter`, `num_epochs`, `display`, `snapshot`, `eval_interval`, `use_model_oversubscriber` and `temp_embedding_dir` within the `solver_parser_helper` method will be ignored if you are using the low-level training API, because they can be specified directly in the low-level python code.

### LearningRateScheduler ###
**get_learning_rate_scheduler method**
```bash
hugectr.get_learning_rate_scheduler()
```
`get_learning_rate_scheduler` generates and returns a LearningRateScheduler object based on the configuration JSON file. When the `SGD` optimizer is adopted for training, the returned object can obtain the dynamically changing learning rate according to the `warmup_steps`, `decay_start`和`decay_steps` configured in the JSON file。Please refer to [SGD Optimizer and Learning Rate Scheduling
](./hugectr_user_guide.md#sgd-optimizer-and-learning-rate-scheduling) if you want to get detailed information about LearningRateScheduler.

**Arguments**
* `configure_file`: The JOSN format configuration file.
***
**get_next method**
```bash
hugectr.LearningRateScheduler.get_next()
```
This method takes no extra arguments and returns the learning rate to be used for the next iteration.

### DataReader ###
**set_source method**
```bash
hugectr.DataReader32.set_source()
hugectr.DataReader64.set_source()
```
The `set_source` method of DataReader currently supports the dataset in Norm and Raw formats, and should be used in epoch mode training. When the data reader reaches the end of file for the current training data or evaluation data, this method can be used to re-specify the training data file or evaluation data file.

**Arguments**
* `file_name`: The file name of the new training source or evaluation source. For Norm format dataset, it takes the form of `file_list.txt`. For Raw format dataset, it appears as `data.bin`. The default value is `''`, which means that the data reader will reset to the beginning of the current data file.

### ModelOversubscriber ###
**update method**
```bash
hugectr.ModelOversubscriber.update()
```
The `update` method of ModelOversubscriber currently supports Norm format datasets. Using this method requires that a series of file lists and the corresponding keyset files are generated at the same time when preprocessing the original data to Norm format. This method gives you the ability to load a subset of an embedding table into the GPU in a coarse grained, on-demand manner during the training stage. Please refer to [Model Oversubscription](./hugectr_user_guide.md#model-oversubscription) if you want to get detailed information about ModelOversubscriber.

**Arguments**
* `keyset_file` or `keyset_file_list`: This method is an overloaded method that can accept str or List[str] as an argument. For the model with multiple embedding tables, if the keyset of each embedding table is not separated when generating the keyset files, then pass in the `keyset_file`. If the keyset of each embedding table has been separated when generating keyset files, you need to pass in the `keyset_file_list`, the size of which should equal to the number of embedding tables.

### Session ###
**Session class**
```bash
hugectr.Session()
```
`Session` groups embeddings and dense network into an object with traning features. The construction of `Session` requires a configuration JSON file to parse the `optimizer` and `layers` clauses. Please refer to [Optimizer](./configuration_file_setup.md#optimizer) and [Layers](./configuration_file_setup.md#layers) to know the correct usage of the configuration file.

**Arguments**
* `solver_config`: A hugectr.SolverParser object, the solver configuration of the session.

* `config_file`: String, the configuration file in JSON format.

* `use_model_oversubscriber`: Boolean, whether to employ the features of ModelOversubscriber. The default value is `False`.

* `temp_embedding_dir:`: String，where to store the temporary embedding table files. The path needs to have write permission to support the features of ModelOversubscriber. The default value is `''`.
***

**get_model_oversubscriber method**
```bash
hugectr.Session.get_model_oversubscriber()
```
This method takes no extra arguments and returns the ModelOversubscriber object.
***

**get_data_reader_train method**
```bash
hugectr.Session.get_data_reader_train()
```
This method takes no extra arguments and returns the DataReader object that reads the training data.
***

**get_data_reader_eval method**
```bash
hugectr.Session.get_data_reader_eval()
```
This method takes no extra arguments and returns the DataReader object that reads the evaluation data.
***

**start_data_reading method**
```bash
hugectr.Session.start_data_reading()
```
This method takes no extra arguments and should be used if and only if it is under the non-epoch mode training. The method starts the `train_data_reader` and `eval_data_reader` before entering the training loop.
***

**set_learning_rate method**
```bash
hugectr.Session.set_learning_rate()
```
This method is used together with the `get_next` method of `LearningRateScheduler` and sets the learning rate for the next training iteration.

**Arguments**
* `lr`: Float, the learning rate to be set。
***

**train method**
```bash
hugectr.Session.train()
```
This method takes no extra arguments and executes one iteration of the model weights based on one minibatch of training data.
***

**get_current_loss method**
```bash
hugectr.Session.get_current_loss()
```
This method takes no extra arguments and returns the loss value for the current iteration.
***

**check_overflow method**
```bash
hugectr.Session.check_overflow()
```
This method takes no extra arguments and checks whether any embedding has encountered overflow.
***

**copy_weights_for_evaluation method**
```bash
hugectr.Session.copy_weights_for_evaluation()
```
This method takes no extra arguments and copies the weights of the dense network from training layers to evaluation layers.
***

**eval method**
```bash
hugectr.Session.eval()
```
This method takes no arguments and calculates the evaluation metrics based on one minibatch of evaluation data.
***

**get_eval_metrics method**
```bash
hugectr.Session.get_eval_metrics()
```
This method takes no extra arguments and returns the average evaluation metrics of several minibatches of evaluation data.
***

**evaluation method**
```bash
hugectr.Session.evaluation()
```
This method returns the average evaluation metrics of several minibatches of evaluation data. You can export predictions and labels to files by passing arguments into this method.

**Arguments**
* `export_predictions_out_file`: Optinal. If passed, the evaluation prediction results will be writen to the file specified by this argument. The order of the prediction results are the same as that of the labels, but may be different with the order of the samples in the dataset.

* `export_labels_out_file:`: Optinal. If passed, the evaluation labels will be writen to the file specified by this argument. The order of the labels are the same as that of the prediction results, but may be different with the order of the samples in the dataset.

## Inference API ##
For HugeCTR inference API, the core data structures are `ParameterServer`, `EmbeddingCache` and `InferenceSession`. Please refer to [Inference Framework](https://gitlab-master.nvidia.com/dl/hugectr/hugectr_inference_backend/-/blob/main/docs/user_guide.md#inference-framework) to get informed of the hierarchy of HugeCTR inference implementation.

Please **NOTE** that Inference API requires a configuration JSON file which is slightly different from the training JSON file. We need `inference` and `layers` clauses in the inference JSON file. The paths of the stored dense model and sparse model(s) should be specified at `dense_model_file` and `sparse_model_file` within the `inference` clause. Some modifications need to be made to `data` within the `layers` clause and the last layer should be replaced by `SigmoidLayer`. Please refer to [HugeCTR Inference Notebook](../notebooks/hugectr_inference.ipynb) for detailed information of the inference JSON file.

Please **NOTE** that if the `input_key_type` of the trained model is `I64`, you need to specify the argument `i64_input_key` as `True` when calling `hugectr.inference.CreateParameterServer`, `hugectr.inference.CreateEmbeddingCache` and `hugectr.inference.InferenceSession.predict()`, i.e., the calling to these methods should be consistent in terms of the `input_key_type`.

### ParameterServer ###
**CreateParameterServer method**
```bash
hugectr.inference.CreateParameterServer()
```
`ParameterServer` resides on the CPU and stores all the embedding tables of multiple models. This factory method `CreateParameterServer` creates and returns a `ParameterServer` object.

**Arguments**
* `model_config_path`: List[str], the list of configuration JSON files for different models.

* `model_name`: List[str], the list of different model names.
 
* `i64_input_key`: Boolean, whether to use I64 input key for the parameter server.

Please **NOTE** that the order of the configuration files within `model_config_path` and that of the model names within `model_name` should be consistent.

### EmbeddingCache ###
**CreateEmbeddingCache method**
```bash
hugectr.inference.CreateEmbeddingCache()
```
`EmbeddingCache` resides on the GPU and stores a portion of embeddings for a specific model. This factory method `CreateEmbeddingCache` creates and returns an `EmbeddingCache` object.

**Arguments**
* `parameter_server`: A hugectr.inference.ParameterServerBase object, the parameter server that accommodates the embedding tables to be cached on GPU.

* `cuda_dev_id`: Integer, the GPU device index.

* `use_gpu_embedding_cache`: Boolean, whether to employ the features of GPU embedding cache. If the value is `True`, the embedding vector look up will go to GPU embedding cache. Otherwise, it will reach out to the CPU parameter server directly.

* `cache_size_percentage`: Float, the percentage of cached embeddings on GPU relative to all the embedding tables on CPU.

* `model_config_path`: String, the configuration file of the model whose embeddings will be on the GPU embedding cache.

* `model_name`: String, the name of the model whose embeddings will be on the GPU embedding cache.

* `i64_input_key`: Boolean, whether to use I64 input key for the embedding cache.

Please **NOTE** that the `parameter_server` specified here should contain the model whose embedding tables are supposed to be cached on GPU. Besides, `i64_input_key` should be consistent with that of the `parameter_server`.

### InferenceSession ###
**InferenceSession class**
```bash
hugectr.inference.InferenceSession()
```
`InferenceSession` groups embedding cache and dense network into an object with inference features. The construction of `InferenceSession` requires an inference configuration JSON file. 

**Arguments**
* `config_file`: String, the inference configuration file.

* `device_id`: Integer, GPU device index.

* `embedding_cache`: A hugectr.inference.EmbeddingCacheInterface object, the embedding cache that loads a portion of embeddings on GPU for the model to be used.

Please **NOTE** that the `device_id` specified here should be the same as that of the `embedding_cache`.
***

**predict method**
```bash
hugectr.inference.InferenceSession.predict()
```
The `predict` method of InferenceSession currently supports model with only one embedding table. This method makes predictions for the samples in the inference inputs. Please refer to [HugeCTR Inference Notebook](../notebooks/hugectr_inference.ipynb) for detailed usage information.

**Arguments**
* `dense_feature`: List[float], the dense features of the samples.

* `embeddingcolumns`: List[int], the embedding keys of the samples.

* `row_ptrs`: List[int], the row pointers that indicate which embedding keys belong to the same slot.

* `i64_input_key`: Boolean, whether to use I64 input key for the inference session.
***
Taking Deep and Cross Model on Criteo dataset for example, if the inference request includes two samples, then `dense_feature` will be of the length 2\*13, `embeddingcolumns` will be of the length 2\*26, and `row_ptrs` will be like [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52].

## Sample Code ##
The sample code for non-epoch mode training, epoch mode training and inference will be given here. Please make sure the JSON files and the datasets are ready to successfully run the sample code. Please refer to [HugeCTR Python Interface Notebook](../notebooks/python_interface.ipynb) and [HugeCTR Inference Notebook](../notebooks/hugectr_inference.ipynb) to get familiar with the workflow of HugeCTR training and inference.

### Non-epoch Mode Training ###
```bash
from hugectr import Session, solver_parser_helper,get_learning_rate_scheduler
from mpi4py import MPI
json_file = "your_config.json"
solver_config = solver_parser_helper(seed = 0,
                                    batchsize = 16384,
                                    batchsize_eval = 16384,
                                    model_file = "",
                                    embedding_files = [],
                                    vvgpu = [[0,1,2,3,4,5,6,7]],
                                    use_mixed_precision = True,
                                    scaler = 1024,
                                    i64_input_key = False,
                                    use_algorithm_search = True,
                                    use_cuda_graph = True,
                                    repeat_dataset = True)
lr_sch = get_learning_rate_scheduler(json_file)
sess = Session(solver_config, json_file)
sess.start_data_reading()
for i in range(10000):
    lr = lr_sch.get_next()
    sess.set_learning_rate(lr)
    sess.train()
    if (i%100 == 0):
        loss = sess.get_current_loss()
        print("[HUGECTR][INFO] iter: {}; loss: {}".format(i, loss))
    if (i%1000 == 0 and i != 0):
        sess.check_overflow()
        sess.copy_weights_for_evaluation()
        for _ in range(solver_config.max_eval_batches):
            sess.eval()
        metrics = sess.get_eval_metrics()
        print("[HUGECTR][INFO] iter: {}, {}".format(i, metrics))
sess.download_params_to_files("./", 10000)
```

### Epoch Mode Training ###
**NOTE** This sample employs the epoch mode training and enables the feature of ModelOversubscriber. Please prepare the JSON file, Norm dataset with file lists together with keyset files and a temporary write-enabled directory before running this script. See [HugeCTR Python Interface Notebook](../notebooks/python_interface.ipynb) for help.
```bash
from hugectr import Session, solver_parser_helper, get_learning_rate_scheduler
from mpi4py import MPI
json_file = "your_config.json"
temp_dir = "./your_temp_embedding_dir"
dataset = [("file_list."+str(i)+".txt", "file_list."+str(i)+".keyset") for i in range(5)]
solver_config = solver_parser_helper(seed = 0,
                                    batchsize = 16384,
                                    batchsize_eval =16384,
                                    model_file = "",
                                    embedding_files = [],
                                    vvgpu = [[0]],
                                    use_mixed_precision = False,
                                    scaler = 1.0,
                                    i64_input_key = False,
                                    use_algorithm_search = True,
                                    use_cuda_graph = True,
                                    repeat_dataset = False)
lr_sch = get_learning_rate_scheduler(json_file)
sess = Session(solver_config, json_file, True, temp_dir)
data_reader_train = sess.get_data_reader_train()
data_reader_eval = sess.get_data_reader_eval()
data_reader_eval.set_source("file_list.5.txt")
model_oversubscriber = sess.get_model_oversubscriber()
iteration = 0
for file_list, keyset_file in dataset:
    data_reader_train.set_source(file_list)
    model_oversubscriber.update(keyset_file)
    while True:
        lr = lr_sch.get_next()
        sess.set_learning_rate(lr)
        good = sess.train()
        if good == False:
            break
        if iteration % 100 == 0:
            sess.check_overflow()
            sess.copy_weights_for_evaluation()
            data_reader_eval = sess.get_data_reader_eval()
            good_eval = True
            j = 0
            while good_eval:
                if j >= solver_config.max_eval_batches:
                    break
                good_eval = sess.eval()
                j += 1
            if good_eval == False:
                data_reader_eval.set_source()
            metrics = sess.get_eval_metrics()
            print("[HUGECTR][INFO] iter: {}, metrics: {}".format(iteration, metrics))
        iteration += 1
    print("[HUGECTR][INFO] trained with data in {}".format(file_list))
sess.download_params_to_files("./", iteration)
```

### Inference ###
**NOTE** Please prepare the inference JSON file, the dense model file, the sparse model file and inference data file in the right format before running this script. See [HugeCTR Inference Notebook](../notebooks/hugectr_inference.ipynb) for help.
```bash
from hugectr.inference import CreateParameterServer, CreateEmbeddingCache, InferenceSession
from mpi4py import MPI
config_file = "your_inference_config.json"
model_name = "your_model_name"
data_path = "your_inference_inputs_data.txt"
use_embedding_cache = True
# read data from file
data_file = open(data_path)
labels = [int(item) for item in data_file.readline().split(' ')]
dense_features = [float(item) for item in data_file.readline().split(' ')]
embedding_columns = [int(item) for item in data_file.readline().split(' ')]
row_ptrs = [int(item) for item in data_file.readline().split(' ')]
# create parameter server, embedding cache and inference session
parameter_server = CreateParameterServer([config_file], [model_name], False)
embedding_cache = CreateEmbeddingCache(parameter_server, 0, use_gpu_embedding_cache, 0.2, config_file, model_name, False)
inference_session = InferenceSession(config_file, 0, embedding_cache)
# make prediction and calculate accuracy
output = inference_session.predict(dense_features, embedding_columns, row_ptrs)
```
