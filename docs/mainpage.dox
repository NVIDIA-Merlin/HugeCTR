/**
 * @mainpage notitle
 * @section intro_ Introduction
 * 
 * HugeCTR is a high-efficiency training framework designed for Click-Through-Rate (CTR) estimating tasks. 
 * High performance and easy use are two goals in our design.
 * 
 * @section arch_ Architecture
 *
 * To enable large embedding training, the embedding table in HugeCTR is model parallel and distributed 
 * across all the GPUs in the training set which includes multiple nodes and multiple GPUs. Meanwhile, 
 * the dense model is data parallel, which has one copy in each GPU.
 * HugeCTR supports Embedding + MLP like Networks, in which Embedding has a three stages workflow: table lookup, 
 * reducing the weights within a slot, concat the weights from different slots.
 *
 * @section features_ Highlight features
 * 
 * @subsection hashtable_ GPU hashtable and dynamic insertion
 *
 * GPU Hashtable makes the data preprocessing easier and enables dynamic insertion in HugeCTR 2.0. 
 * The input training data is hash values (64bit long long) instead of original indexes. 
 * Embeddingâ€™s initialization is not required before training and A pair of key,value (random small weight) 
 * will be inserted during runtime once a new key appears in the training data. 
 *
 * @subsection multi-node_ Multi-node training and enabling large embedding
 * 
 * To enable very large embedding HugeCTR supports multi-node training, so ideally any size of embedding can be trained. 
 * In multi-node solution, sparse model or embedding is also distributed across the nodes, 
 * and dense model is in data parallel. In our implementation, HugeCTR leverages NCCL for high speed and scalable cross GPU/Node transaction.
 *
 * @subsection mixed_precision_ Mixed precision training
 *
 * Mixed Precision is a highlight feature in NVIDIA new GPUs. In our implementation, 
 * fully connected layer can be configured to running on TensorCore with FP16, 
 * while loss scaling will be applied to avoid arithmetic underflow. 
 * Mixed Precision training can be enabled in cmake options.
 */
